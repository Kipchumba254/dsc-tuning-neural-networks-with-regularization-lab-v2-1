{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Tuning Neural Networks with Regularization - Lab \n",
    "\n",
    "## Introduction\n",
    "\n",
    "In this lab, you'll use a train-test partition as well as a validation set to get better insights about how to tune neural networks using regularization techniques. You'll start by repeating the process from the last section: importing the data and performing preprocessing including one-hot encoding. From there, you'll define and compile the model like before. \n",
    "\n",
    "## Objectives\n",
    "\n",
    "You will be able to:\n",
    "\n",
    "- Apply early stopping criteria with a neural network \n",
    "- Apply L1, L2, and dropout regularization on a neural network  \n",
    "- Examine the effects of training with more data on a neural network  \n",
    "\n",
    "\n",
    "## Load the Data\n",
    "\n",
    "Run the following cell to import some of the libraries and classes you'll need in this lab. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import random\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "from sklearn.model_selection import train_test_split\n",
    "from tensorflow.keras.utils import to_categorical\n",
    "from sklearn.preprocessing import LabelBinarizer\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings(action='ignore', category=FutureWarning)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The data is stored in the file `'Bank_complaints.csv'`. Load and preview the dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Product</th>\n",
       "      <th>Consumer complaint narrative</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Student loan</td>\n",
       "      <td>In XX/XX/XXXX I filled out the Fedlaon applica...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Student loan</td>\n",
       "      <td>I am being contacted by a debt collector for p...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Student loan</td>\n",
       "      <td>I cosigned XXXX student loans at SallieMae for...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Student loan</td>\n",
       "      <td>Navient has sytematically and illegally failed...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Student loan</td>\n",
       "      <td>My wife became eligible for XXXX Loan Forgiven...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "        Product                       Consumer complaint narrative\n",
       "0  Student loan  In XX/XX/XXXX I filled out the Fedlaon applica...\n",
       "1  Student loan  I am being contacted by a debt collector for p...\n",
       "2  Student loan  I cosigned XXXX student loans at SallieMae for...\n",
       "3  Student loan  Navient has sytematically and illegally failed...\n",
       "4  Student loan  My wife became eligible for XXXX Loan Forgiven..."
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Load and preview the dataset\n",
    "df = pd.read_csv('Bank_complaints.csv')\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preprocessing Overview\n",
    "\n",
    "Before you begin to practice some of your new tools such as regularization and optimization, let's practice munging some data as you did in the previous section with bank complaints. Recall some techniques:\n",
    "\n",
    "* Sampling in order to reduce training time (investigate model accuracy vs data size later on)\n",
    "* Train - test split\n",
    "* One-hot encoding your complaint text\n",
    "* Transforming your category labels \n",
    "\n",
    "## Preprocessing: Generate a Random Sample\n",
    "\n",
    "Since you have quite a bit of data and training neural networks takes a substantial amount of time and resources, downsample in order to test your initial pipeline. Going forward, these can be interesting areas of investigation: how does your model's performance change as you increase (or decrease) the size of your dataset?  \n",
    "\n",
    "- Generate a random sample of 10,000 observations using seed 123 for consistency of results. \n",
    "- Split this sample into `X` and `y` "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Downsample the data\n",
    "np.random.seed(123)\n",
    "df_sample = df.sample(10000, random_state=123)\n",
    "\n",
    "# Split the data into X and y\n",
    "y = df_sample['Product']\n",
    "X = df_sample.drop(columns='Product', axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train-test split\n",
    "\n",
    "- Split the data into training and test sets \n",
    "- Assign 1500 obervations to the test set and use 42 as the seed "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split data into training and test sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=1500, random_state=42)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Validation set \n",
    "\n",
    "As mentioned in the previous lesson, it is good practice to set aside a validation set, which is then used during hyperparameter tuning. Afterwards, when you have decided upon a final model, the test set can then be used to determine an unbiased perforance of the model. \n",
    "\n",
    "Run the cell below to further divide the training data into training and validation sets. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split the data into training and validation sets\n",
    "X_train_final, X_val, y_train_final, y_val = train_test_split(X_train, y_train, test_size=1000, random_state=42)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preprocessing: One-hot Encoding the Complaints\n",
    "\n",
    "As before, you need to do some preprocessing before building a neural network model. \n",
    "\n",
    "- Keep the 2,000 most common words and use one-hot encoding to reformat the complaints into a matrix of vectors \n",
    "- Transform the training, validate, and test sets "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use one-hot encoding to reformat the complaints into a matrix of vectors \n",
    "# Only keep the 2000 most common words \n",
    "\n",
    "tokenizer = Tokenizer(2000)\n",
    "tokenizer.fit_on_texts(X_train_final['Consumer complaint narrative'])\n",
    "\n",
    "X_train_tokens = tokenizer.texts_to_matrix(X_train_final['Consumer complaint narrative'], mode='binary')\n",
    "X_val_tokens = tokenizer.texts_to_matrix(X_val['Consumer complaint narrative'], mode='binary')\n",
    "X_test_tokens = tokenizer.texts_to_matrix(X_test['Consumer complaint narrative'], mode='binary')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preprocessing: Encoding the Products\n",
    "\n",
    "Similarly, now transform the descriptive product labels to integers labels. After transforming them to integer labels, retransform them into a matrix of binary flags, one for each of the various product labels.  \n",
    "  \n",
    "> **Note**: This is similar to your previous work with dummy variables. Each of the various product categories will be its own column, and each observation will be a row. In turn, each of these observation rows will have a 1 in the column associated with it's label, and all other entries for the row will be zero. \n",
    "\n",
    "Transform the training, validate, and test sets. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Transform the product labels to numerical values\n",
    "lb = LabelBinarizer()\n",
    "\n",
    "\n",
    "y_train_lb = lb.fit_transform(y_train_final)\n",
    "y_val_lb = lb.transform(y_val)\n",
    "y_test_lb = lb.transform(y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## A Baseline Model \n",
    "\n",
    "Rebuild a fully connected (Dense) layer network:  \n",
    "- Use 2 hidden layers with 50 units in the first and 25 in the second layer, both with `'relu'` activation functions (since you are dealing with a multiclass problem, classifying the complaints into 7 classes) \n",
    "- Use a `'softmax'` activation function for the output layer  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Build a baseline neural network model using Keras\n",
    "random.seed(123)\n",
    "from keras import models\n",
    "from keras import layers\n",
    "baseline_model = models.Sequential()\n",
    "\n",
    "baseline_model.add(layers.Dense(50, activation='relu', input_shape=(X_train_tokens.shape[1],)))\n",
    "baseline_model.add(layers.Dense(25, activation='relu'))\n",
    "baseline_model.add(layers.Dense(7, activation='softmax'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Compile the Model\n",
    "\n",
    "Compile this model with: \n",
    "\n",
    "- a stochastic gradient descent optimizer \n",
    "- `'categorical_crossentropy'` as the loss function \n",
    "- a focus on `'accuracy'` "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compile the model\n",
    "baseline_model.compile(optimizer='sgd', loss='categorical_crossentropy', metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train the Model\n",
    "\n",
    "- Train the model for 150 epochs in mini-batches of 256 samples \n",
    "- Include the `validation_data` argument to ensure you keep track of the validation loss  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/150\n",
      "\u001b[1m30/30\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 16ms/step - accuracy: 0.1237 - loss: 1.9616 - val_accuracy: 0.1500 - val_loss: 1.9551\n",
      "Epoch 2/150\n",
      "\u001b[1m30/30\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - accuracy: 0.1649 - loss: 1.9402 - val_accuracy: 0.1830 - val_loss: 1.9404\n",
      "Epoch 3/150\n",
      "\u001b[1m30/30\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - accuracy: 0.2059 - loss: 1.9258 - val_accuracy: 0.2110 - val_loss: 1.9264\n",
      "Epoch 4/150\n",
      "\u001b[1m30/30\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - accuracy: 0.2403 - loss: 1.9125 - val_accuracy: 0.2460 - val_loss: 1.9111\n",
      "Epoch 5/150\n",
      "\u001b[1m30/30\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - accuracy: 0.2540 - loss: 1.8950 - val_accuracy: 0.2650 - val_loss: 1.8929\n",
      "Epoch 6/150\n",
      "\u001b[1m30/30\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - accuracy: 0.2759 - loss: 1.8737 - val_accuracy: 0.2890 - val_loss: 1.8709\n",
      "Epoch 7/150\n",
      "\u001b[1m30/30\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 9ms/step - accuracy: 0.3037 - loss: 1.8494 - val_accuracy: 0.3110 - val_loss: 1.8450\n",
      "Epoch 8/150\n",
      "\u001b[1m30/30\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - accuracy: 0.3235 - loss: 1.8214 - val_accuracy: 0.3290 - val_loss: 1.8143\n",
      "Epoch 9/150\n",
      "\u001b[1m30/30\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - accuracy: 0.3462 - loss: 1.7849 - val_accuracy: 0.3580 - val_loss: 1.7781\n",
      "Epoch 10/150\n",
      "\u001b[1m30/30\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - accuracy: 0.3702 - loss: 1.7501 - val_accuracy: 0.3810 - val_loss: 1.7376\n",
      "Epoch 11/150\n",
      "\u001b[1m30/30\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - accuracy: 0.3987 - loss: 1.7054 - val_accuracy: 0.4030 - val_loss: 1.6931\n",
      "Epoch 12/150\n",
      "\u001b[1m30/30\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - accuracy: 0.4225 - loss: 1.6618 - val_accuracy: 0.4320 - val_loss: 1.6443\n",
      "Epoch 13/150\n",
      "\u001b[1m30/30\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - accuracy: 0.4592 - loss: 1.6029 - val_accuracy: 0.4570 - val_loss: 1.5930\n",
      "Epoch 14/150\n",
      "\u001b[1m30/30\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - accuracy: 0.4831 - loss: 1.5527 - val_accuracy: 0.4840 - val_loss: 1.5398\n",
      "Epoch 15/150\n",
      "\u001b[1m30/30\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - accuracy: 0.5186 - loss: 1.4911 - val_accuracy: 0.5100 - val_loss: 1.4823\n",
      "Epoch 16/150\n",
      "\u001b[1m30/30\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - accuracy: 0.5326 - loss: 1.4378 - val_accuracy: 0.5290 - val_loss: 1.4276\n",
      "Epoch 17/150\n",
      "\u001b[1m30/30\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - accuracy: 0.5672 - loss: 1.3833 - val_accuracy: 0.5670 - val_loss: 1.3726\n",
      "Epoch 18/150\n",
      "\u001b[1m30/30\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - accuracy: 0.5928 - loss: 1.3291 - val_accuracy: 0.5850 - val_loss: 1.3214\n",
      "Epoch 19/150\n",
      "\u001b[1m30/30\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - accuracy: 0.6064 - loss: 1.2626 - val_accuracy: 0.5980 - val_loss: 1.2709\n",
      "Epoch 20/150\n",
      "\u001b[1m30/30\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - accuracy: 0.6319 - loss: 1.2143 - val_accuracy: 0.6170 - val_loss: 1.2225\n",
      "Epoch 21/150\n",
      "\u001b[1m30/30\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - accuracy: 0.6472 - loss: 1.1627 - val_accuracy: 0.6210 - val_loss: 1.1835\n",
      "Epoch 22/150\n",
      "\u001b[1m30/30\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - accuracy: 0.6491 - loss: 1.1257 - val_accuracy: 0.6420 - val_loss: 1.1402\n",
      "Epoch 23/150\n",
      "\u001b[1m30/30\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - accuracy: 0.6679 - loss: 1.0847 - val_accuracy: 0.6470 - val_loss: 1.1062\n",
      "Epoch 24/150\n",
      "\u001b[1m30/30\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - accuracy: 0.6690 - loss: 1.0564 - val_accuracy: 0.6580 - val_loss: 1.0723\n",
      "Epoch 25/150\n",
      "\u001b[1m30/30\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - accuracy: 0.6761 - loss: 1.0182 - val_accuracy: 0.6630 - val_loss: 1.0417\n",
      "Epoch 26/150\n",
      "\u001b[1m30/30\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - accuracy: 0.6848 - loss: 0.9926 - val_accuracy: 0.6680 - val_loss: 1.0164\n",
      "Epoch 27/150\n",
      "\u001b[1m30/30\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - accuracy: 0.6960 - loss: 0.9582 - val_accuracy: 0.6710 - val_loss: 0.9910\n",
      "Epoch 28/150\n",
      "\u001b[1m30/30\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - accuracy: 0.6994 - loss: 0.9405 - val_accuracy: 0.6880 - val_loss: 0.9662\n",
      "Epoch 29/150\n",
      "\u001b[1m30/30\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - accuracy: 0.7058 - loss: 0.9079 - val_accuracy: 0.6910 - val_loss: 0.9443\n",
      "Epoch 30/150\n",
      "\u001b[1m30/30\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - accuracy: 0.7138 - loss: 0.8912 - val_accuracy: 0.6910 - val_loss: 0.9259\n",
      "Epoch 31/150\n",
      "\u001b[1m30/30\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - accuracy: 0.7149 - loss: 0.8667 - val_accuracy: 0.6930 - val_loss: 0.9090\n",
      "Epoch 32/150\n",
      "\u001b[1m30/30\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - accuracy: 0.7199 - loss: 0.8476 - val_accuracy: 0.7020 - val_loss: 0.8938\n",
      "Epoch 33/150\n",
      "\u001b[1m30/30\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 9ms/step - accuracy: 0.7302 - loss: 0.8282 - val_accuracy: 0.7020 - val_loss: 0.8760\n",
      "Epoch 34/150\n",
      "\u001b[1m30/30\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - accuracy: 0.7333 - loss: 0.8073 - val_accuracy: 0.7050 - val_loss: 0.8635\n",
      "Epoch 35/150\n",
      "\u001b[1m30/30\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - accuracy: 0.7295 - loss: 0.8087 - val_accuracy: 0.7070 - val_loss: 0.8526\n",
      "Epoch 36/150\n",
      "\u001b[1m30/30\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - accuracy: 0.7366 - loss: 0.7802 - val_accuracy: 0.7110 - val_loss: 0.8405\n",
      "Epoch 37/150\n",
      "\u001b[1m30/30\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - accuracy: 0.7422 - loss: 0.7742 - val_accuracy: 0.7140 - val_loss: 0.8291\n",
      "Epoch 38/150\n",
      "\u001b[1m30/30\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - accuracy: 0.7504 - loss: 0.7549 - val_accuracy: 0.7150 - val_loss: 0.8181\n",
      "Epoch 39/150\n",
      "\u001b[1m30/30\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - accuracy: 0.7548 - loss: 0.7443 - val_accuracy: 0.7200 - val_loss: 0.8073\n",
      "Epoch 40/150\n",
      "\u001b[1m30/30\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - accuracy: 0.7562 - loss: 0.7259 - val_accuracy: 0.7120 - val_loss: 0.8027\n",
      "Epoch 41/150\n",
      "\u001b[1m30/30\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 9ms/step - accuracy: 0.7464 - loss: 0.7278 - val_accuracy: 0.7210 - val_loss: 0.7911\n",
      "Epoch 42/150\n",
      "\u001b[1m30/30\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 9ms/step - accuracy: 0.7532 - loss: 0.7193 - val_accuracy: 0.7200 - val_loss: 0.7840\n",
      "Epoch 43/150\n",
      "\u001b[1m30/30\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 10ms/step - accuracy: 0.7571 - loss: 0.7049 - val_accuracy: 0.7200 - val_loss: 0.7791\n",
      "Epoch 44/150\n",
      "\u001b[1m30/30\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - accuracy: 0.7566 - loss: 0.6956 - val_accuracy: 0.7250 - val_loss: 0.7729\n",
      "Epoch 45/150\n",
      "\u001b[1m30/30\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - accuracy: 0.7667 - loss: 0.6886 - val_accuracy: 0.7290 - val_loss: 0.7625\n",
      "Epoch 46/150\n",
      "\u001b[1m30/30\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - accuracy: 0.7664 - loss: 0.6771 - val_accuracy: 0.7240 - val_loss: 0.7616\n",
      "Epoch 47/150\n",
      "\u001b[1m30/30\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - accuracy: 0.7707 - loss: 0.6722 - val_accuracy: 0.7290 - val_loss: 0.7521\n",
      "Epoch 48/150\n",
      "\u001b[1m30/30\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - accuracy: 0.7781 - loss: 0.6558 - val_accuracy: 0.7270 - val_loss: 0.7463\n",
      "Epoch 49/150\n",
      "\u001b[1m30/30\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - accuracy: 0.7840 - loss: 0.6405 - val_accuracy: 0.7270 - val_loss: 0.7470\n",
      "Epoch 50/150\n",
      "\u001b[1m30/30\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - accuracy: 0.7798 - loss: 0.6428 - val_accuracy: 0.7270 - val_loss: 0.7446\n",
      "Epoch 51/150\n",
      "\u001b[1m30/30\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - accuracy: 0.7764 - loss: 0.6471 - val_accuracy: 0.7280 - val_loss: 0.7362\n",
      "Epoch 52/150\n",
      "\u001b[1m30/30\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - accuracy: 0.7851 - loss: 0.6325 - val_accuracy: 0.7310 - val_loss: 0.7299\n",
      "Epoch 53/150\n",
      "\u001b[1m30/30\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - accuracy: 0.7904 - loss: 0.6164 - val_accuracy: 0.7270 - val_loss: 0.7313\n",
      "Epoch 54/150\n",
      "\u001b[1m30/30\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - accuracy: 0.7856 - loss: 0.6120 - val_accuracy: 0.7320 - val_loss: 0.7224\n",
      "Epoch 55/150\n",
      "\u001b[1m30/30\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - accuracy: 0.7824 - loss: 0.6297 - val_accuracy: 0.7360 - val_loss: 0.7190\n",
      "Epoch 56/150\n",
      "\u001b[1m30/30\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - accuracy: 0.7804 - loss: 0.6118 - val_accuracy: 0.7350 - val_loss: 0.7148\n",
      "Epoch 57/150\n",
      "\u001b[1m30/30\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - accuracy: 0.7891 - loss: 0.6060 - val_accuracy: 0.7370 - val_loss: 0.7123\n",
      "Epoch 58/150\n",
      "\u001b[1m30/30\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - accuracy: 0.7870 - loss: 0.6115 - val_accuracy: 0.7300 - val_loss: 0.7106\n",
      "Epoch 59/150\n",
      "\u001b[1m30/30\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - accuracy: 0.7905 - loss: 0.5967 - val_accuracy: 0.7350 - val_loss: 0.7054\n",
      "Epoch 60/150\n",
      "\u001b[1m30/30\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - accuracy: 0.7986 - loss: 0.5873 - val_accuracy: 0.7350 - val_loss: 0.7076\n",
      "Epoch 61/150\n",
      "\u001b[1m30/30\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 9ms/step - accuracy: 0.7940 - loss: 0.5869 - val_accuracy: 0.7340 - val_loss: 0.7028\n",
      "Epoch 62/150\n",
      "\u001b[1m30/30\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 9ms/step - accuracy: 0.8029 - loss: 0.5750 - val_accuracy: 0.7360 - val_loss: 0.6977\n",
      "Epoch 63/150\n",
      "\u001b[1m30/30\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - accuracy: 0.8062 - loss: 0.5697 - val_accuracy: 0.7370 - val_loss: 0.6973\n",
      "Epoch 64/150\n",
      "\u001b[1m30/30\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - accuracy: 0.8013 - loss: 0.5637 - val_accuracy: 0.7450 - val_loss: 0.6928\n",
      "Epoch 65/150\n",
      "\u001b[1m30/30\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - accuracy: 0.8166 - loss: 0.5418 - val_accuracy: 0.7410 - val_loss: 0.6898\n",
      "Epoch 66/150\n",
      "\u001b[1m30/30\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - accuracy: 0.8032 - loss: 0.5693 - val_accuracy: 0.7440 - val_loss: 0.6904\n",
      "Epoch 67/150\n",
      "\u001b[1m30/30\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - accuracy: 0.8108 - loss: 0.5485 - val_accuracy: 0.7420 - val_loss: 0.6865\n",
      "Epoch 68/150\n",
      "\u001b[1m30/30\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - accuracy: 0.8121 - loss: 0.5483 - val_accuracy: 0.7400 - val_loss: 0.6869\n",
      "Epoch 69/150\n",
      "\u001b[1m30/30\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - accuracy: 0.8160 - loss: 0.5413 - val_accuracy: 0.7460 - val_loss: 0.6825\n",
      "Epoch 70/150\n",
      "\u001b[1m30/30\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - accuracy: 0.8222 - loss: 0.5382 - val_accuracy: 0.7450 - val_loss: 0.6811\n",
      "Epoch 71/150\n",
      "\u001b[1m30/30\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - accuracy: 0.8160 - loss: 0.5374 - val_accuracy: 0.7390 - val_loss: 0.6808\n",
      "Epoch 72/150\n",
      "\u001b[1m30/30\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - accuracy: 0.8215 - loss: 0.5288 - val_accuracy: 0.7410 - val_loss: 0.6791\n",
      "Epoch 73/150\n",
      "\u001b[1m30/30\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - accuracy: 0.8233 - loss: 0.5260 - val_accuracy: 0.7450 - val_loss: 0.6766\n",
      "Epoch 74/150\n",
      "\u001b[1m30/30\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - accuracy: 0.8259 - loss: 0.5161 - val_accuracy: 0.7400 - val_loss: 0.6769\n",
      "Epoch 75/150\n",
      "\u001b[1m30/30\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - accuracy: 0.8219 - loss: 0.5252 - val_accuracy: 0.7380 - val_loss: 0.6804\n",
      "Epoch 76/150\n",
      "\u001b[1m30/30\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - accuracy: 0.8301 - loss: 0.5059 - val_accuracy: 0.7420 - val_loss: 0.6732\n",
      "Epoch 77/150\n",
      "\u001b[1m30/30\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - accuracy: 0.8188 - loss: 0.5201 - val_accuracy: 0.7440 - val_loss: 0.6722\n",
      "Epoch 78/150\n",
      "\u001b[1m30/30\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - accuracy: 0.8234 - loss: 0.5154 - val_accuracy: 0.7380 - val_loss: 0.6763\n",
      "Epoch 79/150\n",
      "\u001b[1m30/30\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 9ms/step - accuracy: 0.8275 - loss: 0.5027 - val_accuracy: 0.7400 - val_loss: 0.6720\n",
      "Epoch 80/150\n",
      "\u001b[1m30/30\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 9ms/step - accuracy: 0.8296 - loss: 0.5008 - val_accuracy: 0.7410 - val_loss: 0.6717\n",
      "Epoch 81/150\n",
      "\u001b[1m30/30\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 10ms/step - accuracy: 0.8306 - loss: 0.4990 - val_accuracy: 0.7430 - val_loss: 0.6668\n",
      "Epoch 82/150\n",
      "\u001b[1m30/30\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 10ms/step - accuracy: 0.8332 - loss: 0.4884 - val_accuracy: 0.7440 - val_loss: 0.6675\n",
      "Epoch 83/150\n",
      "\u001b[1m30/30\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - accuracy: 0.8415 - loss: 0.4822 - val_accuracy: 0.7400 - val_loss: 0.6692\n",
      "Epoch 84/150\n",
      "\u001b[1m30/30\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 9ms/step - accuracy: 0.8381 - loss: 0.4819 - val_accuracy: 0.7410 - val_loss: 0.6662\n",
      "Epoch 85/150\n",
      "\u001b[1m30/30\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 9ms/step - accuracy: 0.8347 - loss: 0.4836 - val_accuracy: 0.7410 - val_loss: 0.6687\n",
      "Epoch 86/150\n",
      "\u001b[1m30/30\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - accuracy: 0.8406 - loss: 0.4758 - val_accuracy: 0.7420 - val_loss: 0.6654\n",
      "Epoch 87/150\n",
      "\u001b[1m30/30\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 9ms/step - accuracy: 0.8433 - loss: 0.4704 - val_accuracy: 0.7450 - val_loss: 0.6637\n",
      "Epoch 88/150\n",
      "\u001b[1m30/30\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - accuracy: 0.8442 - loss: 0.4732 - val_accuracy: 0.7400 - val_loss: 0.6654\n",
      "Epoch 89/150\n",
      "\u001b[1m30/30\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - accuracy: 0.8380 - loss: 0.4707 - val_accuracy: 0.7420 - val_loss: 0.6615\n",
      "Epoch 90/150\n",
      "\u001b[1m30/30\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 9ms/step - accuracy: 0.8517 - loss: 0.4433 - val_accuracy: 0.7390 - val_loss: 0.6646\n",
      "Epoch 91/150\n",
      "\u001b[1m30/30\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - accuracy: 0.8460 - loss: 0.4619 - val_accuracy: 0.7430 - val_loss: 0.6614\n",
      "Epoch 92/150\n",
      "\u001b[1m30/30\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 10ms/step - accuracy: 0.8515 - loss: 0.4439 - val_accuracy: 0.7390 - val_loss: 0.6644\n",
      "Epoch 93/150\n",
      "\u001b[1m30/30\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - accuracy: 0.8437 - loss: 0.4560 - val_accuracy: 0.7420 - val_loss: 0.6587\n",
      "Epoch 94/150\n",
      "\u001b[1m30/30\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - accuracy: 0.8453 - loss: 0.4606 - val_accuracy: 0.7410 - val_loss: 0.6561\n",
      "Epoch 95/150\n",
      "\u001b[1m30/30\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 10ms/step - accuracy: 0.8537 - loss: 0.4390 - val_accuracy: 0.7410 - val_loss: 0.6613\n",
      "Epoch 96/150\n",
      "\u001b[1m30/30\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 9ms/step - accuracy: 0.8501 - loss: 0.4383 - val_accuracy: 0.7370 - val_loss: 0.6623\n",
      "Epoch 97/150\n",
      "\u001b[1m30/30\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 9ms/step - accuracy: 0.8537 - loss: 0.4418 - val_accuracy: 0.7380 - val_loss: 0.6569\n",
      "Epoch 98/150\n",
      "\u001b[1m30/30\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - accuracy: 0.8593 - loss: 0.4314 - val_accuracy: 0.7420 - val_loss: 0.6615\n",
      "Epoch 99/150\n",
      "\u001b[1m30/30\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 9ms/step - accuracy: 0.8579 - loss: 0.4304 - val_accuracy: 0.7420 - val_loss: 0.6577\n",
      "Epoch 100/150\n",
      "\u001b[1m30/30\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 9ms/step - accuracy: 0.8507 - loss: 0.4346 - val_accuracy: 0.7390 - val_loss: 0.6602\n",
      "Epoch 101/150\n",
      "\u001b[1m30/30\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 10ms/step - accuracy: 0.8589 - loss: 0.4207 - val_accuracy: 0.7380 - val_loss: 0.6559\n",
      "Epoch 102/150\n",
      "\u001b[1m30/30\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 10ms/step - accuracy: 0.8606 - loss: 0.4176 - val_accuracy: 0.7420 - val_loss: 0.6575\n",
      "Epoch 103/150\n",
      "\u001b[1m30/30\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 9ms/step - accuracy: 0.8632 - loss: 0.4167 - val_accuracy: 0.7400 - val_loss: 0.6599\n",
      "Epoch 104/150\n",
      "\u001b[1m30/30\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 10ms/step - accuracy: 0.8628 - loss: 0.4098 - val_accuracy: 0.7410 - val_loss: 0.6562\n",
      "Epoch 105/150\n",
      "\u001b[1m30/30\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 9ms/step - accuracy: 0.8686 - loss: 0.4133 - val_accuracy: 0.7430 - val_loss: 0.6577\n",
      "Epoch 106/150\n",
      "\u001b[1m30/30\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 9ms/step - accuracy: 0.8656 - loss: 0.4145 - val_accuracy: 0.7420 - val_loss: 0.6568\n",
      "Epoch 107/150\n",
      "\u001b[1m30/30\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 10ms/step - accuracy: 0.8594 - loss: 0.4187 - val_accuracy: 0.7410 - val_loss: 0.6562\n",
      "Epoch 108/150\n",
      "\u001b[1m30/30\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 9ms/step - accuracy: 0.8720 - loss: 0.4010 - val_accuracy: 0.7420 - val_loss: 0.6571\n",
      "Epoch 109/150\n",
      "\u001b[1m30/30\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - accuracy: 0.8678 - loss: 0.4007 - val_accuracy: 0.7420 - val_loss: 0.6569\n",
      "Epoch 110/150\n",
      "\u001b[1m30/30\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 9ms/step - accuracy: 0.8713 - loss: 0.3896 - val_accuracy: 0.7430 - val_loss: 0.6573\n",
      "Epoch 111/150\n",
      "\u001b[1m30/30\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - accuracy: 0.8743 - loss: 0.3909 - val_accuracy: 0.7420 - val_loss: 0.6598\n",
      "Epoch 112/150\n",
      "\u001b[1m30/30\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 9ms/step - accuracy: 0.8701 - loss: 0.4048 - val_accuracy: 0.7410 - val_loss: 0.6583\n",
      "Epoch 113/150\n",
      "\u001b[1m30/30\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 9ms/step - accuracy: 0.8779 - loss: 0.3845 - val_accuracy: 0.7430 - val_loss: 0.6586\n",
      "Epoch 114/150\n",
      "\u001b[1m30/30\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - accuracy: 0.8791 - loss: 0.3795 - val_accuracy: 0.7410 - val_loss: 0.6586\n",
      "Epoch 115/150\n",
      "\u001b[1m30/30\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - accuracy: 0.8723 - loss: 0.3884 - val_accuracy: 0.7400 - val_loss: 0.6664\n",
      "Epoch 116/150\n",
      "\u001b[1m30/30\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 9ms/step - accuracy: 0.8774 - loss: 0.3824 - val_accuracy: 0.7420 - val_loss: 0.6616\n",
      "Epoch 117/150\n",
      "\u001b[1m30/30\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - accuracy: 0.8838 - loss: 0.3692 - val_accuracy: 0.7410 - val_loss: 0.6615\n",
      "Epoch 118/150\n",
      "\u001b[1m30/30\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 9ms/step - accuracy: 0.8791 - loss: 0.3732 - val_accuracy: 0.7430 - val_loss: 0.6605\n",
      "Epoch 119/150\n",
      "\u001b[1m30/30\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - accuracy: 0.8833 - loss: 0.3675 - val_accuracy: 0.7410 - val_loss: 0.6606\n",
      "Epoch 120/150\n",
      "\u001b[1m30/30\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 10ms/step - accuracy: 0.8832 - loss: 0.3764 - val_accuracy: 0.7420 - val_loss: 0.6673\n",
      "Epoch 121/150\n",
      "\u001b[1m30/30\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 12ms/step - accuracy: 0.8881 - loss: 0.3598 - val_accuracy: 0.7390 - val_loss: 0.6607\n",
      "Epoch 122/150\n",
      "\u001b[1m30/30\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 9ms/step - accuracy: 0.8890 - loss: 0.3602 - val_accuracy: 0.7410 - val_loss: 0.6615\n",
      "Epoch 123/150\n",
      "\u001b[1m30/30\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 11ms/step - accuracy: 0.8879 - loss: 0.3596 - val_accuracy: 0.7410 - val_loss: 0.6642\n",
      "Epoch 124/150\n",
      "\u001b[1m30/30\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 11ms/step - accuracy: 0.8871 - loss: 0.3508 - val_accuracy: 0.7450 - val_loss: 0.6684\n",
      "Epoch 125/150\n",
      "\u001b[1m30/30\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 11ms/step - accuracy: 0.8838 - loss: 0.3727 - val_accuracy: 0.7430 - val_loss: 0.6617\n",
      "Epoch 126/150\n",
      "\u001b[1m30/30\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step - accuracy: 0.8882 - loss: 0.3563 - val_accuracy: 0.7380 - val_loss: 0.6695\n",
      "Epoch 127/150\n",
      "\u001b[1m30/30\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 11ms/step - accuracy: 0.8820 - loss: 0.3543 - val_accuracy: 0.7430 - val_loss: 0.6635\n",
      "Epoch 128/150\n",
      "\u001b[1m30/30\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 10ms/step - accuracy: 0.8889 - loss: 0.3538 - val_accuracy: 0.7390 - val_loss: 0.6642\n",
      "Epoch 129/150\n",
      "\u001b[1m30/30\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 9ms/step - accuracy: 0.8925 - loss: 0.3500 - val_accuracy: 0.7430 - val_loss: 0.6649\n",
      "Epoch 130/150\n",
      "\u001b[1m30/30\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 10ms/step - accuracy: 0.8955 - loss: 0.3329 - val_accuracy: 0.7420 - val_loss: 0.6628\n",
      "Epoch 131/150\n",
      "\u001b[1m30/30\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 12ms/step - accuracy: 0.8971 - loss: 0.3410 - val_accuracy: 0.7450 - val_loss: 0.6702\n",
      "Epoch 132/150\n",
      "\u001b[1m30/30\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 12ms/step - accuracy: 0.8943 - loss: 0.3368 - val_accuracy: 0.7400 - val_loss: 0.6684\n",
      "Epoch 133/150\n",
      "\u001b[1m30/30\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 9ms/step - accuracy: 0.8911 - loss: 0.3342 - val_accuracy: 0.7430 - val_loss: 0.6673\n",
      "Epoch 134/150\n",
      "\u001b[1m30/30\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 10ms/step - accuracy: 0.8971 - loss: 0.3385 - val_accuracy: 0.7430 - val_loss: 0.6725\n",
      "Epoch 135/150\n",
      "\u001b[1m30/30\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 10ms/step - accuracy: 0.8999 - loss: 0.3306 - val_accuracy: 0.7390 - val_loss: 0.6693\n",
      "Epoch 136/150\n",
      "\u001b[1m30/30\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - accuracy: 0.8992 - loss: 0.3301 - val_accuracy: 0.7400 - val_loss: 0.6686\n",
      "Epoch 137/150\n",
      "\u001b[1m30/30\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 9ms/step - accuracy: 0.9014 - loss: 0.3297 - val_accuracy: 0.7470 - val_loss: 0.6700\n",
      "Epoch 138/150\n",
      "\u001b[1m30/30\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 10ms/step - accuracy: 0.9002 - loss: 0.3250 - val_accuracy: 0.7410 - val_loss: 0.6766\n",
      "Epoch 139/150\n",
      "\u001b[1m30/30\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 9ms/step - accuracy: 0.8950 - loss: 0.3303 - val_accuracy: 0.7380 - val_loss: 0.6730\n",
      "Epoch 140/150\n",
      "\u001b[1m30/30\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 9ms/step - accuracy: 0.9041 - loss: 0.3273 - val_accuracy: 0.7420 - val_loss: 0.6718\n",
      "Epoch 141/150\n",
      "\u001b[1m30/30\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 10ms/step - accuracy: 0.9103 - loss: 0.3057 - val_accuracy: 0.7420 - val_loss: 0.6737\n",
      "Epoch 142/150\n",
      "\u001b[1m30/30\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - accuracy: 0.9058 - loss: 0.3125 - val_accuracy: 0.7410 - val_loss: 0.6727\n",
      "Epoch 143/150\n",
      "\u001b[1m30/30\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 9ms/step - accuracy: 0.9065 - loss: 0.3133 - val_accuracy: 0.7390 - val_loss: 0.6776\n",
      "Epoch 144/150\n",
      "\u001b[1m30/30\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - accuracy: 0.9040 - loss: 0.3139 - val_accuracy: 0.7380 - val_loss: 0.6770\n",
      "Epoch 145/150\n",
      "\u001b[1m30/30\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 9ms/step - accuracy: 0.9069 - loss: 0.3114 - val_accuracy: 0.7410 - val_loss: 0.6789\n",
      "Epoch 146/150\n",
      "\u001b[1m30/30\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 10ms/step - accuracy: 0.9091 - loss: 0.3041 - val_accuracy: 0.7440 - val_loss: 0.6791\n",
      "Epoch 147/150\n",
      "\u001b[1m30/30\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 9ms/step - accuracy: 0.9073 - loss: 0.3089 - val_accuracy: 0.7420 - val_loss: 0.6802\n",
      "Epoch 148/150\n",
      "\u001b[1m30/30\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 9ms/step - accuracy: 0.9083 - loss: 0.2987 - val_accuracy: 0.7420 - val_loss: 0.6813\n",
      "Epoch 149/150\n",
      "\u001b[1m30/30\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - accuracy: 0.9110 - loss: 0.2991 - val_accuracy: 0.7420 - val_loss: 0.6799\n",
      "Epoch 150/150\n",
      "\u001b[1m30/30\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 10ms/step - accuracy: 0.9135 - loss: 0.2978 - val_accuracy: 0.7420 - val_loss: 0.6843\n"
     ]
    }
   ],
   "source": [
    "# Train the model\n",
    "baseline_model_val = baseline_model.fit(X_train_tokens, y_train_lb, epochs=150, batch_size=256, validation_data=(X_val_tokens, y_val_lb))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model Performance\n",
    "\n",
    "The attribute `.history` (stored as a dictionary) contains four entries now: one per metric that was being monitored during training and validation. Print the keys of this dictionary for confirmation: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dict_keys(['accuracy', 'loss', 'val_accuracy', 'val_loss'])"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Access the history attribute and store the dictionary\n",
    "baseline_model_val_dict = baseline_model_val.history\n",
    "\n",
    "# Print the keys\n",
    "baseline_model_val_dict.keys()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Evaluate this model on the training data: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m235/235\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.9153 - loss: 0.2933\n",
      "----------\n",
      "Training Loss: 0.296 \n",
      "Training Accuracy: 0.913\n"
     ]
    }
   ],
   "source": [
    "results_train = baseline_model.evaluate(X_train_tokens, y_train_lb)\n",
    "print('----------')\n",
    "print(f'Training Loss: {results_train[0]:.3} \\nTraining Accuracy: {results_train[1]:.3}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Evaluate this model on the test data: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m47/47\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.7984 - loss: 0.5937\n",
      "----------\n",
      "Test Loss: 0.612 \n",
      "Test Accuracy: 0.791\n"
     ]
    }
   ],
   "source": [
    "results_test = baseline_model.evaluate(X_test_tokens, y_test_lb)\n",
    "print('----------')\n",
    "print(f'Test Loss: {results_test[0]:.3} \\nTest Accuracy: {results_test[1]:.3}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Plot the Results \n",
    "\n",
    "Plot the loss versus the number of epochs. Be sure to include the training and the validation loss in the same plot. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Loss vs number of epochs with train and validation sets\n",
    "plt.plot(baseline_model_val.history['loss'], label='Training Loss')\n",
    "plt.plot(baseline_model_val.history['val_loss'], label='Validation Loss')\n",
    "plt.title('Loss vs Number of Epochs')\n",
    "plt.xlabel('Epochs')\n",
    "plt.ylabel('Loss')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create a second plot comparing training and validation accuracy to the number of epochs. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Accuracy vs number of epochs with train and validation sets\n",
    "plt.plot(baseline_model_val.history['accuracy'])\n",
    "plt.plot(baseline_model_val.history['val_accuracy'])\n",
    "plt.title('Accuracy Vs Number of Epochs')\n",
    "plt.ylabel('Accuracy')\n",
    "plt.xlabel('Epoch')\n",
    "plt.legend(['Train', 'Validation'], loc='lower right')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Did you notice an interesting pattern here? Although the training accuracy keeps increasing when going through more epochs, and the training loss keeps decreasing, the validation accuracy and loss don't necessarily do the same. After a certain point, validation accuracy keeps swinging, which means that you're probably **overfitting** the model to the training data when you train for many epochs past a certain dropoff point. Let's tackle this now. You will now specify an early stopping point when training your model. \n",
    "\n",
    "\n",
    "## Early Stopping\n",
    "\n",
    "Overfitting neural networks is something you **_want_** to avoid at all costs. However, it's not possible to know in advance how many *epochs* you need to train your model on, and running the model multiple times with varying number of *epochs* maybe helpful, but is a time-consuming process. \n",
    "\n",
    "We've defined a model with the same architecture as above. This time specify an early stopping point when training the model. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "random.seed(123)\n",
    "model_2 = models.Sequential()\n",
    "model_2.add(layers.Dense(50, activation='relu', input_shape=(2000,)))\n",
    "model_2.add(layers.Dense(25, activation='relu'))\n",
    "model_2.add(layers.Dense(7, activation='softmax'))\n",
    "\n",
    "model_2.compile(optimizer='SGD', \n",
    "                loss='categorical_crossentropy', \n",
    "                metrics=['acc'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Import `EarlyStopping` and `ModelCheckpoint` from `keras.callbacks` \n",
    "- Define a list, `early_stopping`: \n",
    "  - Monitor `'val_loss'` and continue training for 10 epochs before stopping \n",
    "  - Save the best model while monitoring `'val_loss'` \n",
    " \n",
    "> If you need help, consult [documentation](https://keras.io/callbacks/).   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import EarlyStopping and ModelCheckpoint\n",
    "from tensorflow.keras.callbacks import EarlyStopping, ModelCheckpoint\n",
    "\n",
    "# Define the callbacks\n",
    "early_stopping = EarlyStopping(monitor='val_loss', patience=10)\n",
    "model_checkpoint = ModelCheckpoint(filepath='model.{epoch:02d}-{val_loss:.2f}.keras', monitor='val_loss', save_best_only=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Train `model_2`. Make sure you set the `callbacks` argument to `early_stopping`. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/150\n",
      "\u001b[1m30/30\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 9ms/step - acc: 0.8981 - loss: 0.3399 - val_acc: 0.7440 - val_loss: 0.6656\n",
      "Epoch 2/150\n",
      "\u001b[1m30/30\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - acc: 0.8946 - loss: 0.3395 - val_acc: 0.7450 - val_loss: 0.6654\n",
      "Epoch 3/150\n",
      "\u001b[1m30/30\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - acc: 0.8965 - loss: 0.3414 - val_acc: 0.7410 - val_loss: 0.6685\n",
      "Epoch 4/150\n",
      "\u001b[1m30/30\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - acc: 0.8967 - loss: 0.3374 - val_acc: 0.7450 - val_loss: 0.6689\n",
      "Epoch 5/150\n",
      "\u001b[1m30/30\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 11ms/step - acc: 0.8973 - loss: 0.3357 - val_acc: 0.7460 - val_loss: 0.6653\n",
      "Epoch 6/150\n",
      "\u001b[1m30/30\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - acc: 0.9015 - loss: 0.3282 - val_acc: 0.7450 - val_loss: 0.6702\n",
      "Epoch 7/150\n",
      "\u001b[1m30/30\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - acc: 0.9004 - loss: 0.3238 - val_acc: 0.7410 - val_loss: 0.6744\n",
      "Epoch 8/150\n",
      "\u001b[1m30/30\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - acc: 0.9007 - loss: 0.3257 - val_acc: 0.7460 - val_loss: 0.6739\n",
      "Epoch 9/150\n",
      "\u001b[1m30/30\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 9ms/step - acc: 0.9016 - loss: 0.3259 - val_acc: 0.7410 - val_loss: 0.6725\n",
      "Epoch 10/150\n",
      "\u001b[1m30/30\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - acc: 0.9019 - loss: 0.3213 - val_acc: 0.7460 - val_loss: 0.6685\n",
      "Epoch 11/150\n",
      "\u001b[1m30/30\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - acc: 0.9036 - loss: 0.3218 - val_acc: 0.7410 - val_loss: 0.6746\n",
      "Epoch 12/150\n",
      "\u001b[1m30/30\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - acc: 0.9004 - loss: 0.3212 - val_acc: 0.7400 - val_loss: 0.6768\n",
      "Epoch 13/150\n",
      "\u001b[1m30/30\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 11ms/step - acc: 0.9032 - loss: 0.3101 - val_acc: 0.7420 - val_loss: 0.6750\n",
      "Epoch 14/150\n",
      "\u001b[1m30/30\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - acc: 0.9038 - loss: 0.3202 - val_acc: 0.7440 - val_loss: 0.6726\n",
      "Epoch 15/150\n",
      "\u001b[1m30/30\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - acc: 0.9073 - loss: 0.3131 - val_acc: 0.7410 - val_loss: 0.6770\n"
     ]
    }
   ],
   "source": [
    "model = model_2.fit(X_train_tokens, y_train_lb, epochs=150, batch_size=256, validation_data=(X_val_tokens, y_val_lb), callbacks=[early_stopping, model_checkpoint])\n",
    "\n",
    "model_2_val = model.history"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Load the best (saved) model. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the best (saved) model\n",
    "from tensorflow.keras.models import load_model\n",
    "\n",
    "saved_model = load_model('best_model.keras') "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, use this model to to calculate the training and test accuracy: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m235/235\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - acc: 0.8738 - loss: 0.4006\n",
      "Training Loss: 0.403 \n",
      "Training Accuracy: 0.869\n",
      "----------\n",
      "\u001b[1m47/47\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - acc: 0.7916 - loss: 0.5794\n",
      "Test Loss: 0.596 \n",
      "Test Accuracy: 0.786\n"
     ]
    }
   ],
   "source": [
    "results_train = saved_model.evaluate(X_train_tokens, y_train_lb)\n",
    "print(f'Training Loss: {results_train[0]:.3} \\nTraining Accuracy: {results_train[1]:.3}')\n",
    "\n",
    "print('----------')\n",
    "\n",
    "results_test = saved_model.evaluate(X_test_tokens, y_test_lb)\n",
    "print(f'Test Loss: {results_test[0]:.3} \\nTest Accuracy: {results_test[1]:.3}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Nicely done! Did you notice that the model didn't train for all 150 epochs? You reduced your training time. \n",
    "\n",
    "Now, take a look at how regularization techniques can further improve your model performance. \n",
    "\n",
    "## L2 Regularization \n",
    "\n",
    "First, take a look at L2 regularization. Keras makes L2 regularization easy. Simply add the `kernel_regularizer=keras.regularizers.l2(lambda_coeff)` parameter to any model layer. The `lambda_coeff` parameter determines the strength of the regularization you wish to perform. \n",
    "\n",
    "- Use 2 hidden layers with 50 units in the first and 25 in the second layer, both with `'relu'` activation functions \n",
    "- Add L2 regularization to both the hidden layers with 0.005 as the `lambda_coeff` "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/150\n",
      "\u001b[1m30/30\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 14ms/step - acc: 0.1218 - loss: 2.6179 - val_acc: 0.1480 - val_loss: 2.6084\n",
      "Epoch 2/150\n",
      "\u001b[1m30/30\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - acc: 0.1706 - loss: 2.5927 - val_acc: 0.1880 - val_loss: 2.5900\n",
      "Epoch 3/150\n",
      "\u001b[1m30/30\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - acc: 0.2157 - loss: 2.5734 - val_acc: 0.2110 - val_loss: 2.5726\n",
      "Epoch 4/150\n",
      "\u001b[1m30/30\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 9ms/step - acc: 0.2310 - loss: 2.5572 - val_acc: 0.2440 - val_loss: 2.5537\n",
      "Epoch 5/150\n",
      "\u001b[1m30/30\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 9ms/step - acc: 0.2575 - loss: 2.5351 - val_acc: 0.2660 - val_loss: 2.5313\n",
      "Epoch 6/150\n",
      "\u001b[1m30/30\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - acc: 0.2784 - loss: 2.5138 - val_acc: 0.2940 - val_loss: 2.5065\n",
      "Epoch 7/150\n",
      "\u001b[1m30/30\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - acc: 0.3031 - loss: 2.4854 - val_acc: 0.3060 - val_loss: 2.4786\n",
      "Epoch 8/150\n",
      "\u001b[1m30/30\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - acc: 0.3226 - loss: 2.4579 - val_acc: 0.3260 - val_loss: 2.4452\n",
      "Epoch 9/150\n",
      "\u001b[1m30/30\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 9ms/step - acc: 0.3432 - loss: 2.4191 - val_acc: 0.3460 - val_loss: 2.4081\n",
      "Epoch 10/150\n",
      "\u001b[1m30/30\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - acc: 0.3636 - loss: 2.3811 - val_acc: 0.3780 - val_loss: 2.3661\n",
      "Epoch 11/150\n",
      "\u001b[1m30/30\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - acc: 0.3866 - loss: 2.3370 - val_acc: 0.4050 - val_loss: 2.3203\n",
      "Epoch 12/150\n",
      "\u001b[1m30/30\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - acc: 0.4173 - loss: 2.2855 - val_acc: 0.4350 - val_loss: 2.2716\n",
      "Epoch 13/150\n",
      "\u001b[1m30/30\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - acc: 0.4507 - loss: 2.2358 - val_acc: 0.4560 - val_loss: 2.2214\n",
      "Epoch 14/150\n",
      "\u001b[1m30/30\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - acc: 0.4779 - loss: 2.1812 - val_acc: 0.4760 - val_loss: 2.1692\n",
      "Epoch 15/150\n",
      "\u001b[1m30/30\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - acc: 0.5079 - loss: 2.1263 - val_acc: 0.5020 - val_loss: 2.1132\n",
      "Epoch 16/150\n",
      "\u001b[1m30/30\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - acc: 0.5301 - loss: 2.0711 - val_acc: 0.5290 - val_loss: 2.0571\n",
      "Epoch 17/150\n",
      "\u001b[1m30/30\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 9ms/step - acc: 0.5596 - loss: 2.0078 - val_acc: 0.5530 - val_loss: 2.0017\n",
      "Epoch 18/150\n",
      "\u001b[1m30/30\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - acc: 0.5837 - loss: 1.9614 - val_acc: 0.5750 - val_loss: 1.9508\n",
      "Epoch 19/150\n",
      "\u001b[1m30/30\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 9ms/step - acc: 0.6039 - loss: 1.9054 - val_acc: 0.5870 - val_loss: 1.9064\n",
      "Epoch 20/150\n",
      "\u001b[1m30/30\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - acc: 0.6210 - loss: 1.8528 - val_acc: 0.6080 - val_loss: 1.8550\n",
      "Epoch 21/150\n",
      "\u001b[1m30/30\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - acc: 0.6343 - loss: 1.8005 - val_acc: 0.6380 - val_loss: 1.8087\n",
      "Epoch 22/150\n",
      "\u001b[1m30/30\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - acc: 0.6466 - loss: 1.7588 - val_acc: 0.6390 - val_loss: 1.7717\n",
      "Epoch 23/150\n",
      "\u001b[1m30/30\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - acc: 0.6555 - loss: 1.7233 - val_acc: 0.6480 - val_loss: 1.7331\n",
      "Epoch 24/150\n",
      "\u001b[1m30/30\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - acc: 0.6767 - loss: 1.6700 - val_acc: 0.6660 - val_loss: 1.7014\n",
      "Epoch 25/150\n",
      "\u001b[1m30/30\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - acc: 0.6775 - loss: 1.6501 - val_acc: 0.6610 - val_loss: 1.6676\n",
      "Epoch 26/150\n",
      "\u001b[1m30/30\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - acc: 0.6868 - loss: 1.6147 - val_acc: 0.6740 - val_loss: 1.6402\n",
      "Epoch 27/150\n",
      "\u001b[1m30/30\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - acc: 0.6823 - loss: 1.5972 - val_acc: 0.6830 - val_loss: 1.6112\n",
      "Epoch 28/150\n",
      "\u001b[1m30/30\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - acc: 0.7030 - loss: 1.5565 - val_acc: 0.6760 - val_loss: 1.5865\n",
      "Epoch 29/150\n",
      "\u001b[1m30/30\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - acc: 0.7051 - loss: 1.5398 - val_acc: 0.6870 - val_loss: 1.5672\n",
      "Epoch 30/150\n",
      "\u001b[1m30/30\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - acc: 0.7131 - loss: 1.5057 - val_acc: 0.6850 - val_loss: 1.5417\n",
      "Epoch 31/150\n",
      "\u001b[1m30/30\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - acc: 0.7149 - loss: 1.4893 - val_acc: 0.6950 - val_loss: 1.5218\n",
      "Epoch 32/150\n",
      "\u001b[1m30/30\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - acc: 0.7176 - loss: 1.4644 - val_acc: 0.7100 - val_loss: 1.5019\n",
      "Epoch 33/150\n",
      "\u001b[1m30/30\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - acc: 0.7148 - loss: 1.4498 - val_acc: 0.7090 - val_loss: 1.4842\n",
      "Epoch 34/150\n",
      "\u001b[1m30/30\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - acc: 0.7322 - loss: 1.4258 - val_acc: 0.7080 - val_loss: 1.4712\n",
      "Epoch 35/150\n",
      "\u001b[1m30/30\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - acc: 0.7264 - loss: 1.4153 - val_acc: 0.7090 - val_loss: 1.4600\n",
      "Epoch 36/150\n",
      "\u001b[1m30/30\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - acc: 0.7286 - loss: 1.4022 - val_acc: 0.7150 - val_loss: 1.4407\n",
      "Epoch 37/150\n",
      "\u001b[1m30/30\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - acc: 0.7344 - loss: 1.3805 - val_acc: 0.7150 - val_loss: 1.4276\n",
      "Epoch 38/150\n",
      "\u001b[1m30/30\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - acc: 0.7400 - loss: 1.3755 - val_acc: 0.7100 - val_loss: 1.4153\n",
      "Epoch 39/150\n",
      "\u001b[1m30/30\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - acc: 0.7561 - loss: 1.3335 - val_acc: 0.7100 - val_loss: 1.4071\n",
      "Epoch 40/150\n",
      "\u001b[1m30/30\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - acc: 0.7455 - loss: 1.3494 - val_acc: 0.7140 - val_loss: 1.3953\n",
      "Epoch 41/150\n",
      "\u001b[1m30/30\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - acc: 0.7521 - loss: 1.3306 - val_acc: 0.7210 - val_loss: 1.3841\n",
      "Epoch 42/150\n",
      "\u001b[1m30/30\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - acc: 0.7642 - loss: 1.3139 - val_acc: 0.7180 - val_loss: 1.3753\n",
      "Epoch 43/150\n",
      "\u001b[1m30/30\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 9ms/step - acc: 0.7608 - loss: 1.3032 - val_acc: 0.7250 - val_loss: 1.3631\n",
      "Epoch 44/150\n",
      "\u001b[1m30/30\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - acc: 0.7601 - loss: 1.2950 - val_acc: 0.7250 - val_loss: 1.3551\n",
      "Epoch 45/150\n",
      "\u001b[1m30/30\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 9ms/step - acc: 0.7654 - loss: 1.2859 - val_acc: 0.7250 - val_loss: 1.3467\n",
      "Epoch 46/150\n",
      "\u001b[1m30/30\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - acc: 0.7637 - loss: 1.2777 - val_acc: 0.7280 - val_loss: 1.3399\n",
      "Epoch 47/150\n",
      "\u001b[1m30/30\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - acc: 0.7746 - loss: 1.2618 - val_acc: 0.7270 - val_loss: 1.3296\n",
      "Epoch 48/150\n",
      "\u001b[1m30/30\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - acc: 0.7634 - loss: 1.2644 - val_acc: 0.7300 - val_loss: 1.3284\n",
      "Epoch 49/150\n",
      "\u001b[1m30/30\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - acc: 0.7695 - loss: 1.2420 - val_acc: 0.7330 - val_loss: 1.3169\n",
      "Epoch 50/150\n",
      "\u001b[1m30/30\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - acc: 0.7800 - loss: 1.2275 - val_acc: 0.7370 - val_loss: 1.3098\n",
      "Epoch 51/150\n",
      "\u001b[1m30/30\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - acc: 0.7773 - loss: 1.2179 - val_acc: 0.7350 - val_loss: 1.3049\n",
      "Epoch 52/150\n",
      "\u001b[1m30/30\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - acc: 0.7726 - loss: 1.2278 - val_acc: 0.7320 - val_loss: 1.2976\n",
      "Epoch 53/150\n",
      "\u001b[1m30/30\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 9ms/step - acc: 0.7725 - loss: 1.2169 - val_acc: 0.7350 - val_loss: 1.2888\n",
      "Epoch 54/150\n",
      "\u001b[1m30/30\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 9ms/step - acc: 0.7820 - loss: 1.2000 - val_acc: 0.7280 - val_loss: 1.2843\n",
      "Epoch 55/150\n",
      "\u001b[1m30/30\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - acc: 0.7773 - loss: 1.2044 - val_acc: 0.7350 - val_loss: 1.2775\n",
      "Epoch 56/150\n",
      "\u001b[1m30/30\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - acc: 0.7954 - loss: 1.1720 - val_acc: 0.7300 - val_loss: 1.2766\n",
      "Epoch 57/150\n",
      "\u001b[1m30/30\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - acc: 0.7885 - loss: 1.1754 - val_acc: 0.7280 - val_loss: 1.2672\n",
      "Epoch 58/150\n",
      "\u001b[1m30/30\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - acc: 0.7879 - loss: 1.1694 - val_acc: 0.7280 - val_loss: 1.2628\n",
      "Epoch 59/150\n",
      "\u001b[1m30/30\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - acc: 0.7879 - loss: 1.1713 - val_acc: 0.7360 - val_loss: 1.2568\n",
      "Epoch 60/150\n",
      "\u001b[1m30/30\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - acc: 0.7917 - loss: 1.1644 - val_acc: 0.7380 - val_loss: 1.2517\n",
      "Epoch 61/150\n",
      "\u001b[1m30/30\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - acc: 0.7992 - loss: 1.1382 - val_acc: 0.7260 - val_loss: 1.2530\n",
      "Epoch 62/150\n",
      "\u001b[1m30/30\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 10ms/step - acc: 0.7977 - loss: 1.1376 - val_acc: 0.7470 - val_loss: 1.2438\n",
      "Epoch 63/150\n",
      "\u001b[1m30/30\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - acc: 0.8041 - loss: 1.1200 - val_acc: 0.7360 - val_loss: 1.2398\n",
      "Epoch 64/150\n",
      "\u001b[1m30/30\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - acc: 0.7964 - loss: 1.1350 - val_acc: 0.7380 - val_loss: 1.2327\n",
      "Epoch 65/150\n",
      "\u001b[1m30/30\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - acc: 0.7985 - loss: 1.1191 - val_acc: 0.7440 - val_loss: 1.2287\n",
      "Epoch 66/150\n",
      "\u001b[1m30/30\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - acc: 0.8032 - loss: 1.1115 - val_acc: 0.7420 - val_loss: 1.2245\n",
      "Epoch 67/150\n",
      "\u001b[1m30/30\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - acc: 0.8025 - loss: 1.1125 - val_acc: 0.7410 - val_loss: 1.2213\n",
      "Epoch 68/150\n",
      "\u001b[1m30/30\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - acc: 0.8092 - loss: 1.1025 - val_acc: 0.7400 - val_loss: 1.2188\n",
      "Epoch 69/150\n",
      "\u001b[1m30/30\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - acc: 0.8057 - loss: 1.0977 - val_acc: 0.7360 - val_loss: 1.2188\n",
      "Epoch 70/150\n",
      "\u001b[1m30/30\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - acc: 0.8103 - loss: 1.0863 - val_acc: 0.7410 - val_loss: 1.2116\n",
      "Epoch 71/150\n",
      "\u001b[1m30/30\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - acc: 0.8033 - loss: 1.1066 - val_acc: 0.7370 - val_loss: 1.2113\n",
      "Epoch 72/150\n",
      "\u001b[1m30/30\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 9ms/step - acc: 0.8092 - loss: 1.0859 - val_acc: 0.7430 - val_loss: 1.2022\n",
      "Epoch 73/150\n",
      "\u001b[1m30/30\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - acc: 0.8172 - loss: 1.0725 - val_acc: 0.7450 - val_loss: 1.1989\n",
      "Epoch 74/150\n",
      "\u001b[1m30/30\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - acc: 0.8149 - loss: 1.0721 - val_acc: 0.7430 - val_loss: 1.1923\n",
      "Epoch 75/150\n",
      "\u001b[1m30/30\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - acc: 0.8162 - loss: 1.0667 - val_acc: 0.7460 - val_loss: 1.1900\n",
      "Epoch 76/150\n",
      "\u001b[1m30/30\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - acc: 0.8203 - loss: 1.0620 - val_acc: 0.7410 - val_loss: 1.1877\n",
      "Epoch 77/150\n",
      "\u001b[1m30/30\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - acc: 0.8222 - loss: 1.0475 - val_acc: 0.7440 - val_loss: 1.1839\n",
      "Epoch 78/150\n",
      "\u001b[1m30/30\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - acc: 0.8160 - loss: 1.0524 - val_acc: 0.7500 - val_loss: 1.1834\n",
      "Epoch 79/150\n",
      "\u001b[1m30/30\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - acc: 0.8174 - loss: 1.0505 - val_acc: 0.7450 - val_loss: 1.1790\n",
      "Epoch 80/150\n",
      "\u001b[1m30/30\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - acc: 0.8158 - loss: 1.0547 - val_acc: 0.7480 - val_loss: 1.1726\n",
      "Epoch 81/150\n",
      "\u001b[1m30/30\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - acc: 0.8298 - loss: 1.0330 - val_acc: 0.7400 - val_loss: 1.1736\n",
      "Epoch 82/150\n",
      "\u001b[1m30/30\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - acc: 0.8311 - loss: 1.0190 - val_acc: 0.7450 - val_loss: 1.1681\n",
      "Epoch 83/150\n",
      "\u001b[1m30/30\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - acc: 0.8247 - loss: 1.0288 - val_acc: 0.7440 - val_loss: 1.1695\n",
      "Epoch 84/150\n",
      "\u001b[1m30/30\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 10ms/step - acc: 0.8292 - loss: 1.0226 - val_acc: 0.7480 - val_loss: 1.1621\n",
      "Epoch 85/150\n",
      "\u001b[1m30/30\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - acc: 0.8299 - loss: 1.0154 - val_acc: 0.7460 - val_loss: 1.1601\n",
      "Epoch 86/150\n",
      "\u001b[1m30/30\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - acc: 0.8298 - loss: 1.0188 - val_acc: 0.7450 - val_loss: 1.1573\n",
      "Epoch 87/150\n",
      "\u001b[1m30/30\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - acc: 0.8271 - loss: 1.0227 - val_acc: 0.7480 - val_loss: 1.1540\n",
      "Epoch 88/150\n",
      "\u001b[1m30/30\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - acc: 0.8304 - loss: 1.0106 - val_acc: 0.7480 - val_loss: 1.1520\n",
      "Epoch 89/150\n",
      "\u001b[1m30/30\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - acc: 0.8379 - loss: 0.9901 - val_acc: 0.7460 - val_loss: 1.1483\n",
      "Epoch 90/150\n",
      "\u001b[1m30/30\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - acc: 0.8371 - loss: 0.9825 - val_acc: 0.7500 - val_loss: 1.1459\n",
      "Epoch 91/150\n",
      "\u001b[1m30/30\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - acc: 0.8397 - loss: 0.9914 - val_acc: 0.7450 - val_loss: 1.1426\n",
      "Epoch 92/150\n",
      "\u001b[1m30/30\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - acc: 0.8371 - loss: 0.9768 - val_acc: 0.7480 - val_loss: 1.1424\n",
      "Epoch 93/150\n",
      "\u001b[1m30/30\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 9ms/step - acc: 0.8380 - loss: 0.9772 - val_acc: 0.7460 - val_loss: 1.1383\n",
      "Epoch 94/150\n",
      "\u001b[1m30/30\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - acc: 0.8432 - loss: 0.9674 - val_acc: 0.7500 - val_loss: 1.1392\n",
      "Epoch 95/150\n",
      "\u001b[1m30/30\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - acc: 0.8415 - loss: 0.9610 - val_acc: 0.7450 - val_loss: 1.1310\n",
      "Epoch 96/150\n",
      "\u001b[1m30/30\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 9ms/step - acc: 0.8512 - loss: 0.9525 - val_acc: 0.7450 - val_loss: 1.1309\n",
      "Epoch 97/150\n",
      "\u001b[1m30/30\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - acc: 0.8423 - loss: 0.9595 - val_acc: 0.7440 - val_loss: 1.1335\n",
      "Epoch 98/150\n",
      "\u001b[1m30/30\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - acc: 0.8533 - loss: 0.9445 - val_acc: 0.7470 - val_loss: 1.1259\n",
      "Epoch 99/150\n",
      "\u001b[1m30/30\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - acc: 0.8423 - loss: 0.9474 - val_acc: 0.7470 - val_loss: 1.1288\n",
      "Epoch 100/150\n",
      "\u001b[1m30/30\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - acc: 0.8437 - loss: 0.9487 - val_acc: 0.7470 - val_loss: 1.1185\n",
      "Epoch 101/150\n",
      "\u001b[1m30/30\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - acc: 0.8503 - loss: 0.9430 - val_acc: 0.7460 - val_loss: 1.1182\n",
      "Epoch 102/150\n",
      "\u001b[1m30/30\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - acc: 0.8518 - loss: 0.9333 - val_acc: 0.7430 - val_loss: 1.1221\n",
      "Epoch 103/150\n",
      "\u001b[1m30/30\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - acc: 0.8536 - loss: 0.9243 - val_acc: 0.7500 - val_loss: 1.1134\n",
      "Epoch 104/150\n",
      "\u001b[1m30/30\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - acc: 0.8553 - loss: 0.9324 - val_acc: 0.7440 - val_loss: 1.1132\n",
      "Epoch 105/150\n",
      "\u001b[1m30/30\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - acc: 0.8529 - loss: 0.9231 - val_acc: 0.7460 - val_loss: 1.1108\n",
      "Epoch 106/150\n",
      "\u001b[1m30/30\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - acc: 0.8548 - loss: 0.9163 - val_acc: 0.7470 - val_loss: 1.1085\n",
      "Epoch 107/150\n",
      "\u001b[1m30/30\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - acc: 0.8555 - loss: 0.9141 - val_acc: 0.7470 - val_loss: 1.1071\n",
      "Epoch 108/150\n",
      "\u001b[1m30/30\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - acc: 0.8604 - loss: 0.8998 - val_acc: 0.7440 - val_loss: 1.1059\n",
      "Epoch 109/150\n",
      "\u001b[1m30/30\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - acc: 0.8562 - loss: 0.9154 - val_acc: 0.7480 - val_loss: 1.1005\n",
      "Epoch 110/150\n",
      "\u001b[1m30/30\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 9ms/step - acc: 0.8592 - loss: 0.9017 - val_acc: 0.7460 - val_loss: 1.1023\n",
      "Epoch 111/150\n",
      "\u001b[1m30/30\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - acc: 0.8590 - loss: 0.8961 - val_acc: 0.7420 - val_loss: 1.1003\n",
      "Epoch 112/150\n",
      "\u001b[1m30/30\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 9ms/step - acc: 0.8582 - loss: 0.8992 - val_acc: 0.7470 - val_loss: 1.0957\n",
      "Epoch 113/150\n",
      "\u001b[1m30/30\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - acc: 0.8656 - loss: 0.8897 - val_acc: 0.7450 - val_loss: 1.0940\n",
      "Epoch 114/150\n",
      "\u001b[1m30/30\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - acc: 0.8629 - loss: 0.8878 - val_acc: 0.7490 - val_loss: 1.0963\n",
      "Epoch 115/150\n",
      "\u001b[1m30/30\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - acc: 0.8644 - loss: 0.8842 - val_acc: 0.7430 - val_loss: 1.0938\n",
      "Epoch 116/150\n",
      "\u001b[1m30/30\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - acc: 0.8675 - loss: 0.8697 - val_acc: 0.7460 - val_loss: 1.0894\n",
      "Epoch 117/150\n",
      "\u001b[1m30/30\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - acc: 0.8629 - loss: 0.8742 - val_acc: 0.7440 - val_loss: 1.0870\n",
      "Epoch 118/150\n",
      "\u001b[1m30/30\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - acc: 0.8725 - loss: 0.8625 - val_acc: 0.7490 - val_loss: 1.0848\n",
      "Epoch 119/150\n",
      "\u001b[1m30/30\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - acc: 0.8703 - loss: 0.8634 - val_acc: 0.7500 - val_loss: 1.0818\n",
      "Epoch 120/150\n",
      "\u001b[1m30/30\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - acc: 0.8685 - loss: 0.8614 - val_acc: 0.7440 - val_loss: 1.0853\n",
      "Epoch 121/150\n",
      "\u001b[1m30/30\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - acc: 0.8710 - loss: 0.8567 - val_acc: 0.7420 - val_loss: 1.0804\n",
      "Epoch 122/150\n",
      "\u001b[1m30/30\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - acc: 0.8695 - loss: 0.8620 - val_acc: 0.7450 - val_loss: 1.0797\n",
      "Epoch 123/150\n",
      "\u001b[1m30/30\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 10ms/step - acc: 0.8715 - loss: 0.8550 - val_acc: 0.7470 - val_loss: 1.0782\n",
      "Epoch 124/150\n",
      "\u001b[1m30/30\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 10ms/step - acc: 0.8732 - loss: 0.8452 - val_acc: 0.7470 - val_loss: 1.0773\n",
      "Epoch 125/150\n",
      "\u001b[1m30/30\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 11ms/step - acc: 0.8667 - loss: 0.8589 - val_acc: 0.7490 - val_loss: 1.0746\n",
      "Epoch 126/150\n",
      "\u001b[1m30/30\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 9ms/step - acc: 0.8685 - loss: 0.8533 - val_acc: 0.7490 - val_loss: 1.0764\n",
      "Epoch 127/150\n",
      "\u001b[1m30/30\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 10ms/step - acc: 0.8785 - loss: 0.8426 - val_acc: 0.7470 - val_loss: 1.0740\n",
      "Epoch 128/150\n",
      "\u001b[1m30/30\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 10ms/step - acc: 0.8713 - loss: 0.8404 - val_acc: 0.7440 - val_loss: 1.0698\n",
      "Epoch 129/150\n",
      "\u001b[1m30/30\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 9ms/step - acc: 0.8783 - loss: 0.8310 - val_acc: 0.7440 - val_loss: 1.0721\n",
      "Epoch 130/150\n",
      "\u001b[1m30/30\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 9ms/step - acc: 0.8807 - loss: 0.8226 - val_acc: 0.7410 - val_loss: 1.0697\n",
      "Epoch 131/150\n",
      "\u001b[1m30/30\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 9ms/step - acc: 0.8791 - loss: 0.8297 - val_acc: 0.7430 - val_loss: 1.0646\n",
      "Epoch 132/150\n",
      "\u001b[1m30/30\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 9ms/step - acc: 0.8801 - loss: 0.8245 - val_acc: 0.7450 - val_loss: 1.0681\n",
      "Epoch 133/150\n",
      "\u001b[1m30/30\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - acc: 0.8813 - loss: 0.8158 - val_acc: 0.7470 - val_loss: 1.0642\n",
      "Epoch 134/150\n",
      "\u001b[1m30/30\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - acc: 0.8801 - loss: 0.8252 - val_acc: 0.7440 - val_loss: 1.0640\n",
      "Epoch 135/150\n",
      "\u001b[1m30/30\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - acc: 0.8858 - loss: 0.8114 - val_acc: 0.7450 - val_loss: 1.0602\n",
      "Epoch 136/150\n",
      "\u001b[1m30/30\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - acc: 0.8825 - loss: 0.8078 - val_acc: 0.7500 - val_loss: 1.0631\n",
      "Epoch 137/150\n",
      "\u001b[1m30/30\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - acc: 0.8824 - loss: 0.8106 - val_acc: 0.7500 - val_loss: 1.0602\n",
      "Epoch 138/150\n",
      "\u001b[1m30/30\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - acc: 0.8865 - loss: 0.8033 - val_acc: 0.7490 - val_loss: 1.0581\n",
      "Epoch 139/150\n",
      "\u001b[1m30/30\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - acc: 0.8832 - loss: 0.7985 - val_acc: 0.7480 - val_loss: 1.0571\n",
      "Epoch 140/150\n",
      "\u001b[1m30/30\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 9ms/step - acc: 0.8901 - loss: 0.7941 - val_acc: 0.7480 - val_loss: 1.0537\n",
      "Epoch 141/150\n",
      "\u001b[1m30/30\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - acc: 0.8887 - loss: 0.7918 - val_acc: 0.7510 - val_loss: 1.0546\n",
      "Epoch 142/150\n",
      "\u001b[1m30/30\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - acc: 0.8905 - loss: 0.7919 - val_acc: 0.7440 - val_loss: 1.0517\n",
      "Epoch 143/150\n",
      "\u001b[1m30/30\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 9ms/step - acc: 0.8840 - loss: 0.7909 - val_acc: 0.7500 - val_loss: 1.0512\n",
      "Epoch 144/150\n",
      "\u001b[1m30/30\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 9ms/step - acc: 0.8908 - loss: 0.7821 - val_acc: 0.7470 - val_loss: 1.0526\n",
      "Epoch 145/150\n",
      "\u001b[1m30/30\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 9ms/step - acc: 0.8911 - loss: 0.7786 - val_acc: 0.7410 - val_loss: 1.0493\n",
      "Epoch 146/150\n",
      "\u001b[1m30/30\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 9ms/step - acc: 0.8894 - loss: 0.7781 - val_acc: 0.7460 - val_loss: 1.0475\n",
      "Epoch 147/150\n",
      "\u001b[1m30/30\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - acc: 0.8895 - loss: 0.7868 - val_acc: 0.7520 - val_loss: 1.0467\n",
      "Epoch 148/150\n",
      "\u001b[1m30/30\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 9ms/step - acc: 0.8944 - loss: 0.7762 - val_acc: 0.7440 - val_loss: 1.0461\n",
      "Epoch 149/150\n",
      "\u001b[1m30/30\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 9ms/step - acc: 0.8922 - loss: 0.7758 - val_acc: 0.7470 - val_loss: 1.0441\n",
      "Epoch 150/150\n",
      "\u001b[1m30/30\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 9ms/step - acc: 0.8886 - loss: 0.7773 - val_acc: 0.7500 - val_loss: 1.0422\n"
     ]
    }
   ],
   "source": [
    "# Import regularizers\n",
    "from tensorflow import keras\n",
    "random.seed(123)\n",
    "L2_model = models.Sequential()\n",
    "\n",
    "# Add the input and first hidden layer\n",
    "L2_model.add(layers.Dense(50, activation='relu', kernel_regularizer=keras.regularizers.l2(0.005), input_shape=(X_train_tokens.shape[1],)))\n",
    "\n",
    "# Add another hidden layer\n",
    "L2_model.add(layers.Dense(25, activation='relu', kernel_regularizer=keras.regularizers.l2(0.005)))\n",
    "\n",
    "# Add an output layer\n",
    "L2_model.add(layers.Dense(7, activation='softmax'))\n",
    "\n",
    "# Compile the model\n",
    "L2_model.compile(optimizer='SGD', \n",
    "                 loss='categorical_crossentropy', \n",
    "                 metrics=['acc'])\n",
    "\n",
    "# Train the model \n",
    "L2_model_val = L2_model.fit(X_train_tokens, \n",
    "                            y_train_lb, \n",
    "                            epochs=150, \n",
    "                            batch_size=256, \n",
    "                            validation_data=(X_val_tokens, y_val_lb))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, look at the training as well as the validation accuracy for both the L2 and the baseline models. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "# L2 model details\n",
    "L2_model_dict = L2_model_val.history\n",
    "L2_acc_values = L2_model_dict['acc'] \n",
    "L2_val_acc_values = L2_model_dict['val_acc']\n",
    "\n",
    "# Baseline model\n",
    "baseline_model_acc = baseline_model_val_dict['accuracy'] \n",
    "baseline_model_val_acc = baseline_model_val_dict['val_accuracy']\n",
    "\n",
    "# Plot the accuracy for these models\n",
    "fig, ax = plt.subplots(figsize=(12, 8))\n",
    "epochs = range(1, len(L2_acc_values) + 1)\n",
    "ax.plot(epochs, L2_acc_values, label='Training acc L2')\n",
    "ax.plot(epochs, L2_val_acc_values, label='Validation acc L2')\n",
    "ax.plot(epochs, baseline_model_acc, label='Training acc')\n",
    "ax.plot(epochs, baseline_model_val_acc, label='Validation acc')\n",
    "ax.set_title('Training & validation accuracy L2 vs regular')\n",
    "ax.set_xlabel('Epochs')\n",
    "ax.set_ylabel('Accuracy')\n",
    "ax.legend();"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The results of L2 regularization are quite disappointing here. Notice the discrepancy between validation and training accuracy seems to have decreased slightly, but the end result is definitely not getting better.  \n",
    "\n",
    "\n",
    "## L1 Regularization\n",
    "\n",
    "Now have a look at L1 regularization. Will this work better? \n",
    "\n",
    "- Use 2 hidden layers with 50 units in the first and 25 in the second layer, both with `'relu'` activation functions \n",
    "- Add L1 regularization to both the hidden layers with 0.005 as the `lambda_coeff` "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/150\n",
      "\u001b[1m30/30\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 15ms/step - acc: 0.1261 - loss: 16.1851 - val_acc: 0.1530 - val_loss: 15.6191\n",
      "Epoch 2/150\n",
      "\u001b[1m30/30\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - acc: 0.1708 - loss: 15.4218 - val_acc: 0.1860 - val_loss: 14.8762\n",
      "Epoch 3/150\n",
      "\u001b[1m30/30\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - acc: 0.2078 - loss: 14.6862 - val_acc: 0.2150 - val_loss: 14.1561\n",
      "Epoch 4/150\n",
      "\u001b[1m30/30\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - acc: 0.2403 - loss: 13.9701 - val_acc: 0.2550 - val_loss: 13.4547\n",
      "Epoch 5/150\n",
      "\u001b[1m30/30\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 10ms/step - acc: 0.2692 - loss: 13.2726 - val_acc: 0.2810 - val_loss: 12.7730\n",
      "Epoch 6/150\n",
      "\u001b[1m30/30\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - acc: 0.2904 - loss: 12.5949 - val_acc: 0.2990 - val_loss: 12.1099\n",
      "Epoch 7/150\n",
      "\u001b[1m30/30\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - acc: 0.3141 - loss: 11.9378 - val_acc: 0.3130 - val_loss: 11.4664\n",
      "Epoch 8/150\n",
      "\u001b[1m30/30\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - acc: 0.3286 - loss: 11.2996 - val_acc: 0.3340 - val_loss: 10.8423\n",
      "Epoch 9/150\n",
      "\u001b[1m30/30\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 9ms/step - acc: 0.3576 - loss: 10.6741 - val_acc: 0.3600 - val_loss: 10.2355\n",
      "Epoch 10/150\n",
      "\u001b[1m30/30\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - acc: 0.3711 - loss: 10.0743 - val_acc: 0.3820 - val_loss: 9.6494\n",
      "Epoch 11/150\n",
      "\u001b[1m30/30\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 9ms/step - acc: 0.3893 - loss: 9.4931 - val_acc: 0.4090 - val_loss: 9.0820\n",
      "Epoch 12/150\n",
      "\u001b[1m30/30\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - acc: 0.4205 - loss: 8.9249 - val_acc: 0.4330 - val_loss: 8.5364\n",
      "Epoch 13/150\n",
      "\u001b[1m30/30\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - acc: 0.4525 - loss: 8.3832 - val_acc: 0.4620 - val_loss: 8.0082\n",
      "Epoch 14/150\n",
      "\u001b[1m30/30\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - acc: 0.4856 - loss: 7.8633 - val_acc: 0.4740 - val_loss: 7.5055\n",
      "Epoch 15/150\n",
      "\u001b[1m30/30\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - acc: 0.4983 - loss: 7.3638 - val_acc: 0.5020 - val_loss: 7.0214\n",
      "Epoch 16/150\n",
      "\u001b[1m30/30\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - acc: 0.5253 - loss: 6.8812 - val_acc: 0.5350 - val_loss: 6.5605\n",
      "Epoch 17/150\n",
      "\u001b[1m30/30\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - acc: 0.5571 - loss: 6.4233 - val_acc: 0.5470 - val_loss: 6.1252\n",
      "Epoch 18/150\n",
      "\u001b[1m30/30\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - acc: 0.5717 - loss: 6.0027 - val_acc: 0.5670 - val_loss: 5.7121\n",
      "Epoch 19/150\n",
      "\u001b[1m30/30\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - acc: 0.5895 - loss: 5.5882 - val_acc: 0.5830 - val_loss: 5.3195\n",
      "Epoch 20/150\n",
      "\u001b[1m30/30\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - acc: 0.6098 - loss: 5.2015 - val_acc: 0.5910 - val_loss: 4.9568\n",
      "Epoch 21/150\n",
      "\u001b[1m30/30\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - acc: 0.6009 - loss: 4.8486 - val_acc: 0.5970 - val_loss: 4.6104\n",
      "Epoch 22/150\n",
      "\u001b[1m30/30\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - acc: 0.6179 - loss: 4.5048 - val_acc: 0.6130 - val_loss: 4.2904\n",
      "Epoch 23/150\n",
      "\u001b[1m30/30\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - acc: 0.6295 - loss: 4.1797 - val_acc: 0.6220 - val_loss: 3.9965\n",
      "Epoch 24/150\n",
      "\u001b[1m30/30\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - acc: 0.6324 - loss: 3.9064 - val_acc: 0.6360 - val_loss: 3.7204\n",
      "Epoch 25/150\n",
      "\u001b[1m30/30\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - acc: 0.6499 - loss: 3.6205 - val_acc: 0.6370 - val_loss: 3.4730\n",
      "Epoch 26/150\n",
      "\u001b[1m30/30\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - acc: 0.6480 - loss: 3.3893 - val_acc: 0.6460 - val_loss: 3.2413\n",
      "Epoch 27/150\n",
      "\u001b[1m30/30\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - acc: 0.6594 - loss: 3.1674 - val_acc: 0.6490 - val_loss: 3.0346\n",
      "Epoch 28/150\n",
      "\u001b[1m30/30\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - acc: 0.6602 - loss: 2.9726 - val_acc: 0.6420 - val_loss: 2.8522\n",
      "Epoch 29/150\n",
      "\u001b[1m30/30\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - acc: 0.6611 - loss: 2.7833 - val_acc: 0.6460 - val_loss: 2.6868\n",
      "Epoch 30/150\n",
      "\u001b[1m30/30\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - acc: 0.6640 - loss: 2.6286 - val_acc: 0.6470 - val_loss: 2.5444\n",
      "Epoch 31/150\n",
      "\u001b[1m30/30\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - acc: 0.6718 - loss: 2.4830 - val_acc: 0.6510 - val_loss: 2.4185\n",
      "Epoch 32/150\n",
      "\u001b[1m30/30\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - acc: 0.6659 - loss: 2.3688 - val_acc: 0.6580 - val_loss: 2.3132\n",
      "Epoch 33/150\n",
      "\u001b[1m30/30\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - acc: 0.6768 - loss: 2.2694 - val_acc: 0.6570 - val_loss: 2.2297\n",
      "Epoch 34/150\n",
      "\u001b[1m30/30\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 9ms/step - acc: 0.6673 - loss: 2.2002 - val_acc: 0.6520 - val_loss: 2.1637\n",
      "Epoch 35/150\n",
      "\u001b[1m30/30\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - acc: 0.6718 - loss: 2.1408 - val_acc: 0.6590 - val_loss: 2.1126\n",
      "Epoch 36/150\n",
      "\u001b[1m30/30\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - acc: 0.6742 - loss: 2.0820 - val_acc: 0.6640 - val_loss: 2.0732\n",
      "Epoch 37/150\n",
      "\u001b[1m30/30\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - acc: 0.6742 - loss: 2.0642 - val_acc: 0.6560 - val_loss: 2.0488\n",
      "Epoch 38/150\n",
      "\u001b[1m30/30\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - acc: 0.6808 - loss: 2.0239 - val_acc: 0.6620 - val_loss: 2.0211\n",
      "Epoch 39/150\n",
      "\u001b[1m30/30\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - acc: 0.6777 - loss: 2.0090 - val_acc: 0.6630 - val_loss: 1.9996\n",
      "Epoch 40/150\n",
      "\u001b[1m30/30\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - acc: 0.6825 - loss: 1.9900 - val_acc: 0.6720 - val_loss: 1.9851\n",
      "Epoch 41/150\n",
      "\u001b[1m30/30\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - acc: 0.6802 - loss: 1.9633 - val_acc: 0.6650 - val_loss: 1.9632\n",
      "Epoch 42/150\n",
      "\u001b[1m30/30\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - acc: 0.6794 - loss: 1.9495 - val_acc: 0.6740 - val_loss: 1.9461\n",
      "Epoch 43/150\n",
      "\u001b[1m30/30\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - acc: 0.6720 - loss: 1.9341 - val_acc: 0.6700 - val_loss: 1.9303\n",
      "Epoch 44/150\n",
      "\u001b[1m30/30\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - acc: 0.6813 - loss: 1.9132 - val_acc: 0.6710 - val_loss: 1.9147\n",
      "Epoch 45/150\n",
      "\u001b[1m30/30\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - acc: 0.6907 - loss: 1.8880 - val_acc: 0.6710 - val_loss: 1.9049\n",
      "Epoch 46/150\n",
      "\u001b[1m30/30\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - acc: 0.6812 - loss: 1.8881 - val_acc: 0.6700 - val_loss: 1.8873\n",
      "Epoch 47/150\n",
      "\u001b[1m30/30\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - acc: 0.6859 - loss: 1.8753 - val_acc: 0.6780 - val_loss: 1.8756\n",
      "Epoch 48/150\n",
      "\u001b[1m30/30\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - acc: 0.6772 - loss: 1.8772 - val_acc: 0.6780 - val_loss: 1.8595\n",
      "Epoch 49/150\n",
      "\u001b[1m30/30\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - acc: 0.6892 - loss: 1.8408 - val_acc: 0.6770 - val_loss: 1.8465\n",
      "Epoch 50/150\n",
      "\u001b[1m30/30\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 9ms/step - acc: 0.6921 - loss: 1.8288 - val_acc: 0.6780 - val_loss: 1.8391\n",
      "Epoch 51/150\n",
      "\u001b[1m30/30\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - acc: 0.6845 - loss: 1.8210 - val_acc: 0.6770 - val_loss: 1.8267\n",
      "Epoch 52/150\n",
      "\u001b[1m30/30\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 9ms/step - acc: 0.6876 - loss: 1.8075 - val_acc: 0.6790 - val_loss: 1.8116\n",
      "Epoch 53/150\n",
      "\u001b[1m30/30\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - acc: 0.6892 - loss: 1.8026 - val_acc: 0.6760 - val_loss: 1.8027\n",
      "Epoch 54/150\n",
      "\u001b[1m30/30\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - acc: 0.6804 - loss: 1.7939 - val_acc: 0.6690 - val_loss: 1.7951\n",
      "Epoch 55/150\n",
      "\u001b[1m30/30\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - acc: 0.6745 - loss: 1.7924 - val_acc: 0.6760 - val_loss: 1.7833\n",
      "Epoch 56/150\n",
      "\u001b[1m30/30\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - acc: 0.6847 - loss: 1.7759 - val_acc: 0.6790 - val_loss: 1.7679\n",
      "Epoch 57/150\n",
      "\u001b[1m30/30\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - acc: 0.6796 - loss: 1.7654 - val_acc: 0.6820 - val_loss: 1.7590\n",
      "Epoch 58/150\n",
      "\u001b[1m30/30\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - acc: 0.6813 - loss: 1.7545 - val_acc: 0.6830 - val_loss: 1.7488\n",
      "Epoch 59/150\n",
      "\u001b[1m30/30\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - acc: 0.6928 - loss: 1.7371 - val_acc: 0.6860 - val_loss: 1.7401\n",
      "Epoch 60/150\n",
      "\u001b[1m30/30\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - acc: 0.6883 - loss: 1.7357 - val_acc: 0.6850 - val_loss: 1.7299\n",
      "Epoch 61/150\n",
      "\u001b[1m30/30\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - acc: 0.6987 - loss: 1.7117 - val_acc: 0.6790 - val_loss: 1.7197\n",
      "Epoch 62/150\n",
      "\u001b[1m30/30\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - acc: 0.6927 - loss: 1.7141 - val_acc: 0.6810 - val_loss: 1.7101\n",
      "Epoch 63/150\n",
      "\u001b[1m30/30\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - acc: 0.6909 - loss: 1.7010 - val_acc: 0.6850 - val_loss: 1.7034\n",
      "Epoch 64/150\n",
      "\u001b[1m30/30\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - acc: 0.6901 - loss: 1.6935 - val_acc: 0.6810 - val_loss: 1.6950\n",
      "Epoch 65/150\n",
      "\u001b[1m30/30\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - acc: 0.6953 - loss: 1.6811 - val_acc: 0.6810 - val_loss: 1.6864\n",
      "Epoch 66/150\n",
      "\u001b[1m30/30\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - acc: 0.6925 - loss: 1.6807 - val_acc: 0.6860 - val_loss: 1.6802\n",
      "Epoch 67/150\n",
      "\u001b[1m30/30\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 9ms/step - acc: 0.6896 - loss: 1.6698 - val_acc: 0.6820 - val_loss: 1.6674\n",
      "Epoch 68/150\n",
      "\u001b[1m30/30\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - acc: 0.6835 - loss: 1.6753 - val_acc: 0.6850 - val_loss: 1.6611\n",
      "Epoch 69/150\n",
      "\u001b[1m30/30\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 10ms/step - acc: 0.6928 - loss: 1.6497 - val_acc: 0.6840 - val_loss: 1.6562\n",
      "Epoch 70/150\n",
      "\u001b[1m30/30\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - acc: 0.6926 - loss: 1.6466 - val_acc: 0.6780 - val_loss: 1.6485\n",
      "Epoch 71/150\n",
      "\u001b[1m30/30\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - acc: 0.6987 - loss: 1.6174 - val_acc: 0.6840 - val_loss: 1.6360\n",
      "Epoch 72/150\n",
      "\u001b[1m30/30\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - acc: 0.6905 - loss: 1.6312 - val_acc: 0.6830 - val_loss: 1.6302\n",
      "Epoch 73/150\n",
      "\u001b[1m30/30\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - acc: 0.6892 - loss: 1.6282 - val_acc: 0.6850 - val_loss: 1.6260\n",
      "Epoch 74/150\n",
      "\u001b[1m30/30\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - acc: 0.6944 - loss: 1.6149 - val_acc: 0.6800 - val_loss: 1.6160\n",
      "Epoch 75/150\n",
      "\u001b[1m30/30\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - acc: 0.6959 - loss: 1.5965 - val_acc: 0.6850 - val_loss: 1.6115\n",
      "Epoch 76/150\n",
      "\u001b[1m30/30\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - acc: 0.6868 - loss: 1.6048 - val_acc: 0.6850 - val_loss: 1.5998\n",
      "Epoch 77/150\n",
      "\u001b[1m30/30\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - acc: 0.6934 - loss: 1.5851 - val_acc: 0.6840 - val_loss: 1.5950\n",
      "Epoch 78/150\n",
      "\u001b[1m30/30\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 9ms/step - acc: 0.6967 - loss: 1.5876 - val_acc: 0.6840 - val_loss: 1.5841\n",
      "Epoch 79/150\n",
      "\u001b[1m30/30\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - acc: 0.6938 - loss: 1.5761 - val_acc: 0.6850 - val_loss: 1.5804\n",
      "Epoch 80/150\n",
      "\u001b[1m30/30\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - acc: 0.6981 - loss: 1.5572 - val_acc: 0.6830 - val_loss: 1.5723\n",
      "Epoch 81/150\n",
      "\u001b[1m30/30\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - acc: 0.6975 - loss: 1.5642 - val_acc: 0.6830 - val_loss: 1.5627\n",
      "Epoch 82/150\n",
      "\u001b[1m30/30\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - acc: 0.6959 - loss: 1.5540 - val_acc: 0.6830 - val_loss: 1.5595\n",
      "Epoch 83/150\n",
      "\u001b[1m30/30\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - acc: 0.7019 - loss: 1.5438 - val_acc: 0.6860 - val_loss: 1.5552\n",
      "Epoch 84/150\n",
      "\u001b[1m30/30\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - acc: 0.7024 - loss: 1.5354 - val_acc: 0.6830 - val_loss: 1.5501\n",
      "Epoch 85/150\n",
      "\u001b[1m30/30\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - acc: 0.7005 - loss: 1.5419 - val_acc: 0.6870 - val_loss: 1.5371\n",
      "Epoch 86/150\n",
      "\u001b[1m30/30\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - acc: 0.7027 - loss: 1.5266 - val_acc: 0.6860 - val_loss: 1.5322\n",
      "Epoch 87/150\n",
      "\u001b[1m30/30\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - acc: 0.7024 - loss: 1.5185 - val_acc: 0.6830 - val_loss: 1.5273\n",
      "Epoch 88/150\n",
      "\u001b[1m30/30\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - acc: 0.7043 - loss: 1.5130 - val_acc: 0.6820 - val_loss: 1.5202\n",
      "Epoch 89/150\n",
      "\u001b[1m30/30\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - acc: 0.6924 - loss: 1.5305 - val_acc: 0.6830 - val_loss: 1.5113\n",
      "Epoch 90/150\n",
      "\u001b[1m30/30\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - acc: 0.7045 - loss: 1.5065 - val_acc: 0.6900 - val_loss: 1.5134\n",
      "Epoch 91/150\n",
      "\u001b[1m30/30\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 9ms/step - acc: 0.7014 - loss: 1.5017 - val_acc: 0.6850 - val_loss: 1.5026\n",
      "Epoch 92/150\n",
      "\u001b[1m30/30\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - acc: 0.6997 - loss: 1.4883 - val_acc: 0.6850 - val_loss: 1.4951\n",
      "Epoch 93/150\n",
      "\u001b[1m30/30\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - acc: 0.6995 - loss: 1.4821 - val_acc: 0.6850 - val_loss: 1.4910\n",
      "Epoch 94/150\n",
      "\u001b[1m30/30\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - acc: 0.7042 - loss: 1.4750 - val_acc: 0.6870 - val_loss: 1.4849\n",
      "Epoch 95/150\n",
      "\u001b[1m30/30\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - acc: 0.6963 - loss: 1.4834 - val_acc: 0.6880 - val_loss: 1.4773\n",
      "Epoch 96/150\n",
      "\u001b[1m30/30\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - acc: 0.7033 - loss: 1.4662 - val_acc: 0.6870 - val_loss: 1.4720\n",
      "Epoch 97/150\n",
      "\u001b[1m30/30\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - acc: 0.7073 - loss: 1.4537 - val_acc: 0.6840 - val_loss: 1.4671\n",
      "Epoch 98/150\n",
      "\u001b[1m30/30\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - acc: 0.7032 - loss: 1.4515 - val_acc: 0.6870 - val_loss: 1.4642\n",
      "Epoch 99/150\n",
      "\u001b[1m30/30\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - acc: 0.7072 - loss: 1.4401 - val_acc: 0.6830 - val_loss: 1.4584\n",
      "Epoch 100/150\n",
      "\u001b[1m30/30\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - acc: 0.7134 - loss: 1.4322 - val_acc: 0.6850 - val_loss: 1.4513\n",
      "Epoch 101/150\n",
      "\u001b[1m30/30\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - acc: 0.7014 - loss: 1.4410 - val_acc: 0.6890 - val_loss: 1.4438\n",
      "Epoch 102/150\n",
      "\u001b[1m30/30\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - acc: 0.7180 - loss: 1.4178 - val_acc: 0.6860 - val_loss: 1.4371\n",
      "Epoch 103/150\n",
      "\u001b[1m30/30\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - acc: 0.7060 - loss: 1.4224 - val_acc: 0.6960 - val_loss: 1.4378\n",
      "Epoch 104/150\n",
      "\u001b[1m30/30\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 9ms/step - acc: 0.7021 - loss: 1.4261 - val_acc: 0.6870 - val_loss: 1.4301\n",
      "Epoch 105/150\n",
      "\u001b[1m30/30\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - acc: 0.7053 - loss: 1.4177 - val_acc: 0.6880 - val_loss: 1.4226\n",
      "Epoch 106/150\n",
      "\u001b[1m30/30\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - acc: 0.7155 - loss: 1.3949 - val_acc: 0.6890 - val_loss: 1.4193\n",
      "Epoch 107/150\n",
      "\u001b[1m30/30\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - acc: 0.7033 - loss: 1.4062 - val_acc: 0.6880 - val_loss: 1.4114\n",
      "Epoch 108/150\n",
      "\u001b[1m30/30\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - acc: 0.6975 - loss: 1.4042 - val_acc: 0.6860 - val_loss: 1.4072\n",
      "Epoch 109/150\n",
      "\u001b[1m30/30\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - acc: 0.7028 - loss: 1.3936 - val_acc: 0.6940 - val_loss: 1.4010\n",
      "Epoch 110/150\n",
      "\u001b[1m30/30\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - acc: 0.7161 - loss: 1.3746 - val_acc: 0.6860 - val_loss: 1.3970\n",
      "Epoch 111/150\n",
      "\u001b[1m30/30\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - acc: 0.7102 - loss: 1.3759 - val_acc: 0.6880 - val_loss: 1.3903\n",
      "Epoch 112/150\n",
      "\u001b[1m30/30\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - acc: 0.7085 - loss: 1.3744 - val_acc: 0.6910 - val_loss: 1.3855\n",
      "Epoch 113/150\n",
      "\u001b[1m30/30\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - acc: 0.7132 - loss: 1.3665 - val_acc: 0.6910 - val_loss: 1.3799\n",
      "Epoch 114/150\n",
      "\u001b[1m30/30\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - acc: 0.7088 - loss: 1.3653 - val_acc: 0.6940 - val_loss: 1.3798\n",
      "Epoch 115/150\n",
      "\u001b[1m30/30\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - acc: 0.7006 - loss: 1.3741 - val_acc: 0.6890 - val_loss: 1.3730\n",
      "Epoch 116/150\n",
      "\u001b[1m30/30\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - acc: 0.7148 - loss: 1.3424 - val_acc: 0.6930 - val_loss: 1.3686\n",
      "Epoch 117/150\n",
      "\u001b[1m30/30\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - acc: 0.7047 - loss: 1.3627 - val_acc: 0.6880 - val_loss: 1.3638\n",
      "Epoch 118/150\n",
      "\u001b[1m30/30\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - acc: 0.7119 - loss: 1.3544 - val_acc: 0.6950 - val_loss: 1.3569\n",
      "Epoch 119/150\n",
      "\u001b[1m30/30\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - acc: 0.7115 - loss: 1.3443 - val_acc: 0.6840 - val_loss: 1.3595\n",
      "Epoch 120/150\n",
      "\u001b[1m30/30\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - acc: 0.7087 - loss: 1.3484 - val_acc: 0.6980 - val_loss: 1.3500\n",
      "Epoch 121/150\n",
      "\u001b[1m30/30\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - acc: 0.7057 - loss: 1.3407 - val_acc: 0.6960 - val_loss: 1.3443\n",
      "Epoch 122/150\n",
      "\u001b[1m30/30\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - acc: 0.7081 - loss: 1.3308 - val_acc: 0.7000 - val_loss: 1.3422\n",
      "Epoch 123/150\n",
      "\u001b[1m30/30\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 9ms/step - acc: 0.7056 - loss: 1.3284 - val_acc: 0.6920 - val_loss: 1.3395\n",
      "Epoch 124/150\n",
      "\u001b[1m30/30\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - acc: 0.7060 - loss: 1.3181 - val_acc: 0.6950 - val_loss: 1.3329\n",
      "Epoch 125/150\n",
      "\u001b[1m30/30\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 9ms/step - acc: 0.7108 - loss: 1.3200 - val_acc: 0.6960 - val_loss: 1.3266\n",
      "Epoch 126/150\n",
      "\u001b[1m30/30\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - acc: 0.7106 - loss: 1.3078 - val_acc: 0.6930 - val_loss: 1.3250\n",
      "Epoch 127/150\n",
      "\u001b[1m30/30\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - acc: 0.7157 - loss: 1.3094 - val_acc: 0.6970 - val_loss: 1.3208\n",
      "Epoch 128/150\n",
      "\u001b[1m30/30\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - acc: 0.7132 - loss: 1.3001 - val_acc: 0.6940 - val_loss: 1.3180\n",
      "Epoch 129/150\n",
      "\u001b[1m30/30\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - acc: 0.7186 - loss: 1.2987 - val_acc: 0.6980 - val_loss: 1.3181\n",
      "Epoch 130/150\n",
      "\u001b[1m30/30\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 9ms/step - acc: 0.7188 - loss: 1.2841 - val_acc: 0.6950 - val_loss: 1.3114\n",
      "Epoch 131/150\n",
      "\u001b[1m30/30\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 9ms/step - acc: 0.7128 - loss: 1.2847 - val_acc: 0.6990 - val_loss: 1.3085\n",
      "Epoch 132/150\n",
      "\u001b[1m30/30\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - acc: 0.7193 - loss: 1.2899 - val_acc: 0.6970 - val_loss: 1.3021\n",
      "Epoch 133/150\n",
      "\u001b[1m30/30\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 9ms/step - acc: 0.7212 - loss: 1.2768 - val_acc: 0.6910 - val_loss: 1.3025\n",
      "Epoch 134/150\n",
      "\u001b[1m30/30\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 9ms/step - acc: 0.7216 - loss: 1.2723 - val_acc: 0.6890 - val_loss: 1.3077\n",
      "Epoch 135/150\n",
      "\u001b[1m30/30\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - acc: 0.7199 - loss: 1.2889 - val_acc: 0.6990 - val_loss: 1.2924\n",
      "Epoch 136/150\n",
      "\u001b[1m30/30\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - acc: 0.7214 - loss: 1.2619 - val_acc: 0.6970 - val_loss: 1.2854\n",
      "Epoch 137/150\n",
      "\u001b[1m30/30\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - acc: 0.7123 - loss: 1.2682 - val_acc: 0.7010 - val_loss: 1.2820\n",
      "Epoch 138/150\n",
      "\u001b[1m30/30\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 11ms/step - acc: 0.7186 - loss: 1.2615 - val_acc: 0.6980 - val_loss: 1.2851\n",
      "Epoch 139/150\n",
      "\u001b[1m30/30\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 9ms/step - acc: 0.7180 - loss: 1.2641 - val_acc: 0.7000 - val_loss: 1.2784\n",
      "Epoch 140/150\n",
      "\u001b[1m30/30\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 9ms/step - acc: 0.7216 - loss: 1.2490 - val_acc: 0.6990 - val_loss: 1.2728\n",
      "Epoch 141/150\n",
      "\u001b[1m30/30\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - acc: 0.7202 - loss: 1.2513 - val_acc: 0.6970 - val_loss: 1.2796\n",
      "Epoch 142/150\n",
      "\u001b[1m30/30\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - acc: 0.7190 - loss: 1.2496 - val_acc: 0.6960 - val_loss: 1.2669\n",
      "Epoch 143/150\n",
      "\u001b[1m30/30\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - acc: 0.7135 - loss: 1.2490 - val_acc: 0.7000 - val_loss: 1.2615\n",
      "Epoch 144/150\n",
      "\u001b[1m30/30\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - acc: 0.7190 - loss: 1.2525 - val_acc: 0.6990 - val_loss: 1.2590\n",
      "Epoch 145/150\n",
      "\u001b[1m30/30\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 10ms/step - acc: 0.7329 - loss: 1.2241 - val_acc: 0.6980 - val_loss: 1.2596\n",
      "Epoch 146/150\n",
      "\u001b[1m30/30\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - acc: 0.7156 - loss: 1.2396 - val_acc: 0.7000 - val_loss: 1.2548\n",
      "Epoch 147/150\n",
      "\u001b[1m30/30\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - acc: 0.7215 - loss: 1.2307 - val_acc: 0.7010 - val_loss: 1.2533\n",
      "Epoch 148/150\n",
      "\u001b[1m30/30\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - acc: 0.7250 - loss: 1.2188 - val_acc: 0.7050 - val_loss: 1.2464\n",
      "Epoch 149/150\n",
      "\u001b[1m30/30\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - acc: 0.7251 - loss: 1.2196 - val_acc: 0.6990 - val_loss: 1.2474\n",
      "Epoch 150/150\n",
      "\u001b[1m30/30\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - acc: 0.7340 - loss: 1.2106 - val_acc: 0.7010 - val_loss: 1.2428\n"
     ]
    }
   ],
   "source": [
    "random.seed(123)\n",
    "L1_model = models.Sequential()\n",
    "\n",
    "# Add the input and first hidden layer\n",
    "L1_model.add(layers.Dense(50, activation='relu', kernel_regularizer=keras.regularizers.l1(0.005), input_shape=(X_train_tokens.shape[1],)))\n",
    "\n",
    "# Add a hidden layer\n",
    "L1_model.add(layers.Dense(25, activation='relu', \n",
    "                          kernel_regularizer=keras.regularizers.l1(0.005)))\n",
    "\n",
    "# Add an output layer\n",
    "L1_model.add(layers.Dense(7, activation='softmax'))\n",
    "\n",
    "# Compile the model\n",
    "L1_model.compile(optimizer='SGD', \n",
    "                 loss='categorical_crossentropy', \n",
    "                 metrics=['acc'])\n",
    "\n",
    "# Train the model \n",
    "L1_model_val = L1_model.fit(X_train_tokens, \n",
    "                            y_train_lb, \n",
    "                            epochs=150, \n",
    "                            batch_size=256, \n",
    "                            validation_data=(X_val_tokens, y_val_lb))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Plot the training as well as the validation accuracy for the L1 model: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(figsize=(12, 8))\n",
    "\n",
    "L1_model_dict = L1_model_val.history\n",
    "\n",
    "acc_values = L1_model_dict['acc'] \n",
    "val_acc_values = L1_model_dict['val_acc']\n",
    "\n",
    "epochs = range(1, len(acc_values) + 1)\n",
    "ax.plot(epochs, acc_values, label='Training acc L1')\n",
    "ax.plot(epochs, val_acc_values, label='Validation acc L1')\n",
    "ax.set_title('Training & validation accuracy with L1 regularization')\n",
    "ax.set_xlabel('Epochs')\n",
    "ax.set_ylabel('Accuracy')\n",
    "ax.legend();"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Notice how the training and validation accuracy don't diverge as much as before. Unfortunately, the validation accuracy isn't still that good. Next, experiment with dropout regularization to see if it offers any advantages. \n",
    "\n",
    "\n",
    "## Dropout Regularization \n",
    "\n",
    "It's time to try another technique: applying dropout to layers. As discussed in the earlier lesson, this involves setting a certain proportion of units in each layer to zero. In the following cell: \n",
    "\n",
    "- Apply a dropout rate of 30% to the input layer \n",
    "- Add a first hidden layer with 50 units and `'relu'` activation \n",
    "- Apply a dropout rate of 30% to the first hidden layer \n",
    "- Add a second hidden layer with 25 units and `'relu'` activation \n",
    "- Apply a dropout rate of 30% to the second hidden layer \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\ADMIN\\anaconda3\\Lib\\site-packages\\keras\\src\\layers\\regularization\\dropout.py:42: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
      "  super().__init__(**kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/150\n",
      "\u001b[1m30/30\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 19ms/step - acc: 0.1637 - loss: 1.9694 - val_acc: 0.1790 - val_loss: 1.9328\n",
      "Epoch 2/150\n",
      "\u001b[1m30/30\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 9ms/step - acc: 0.1679 - loss: 1.9562 - val_acc: 0.1910 - val_loss: 1.9213\n",
      "Epoch 3/150\n",
      "\u001b[1m30/30\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 10ms/step - acc: 0.1752 - loss: 1.9427 - val_acc: 0.1990 - val_loss: 1.9143\n",
      "Epoch 4/150\n",
      "\u001b[1m30/30\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 10ms/step - acc: 0.1800 - loss: 1.9313 - val_acc: 0.2110 - val_loss: 1.9079\n",
      "Epoch 5/150\n",
      "\u001b[1m30/30\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 9ms/step - acc: 0.1780 - loss: 1.9198 - val_acc: 0.2150 - val_loss: 1.9020\n",
      "Epoch 6/150\n",
      "\u001b[1m30/30\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 10ms/step - acc: 0.1884 - loss: 1.9141 - val_acc: 0.2240 - val_loss: 1.8955\n",
      "Epoch 7/150\n",
      "\u001b[1m30/30\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 10ms/step - acc: 0.1935 - loss: 1.9080 - val_acc: 0.2370 - val_loss: 1.8888\n",
      "Epoch 8/150\n",
      "\u001b[1m30/30\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 10ms/step - acc: 0.1985 - loss: 1.9089 - val_acc: 0.2350 - val_loss: 1.8812\n",
      "Epoch 9/150\n",
      "\u001b[1m30/30\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 9ms/step - acc: 0.2124 - loss: 1.8910 - val_acc: 0.2350 - val_loss: 1.8727\n",
      "Epoch 10/150\n",
      "\u001b[1m30/30\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 11ms/step - acc: 0.2211 - loss: 1.8843 - val_acc: 0.2480 - val_loss: 1.8632\n",
      "Epoch 11/150\n",
      "\u001b[1m30/30\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 9ms/step - acc: 0.2297 - loss: 1.8760 - val_acc: 0.2590 - val_loss: 1.8529\n",
      "Epoch 12/150\n",
      "\u001b[1m30/30\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 9ms/step - acc: 0.2349 - loss: 1.8690 - val_acc: 0.2620 - val_loss: 1.8415\n",
      "Epoch 13/150\n",
      "\u001b[1m30/30\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 9ms/step - acc: 0.2319 - loss: 1.8630 - val_acc: 0.2720 - val_loss: 1.8295\n",
      "Epoch 14/150\n",
      "\u001b[1m30/30\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 9ms/step - acc: 0.2527 - loss: 1.8441 - val_acc: 0.2930 - val_loss: 1.8163\n",
      "Epoch 15/150\n",
      "\u001b[1m30/30\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 9ms/step - acc: 0.2532 - loss: 1.8338 - val_acc: 0.3010 - val_loss: 1.8019\n",
      "Epoch 16/150\n",
      "\u001b[1m30/30\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 12ms/step - acc: 0.2620 - loss: 1.8305 - val_acc: 0.3090 - val_loss: 1.7867\n",
      "Epoch 17/150\n",
      "\u001b[1m30/30\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 10ms/step - acc: 0.2684 - loss: 1.8156 - val_acc: 0.3180 - val_loss: 1.7711\n",
      "Epoch 18/150\n",
      "\u001b[1m30/30\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 10ms/step - acc: 0.2802 - loss: 1.7982 - val_acc: 0.3290 - val_loss: 1.7523\n",
      "Epoch 19/150\n",
      "\u001b[1m30/30\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 9ms/step - acc: 0.2843 - loss: 1.7823 - val_acc: 0.3380 - val_loss: 1.7320\n",
      "Epoch 20/150\n",
      "\u001b[1m30/30\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 9ms/step - acc: 0.2965 - loss: 1.7659 - val_acc: 0.3400 - val_loss: 1.7120\n",
      "Epoch 21/150\n",
      "\u001b[1m30/30\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 10ms/step - acc: 0.2960 - loss: 1.7643 - val_acc: 0.3550 - val_loss: 1.6940\n",
      "Epoch 22/150\n",
      "\u001b[1m30/30\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 10ms/step - acc: 0.2940 - loss: 1.7493 - val_acc: 0.3730 - val_loss: 1.6748\n",
      "Epoch 23/150\n",
      "\u001b[1m30/30\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 12ms/step - acc: 0.3073 - loss: 1.7325 - val_acc: 0.3910 - val_loss: 1.6534\n",
      "Epoch 24/150\n",
      "\u001b[1m30/30\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 9ms/step - acc: 0.3112 - loss: 1.7269 - val_acc: 0.4070 - val_loss: 1.6340\n",
      "Epoch 25/150\n",
      "\u001b[1m30/30\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 9ms/step - acc: 0.3263 - loss: 1.6995 - val_acc: 0.4140 - val_loss: 1.6117\n",
      "Epoch 26/150\n",
      "\u001b[1m30/30\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 9ms/step - acc: 0.3397 - loss: 1.6904 - val_acc: 0.4320 - val_loss: 1.5904\n",
      "Epoch 27/150\n",
      "\u001b[1m30/30\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 10ms/step - acc: 0.3432 - loss: 1.6760 - val_acc: 0.4390 - val_loss: 1.5678\n",
      "Epoch 28/150\n",
      "\u001b[1m30/30\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 11ms/step - acc: 0.3582 - loss: 1.6514 - val_acc: 0.4550 - val_loss: 1.5472\n",
      "Epoch 29/150\n",
      "\u001b[1m30/30\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step - acc: 0.3680 - loss: 1.6334 - val_acc: 0.4750 - val_loss: 1.5281\n",
      "Epoch 30/150\n",
      "\u001b[1m30/30\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 11ms/step - acc: 0.3740 - loss: 1.6218 - val_acc: 0.4820 - val_loss: 1.5066\n",
      "Epoch 31/150\n",
      "\u001b[1m30/30\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 10ms/step - acc: 0.3742 - loss: 1.6049 - val_acc: 0.4980 - val_loss: 1.4866\n",
      "Epoch 32/150\n",
      "\u001b[1m30/30\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 10ms/step - acc: 0.3885 - loss: 1.5855 - val_acc: 0.5120 - val_loss: 1.4659\n",
      "Epoch 33/150\n",
      "\u001b[1m30/30\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 9ms/step - acc: 0.3909 - loss: 1.5752 - val_acc: 0.5210 - val_loss: 1.4464\n",
      "Epoch 34/150\n",
      "\u001b[1m30/30\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 11ms/step - acc: 0.3996 - loss: 1.5628 - val_acc: 0.5350 - val_loss: 1.4284\n",
      "Epoch 35/150\n",
      "\u001b[1m30/30\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 12ms/step - acc: 0.4023 - loss: 1.5430 - val_acc: 0.5480 - val_loss: 1.4108\n",
      "Epoch 36/150\n",
      "\u001b[1m30/30\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step - acc: 0.4220 - loss: 1.5380 - val_acc: 0.5540 - val_loss: 1.3914\n",
      "Epoch 37/150\n",
      "\u001b[1m30/30\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step - acc: 0.4199 - loss: 1.5209 - val_acc: 0.5680 - val_loss: 1.3739\n",
      "Epoch 38/150\n",
      "\u001b[1m30/30\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 22ms/step - acc: 0.4256 - loss: 1.5063 - val_acc: 0.5780 - val_loss: 1.3557\n",
      "Epoch 39/150\n",
      "\u001b[1m30/30\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 20ms/step - acc: 0.4252 - loss: 1.4974 - val_acc: 0.5850 - val_loss: 1.3396\n",
      "Epoch 40/150\n",
      "\u001b[1m30/30\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 16ms/step - acc: 0.4298 - loss: 1.4867 - val_acc: 0.5950 - val_loss: 1.3215\n",
      "Epoch 41/150\n",
      "\u001b[1m30/30\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 18ms/step - acc: 0.4347 - loss: 1.4728 - val_acc: 0.6030 - val_loss: 1.3036\n",
      "Epoch 42/150\n",
      "\u001b[1m30/30\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 17ms/step - acc: 0.4478 - loss: 1.4537 - val_acc: 0.6130 - val_loss: 1.2895\n",
      "Epoch 43/150\n",
      "\u001b[1m30/30\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 23ms/step - acc: 0.4574 - loss: 1.4357 - val_acc: 0.6180 - val_loss: 1.2734\n",
      "Epoch 44/150\n",
      "\u001b[1m30/30\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 17ms/step - acc: 0.4518 - loss: 1.4310 - val_acc: 0.6180 - val_loss: 1.2564\n",
      "Epoch 45/150\n",
      "\u001b[1m30/30\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 16ms/step - acc: 0.4568 - loss: 1.4344 - val_acc: 0.6300 - val_loss: 1.2417\n",
      "Epoch 46/150\n",
      "\u001b[1m30/30\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 14ms/step - acc: 0.4632 - loss: 1.4116 - val_acc: 0.6270 - val_loss: 1.2263\n",
      "Epoch 47/150\n",
      "\u001b[1m30/30\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 19ms/step - acc: 0.4733 - loss: 1.3800 - val_acc: 0.6310 - val_loss: 1.2114\n",
      "Epoch 48/150\n",
      "\u001b[1m30/30\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 16ms/step - acc: 0.4814 - loss: 1.3803 - val_acc: 0.6400 - val_loss: 1.1954\n",
      "Epoch 49/150\n",
      "\u001b[1m30/30\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 16ms/step - acc: 0.4814 - loss: 1.3767 - val_acc: 0.6470 - val_loss: 1.1845\n",
      "Epoch 50/150\n",
      "\u001b[1m30/30\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 20ms/step - acc: 0.4901 - loss: 1.3493 - val_acc: 0.6430 - val_loss: 1.1713\n",
      "Epoch 51/150\n",
      "\u001b[1m30/30\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 19ms/step - acc: 0.4957 - loss: 1.3401 - val_acc: 0.6460 - val_loss: 1.1587\n",
      "Epoch 52/150\n",
      "\u001b[1m30/30\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 18ms/step - acc: 0.5003 - loss: 1.3307 - val_acc: 0.6490 - val_loss: 1.1430\n",
      "Epoch 53/150\n",
      "\u001b[1m30/30\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 17ms/step - acc: 0.5133 - loss: 1.3091 - val_acc: 0.6520 - val_loss: 1.1311\n",
      "Epoch 54/150\n",
      "\u001b[1m30/30\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 16ms/step - acc: 0.5088 - loss: 1.3105 - val_acc: 0.6640 - val_loss: 1.1205\n",
      "Epoch 55/150\n",
      "\u001b[1m30/30\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 19ms/step - acc: 0.5161 - loss: 1.2978 - val_acc: 0.6630 - val_loss: 1.1081\n",
      "Epoch 56/150\n",
      "\u001b[1m30/30\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 16ms/step - acc: 0.5083 - loss: 1.3045 - val_acc: 0.6690 - val_loss: 1.0967\n",
      "Epoch 57/150\n",
      "\u001b[1m30/30\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 13ms/step - acc: 0.5189 - loss: 1.2841 - val_acc: 0.6710 - val_loss: 1.0858\n",
      "Epoch 58/150\n",
      "\u001b[1m30/30\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step - acc: 0.5224 - loss: 1.2671 - val_acc: 0.6700 - val_loss: 1.0739\n",
      "Epoch 59/150\n",
      "\u001b[1m30/30\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 13ms/step - acc: 0.5183 - loss: 1.2748 - val_acc: 0.6780 - val_loss: 1.0626\n",
      "Epoch 60/150\n",
      "\u001b[1m30/30\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 12ms/step - acc: 0.5334 - loss: 1.2505 - val_acc: 0.6770 - val_loss: 1.0527\n",
      "Epoch 61/150\n",
      "\u001b[1m30/30\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 13ms/step - acc: 0.5294 - loss: 1.2628 - val_acc: 0.6820 - val_loss: 1.0435\n",
      "Epoch 62/150\n",
      "\u001b[1m30/30\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 12ms/step - acc: 0.5394 - loss: 1.2297 - val_acc: 0.6830 - val_loss: 1.0328\n",
      "Epoch 63/150\n",
      "\u001b[1m30/30\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 13ms/step - acc: 0.5327 - loss: 1.2484 - val_acc: 0.6810 - val_loss: 1.0243\n",
      "Epoch 64/150\n",
      "\u001b[1m30/30\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 12ms/step - acc: 0.5376 - loss: 1.2374 - val_acc: 0.6930 - val_loss: 1.0173\n",
      "Epoch 65/150\n",
      "\u001b[1m30/30\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 10ms/step - acc: 0.5441 - loss: 1.2250 - val_acc: 0.6930 - val_loss: 1.0071\n",
      "Epoch 66/150\n",
      "\u001b[1m30/30\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step - acc: 0.5522 - loss: 1.1864 - val_acc: 0.7010 - val_loss: 0.9949\n",
      "Epoch 67/150\n",
      "\u001b[1m30/30\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 12ms/step - acc: 0.5535 - loss: 1.2011 - val_acc: 0.6980 - val_loss: 0.9859\n",
      "Epoch 68/150\n",
      "\u001b[1m30/30\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 12ms/step - acc: 0.5469 - loss: 1.1887 - val_acc: 0.7020 - val_loss: 0.9775\n",
      "Epoch 69/150\n",
      "\u001b[1m30/30\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step - acc: 0.5664 - loss: 1.1579 - val_acc: 0.7050 - val_loss: 0.9678\n",
      "Epoch 70/150\n",
      "\u001b[1m30/30\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 13ms/step - acc: 0.5648 - loss: 1.1738 - val_acc: 0.7000 - val_loss: 0.9602\n",
      "Epoch 71/150\n",
      "\u001b[1m30/30\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 16ms/step - acc: 0.5600 - loss: 1.1852 - val_acc: 0.7080 - val_loss: 0.9534\n",
      "Epoch 72/150\n",
      "\u001b[1m30/30\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 13ms/step - acc: 0.5711 - loss: 1.1635 - val_acc: 0.7070 - val_loss: 0.9443\n",
      "Epoch 73/150\n",
      "\u001b[1m30/30\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 11ms/step - acc: 0.5677 - loss: 1.1577 - val_acc: 0.7100 - val_loss: 0.9383\n",
      "Epoch 74/150\n",
      "\u001b[1m30/30\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 12ms/step - acc: 0.5720 - loss: 1.1409 - val_acc: 0.7090 - val_loss: 0.9297\n",
      "Epoch 75/150\n",
      "\u001b[1m30/30\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 12ms/step - acc: 0.5789 - loss: 1.1406 - val_acc: 0.7090 - val_loss: 0.9242\n",
      "Epoch 76/150\n",
      "\u001b[1m30/30\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 10ms/step - acc: 0.5729 - loss: 1.1273 - val_acc: 0.7100 - val_loss: 0.9186\n",
      "Epoch 77/150\n",
      "\u001b[1m30/30\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 11ms/step - acc: 0.5748 - loss: 1.1303 - val_acc: 0.7130 - val_loss: 0.9081\n",
      "Epoch 78/150\n",
      "\u001b[1m30/30\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 13ms/step - acc: 0.5863 - loss: 1.1205 - val_acc: 0.7170 - val_loss: 0.9036\n",
      "Epoch 79/150\n",
      "\u001b[1m30/30\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step - acc: 0.5778 - loss: 1.1135 - val_acc: 0.7160 - val_loss: 0.8946\n",
      "Epoch 80/150\n",
      "\u001b[1m30/30\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 13ms/step - acc: 0.5878 - loss: 1.1128 - val_acc: 0.7160 - val_loss: 0.8926\n",
      "Epoch 81/150\n",
      "\u001b[1m30/30\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 11ms/step - acc: 0.5749 - loss: 1.1224 - val_acc: 0.7150 - val_loss: 0.8860\n",
      "Epoch 82/150\n",
      "\u001b[1m30/30\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 12ms/step - acc: 0.5927 - loss: 1.0839 - val_acc: 0.7190 - val_loss: 0.8783\n",
      "Epoch 83/150\n",
      "\u001b[1m30/30\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 13ms/step - acc: 0.5893 - loss: 1.1009 - val_acc: 0.7150 - val_loss: 0.8745\n",
      "Epoch 84/150\n",
      "\u001b[1m30/30\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 12ms/step - acc: 0.5962 - loss: 1.0955 - val_acc: 0.7230 - val_loss: 0.8686\n",
      "Epoch 85/150\n",
      "\u001b[1m30/30\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 13ms/step - acc: 0.5980 - loss: 1.0689 - val_acc: 0.7200 - val_loss: 0.8623\n",
      "Epoch 86/150\n",
      "\u001b[1m30/30\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 12ms/step - acc: 0.6075 - loss: 1.0767 - val_acc: 0.7230 - val_loss: 0.8533\n",
      "Epoch 87/150\n",
      "\u001b[1m30/30\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step - acc: 0.5858 - loss: 1.0928 - val_acc: 0.7250 - val_loss: 0.8518\n",
      "Epoch 88/150\n",
      "\u001b[1m30/30\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 12ms/step - acc: 0.6046 - loss: 1.0712 - val_acc: 0.7210 - val_loss: 0.8451\n",
      "Epoch 89/150\n",
      "\u001b[1m30/30\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 13ms/step - acc: 0.6039 - loss: 1.0628 - val_acc: 0.7230 - val_loss: 0.8430\n",
      "Epoch 90/150\n",
      "\u001b[1m30/30\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 16ms/step - acc: 0.6048 - loss: 1.0716 - val_acc: 0.7240 - val_loss: 0.8385\n",
      "Epoch 91/150\n",
      "\u001b[1m30/30\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 15ms/step - acc: 0.6194 - loss: 1.0454 - val_acc: 0.7230 - val_loss: 0.8336\n",
      "Epoch 92/150\n",
      "\u001b[1m30/30\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 13ms/step - acc: 0.6140 - loss: 1.0386 - val_acc: 0.7280 - val_loss: 0.8276\n",
      "Epoch 93/150\n",
      "\u001b[1m30/30\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 15ms/step - acc: 0.6101 - loss: 1.0407 - val_acc: 0.7290 - val_loss: 0.8225\n",
      "Epoch 94/150\n",
      "\u001b[1m30/30\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step - acc: 0.6141 - loss: 1.0237 - val_acc: 0.7310 - val_loss: 0.8185\n",
      "Epoch 95/150\n",
      "\u001b[1m30/30\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 12ms/step - acc: 0.6260 - loss: 1.0111 - val_acc: 0.7310 - val_loss: 0.8141\n",
      "Epoch 96/150\n",
      "\u001b[1m30/30\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 11ms/step - acc: 0.6091 - loss: 1.0355 - val_acc: 0.7320 - val_loss: 0.8093\n",
      "Epoch 97/150\n",
      "\u001b[1m30/30\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 10ms/step - acc: 0.6215 - loss: 1.0255 - val_acc: 0.7320 - val_loss: 0.8062\n",
      "Epoch 98/150\n",
      "\u001b[1m30/30\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 10ms/step - acc: 0.6295 - loss: 1.0034 - val_acc: 0.7290 - val_loss: 0.8014\n",
      "Epoch 99/150\n",
      "\u001b[1m30/30\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 11ms/step - acc: 0.6352 - loss: 1.0083 - val_acc: 0.7300 - val_loss: 0.7980\n",
      "Epoch 100/150\n",
      "\u001b[1m30/30\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 12ms/step - acc: 0.6340 - loss: 0.9972 - val_acc: 0.7340 - val_loss: 0.7929\n",
      "Epoch 101/150\n",
      "\u001b[1m30/30\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 10ms/step - acc: 0.6319 - loss: 1.0140 - val_acc: 0.7380 - val_loss: 0.7887\n",
      "Epoch 102/150\n",
      "\u001b[1m30/30\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 10ms/step - acc: 0.6259 - loss: 1.0092 - val_acc: 0.7330 - val_loss: 0.7880\n",
      "Epoch 103/150\n",
      "\u001b[1m30/30\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 11ms/step - acc: 0.6307 - loss: 0.9976 - val_acc: 0.7350 - val_loss: 0.7832\n",
      "Epoch 104/150\n",
      "\u001b[1m30/30\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 11ms/step - acc: 0.6293 - loss: 0.9921 - val_acc: 0.7320 - val_loss: 0.7789\n",
      "Epoch 105/150\n",
      "\u001b[1m30/30\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 12ms/step - acc: 0.6346 - loss: 0.9875 - val_acc: 0.7310 - val_loss: 0.7763\n",
      "Epoch 106/150\n",
      "\u001b[1m30/30\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step - acc: 0.6460 - loss: 0.9735 - val_acc: 0.7350 - val_loss: 0.7716\n",
      "Epoch 107/150\n",
      "\u001b[1m30/30\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 11ms/step - acc: 0.6443 - loss: 0.9750 - val_acc: 0.7360 - val_loss: 0.7682\n",
      "Epoch 108/150\n",
      "\u001b[1m30/30\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 13ms/step - acc: 0.6411 - loss: 0.9787 - val_acc: 0.7330 - val_loss: 0.7675\n",
      "Epoch 109/150\n",
      "\u001b[1m30/30\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 10ms/step - acc: 0.6303 - loss: 0.9727 - val_acc: 0.7320 - val_loss: 0.7610\n",
      "Epoch 110/150\n",
      "\u001b[1m30/30\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 12ms/step - acc: 0.6285 - loss: 0.9861 - val_acc: 0.7350 - val_loss: 0.7597\n",
      "Epoch 111/150\n",
      "\u001b[1m30/30\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 10ms/step - acc: 0.6418 - loss: 0.9801 - val_acc: 0.7320 - val_loss: 0.7571\n",
      "Epoch 112/150\n",
      "\u001b[1m30/30\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 13ms/step - acc: 0.6355 - loss: 0.9885 - val_acc: 0.7330 - val_loss: 0.7537\n",
      "Epoch 113/150\n",
      "\u001b[1m30/30\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 11ms/step - acc: 0.6410 - loss: 0.9520 - val_acc: 0.7360 - val_loss: 0.7489\n",
      "Epoch 114/150\n",
      "\u001b[1m30/30\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 12ms/step - acc: 0.6467 - loss: 0.9594 - val_acc: 0.7340 - val_loss: 0.7468\n",
      "Epoch 115/150\n",
      "\u001b[1m30/30\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 11ms/step - acc: 0.6497 - loss: 0.9556 - val_acc: 0.7370 - val_loss: 0.7455\n",
      "Epoch 116/150\n",
      "\u001b[1m30/30\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 12ms/step - acc: 0.6393 - loss: 0.9593 - val_acc: 0.7360 - val_loss: 0.7425\n",
      "Epoch 117/150\n",
      "\u001b[1m30/30\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 12ms/step - acc: 0.6427 - loss: 0.9593 - val_acc: 0.7360 - val_loss: 0.7392\n",
      "Epoch 118/150\n",
      "\u001b[1m30/30\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step - acc: 0.6590 - loss: 0.9489 - val_acc: 0.7320 - val_loss: 0.7370\n",
      "Epoch 119/150\n",
      "\u001b[1m30/30\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 12ms/step - acc: 0.6465 - loss: 0.9428 - val_acc: 0.7360 - val_loss: 0.7350\n",
      "Epoch 120/150\n",
      "\u001b[1m30/30\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 12ms/step - acc: 0.6522 - loss: 0.9443 - val_acc: 0.7330 - val_loss: 0.7351\n",
      "Epoch 121/150\n",
      "\u001b[1m30/30\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 12ms/step - acc: 0.6448 - loss: 0.9487 - val_acc: 0.7330 - val_loss: 0.7316\n",
      "Epoch 122/150\n",
      "\u001b[1m30/30\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 11ms/step - acc: 0.6580 - loss: 0.9239 - val_acc: 0.7380 - val_loss: 0.7271\n",
      "Epoch 123/150\n",
      "\u001b[1m30/30\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 11ms/step - acc: 0.6632 - loss: 0.9172 - val_acc: 0.7370 - val_loss: 0.7247\n",
      "Epoch 124/150\n",
      "\u001b[1m30/30\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step - acc: 0.6594 - loss: 0.9261 - val_acc: 0.7410 - val_loss: 0.7203\n",
      "Epoch 125/150\n",
      "\u001b[1m30/30\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step - acc: 0.6587 - loss: 0.9246 - val_acc: 0.7320 - val_loss: 0.7193\n",
      "Epoch 126/150\n",
      "\u001b[1m30/30\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 12ms/step - acc: 0.6572 - loss: 0.9207 - val_acc: 0.7350 - val_loss: 0.7171\n",
      "Epoch 127/150\n",
      "\u001b[1m30/30\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step - acc: 0.6629 - loss: 0.9084 - val_acc: 0.7350 - val_loss: 0.7161\n",
      "Epoch 128/150\n",
      "\u001b[1m30/30\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 13ms/step - acc: 0.6587 - loss: 0.9157 - val_acc: 0.7400 - val_loss: 0.7129\n",
      "Epoch 129/150\n",
      "\u001b[1m30/30\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 15ms/step - acc: 0.6641 - loss: 0.9113 - val_acc: 0.7410 - val_loss: 0.7118\n",
      "Epoch 130/150\n",
      "\u001b[1m30/30\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step - acc: 0.6724 - loss: 0.8847 - val_acc: 0.7400 - val_loss: 0.7085\n",
      "Epoch 131/150\n",
      "\u001b[1m30/30\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step - acc: 0.6652 - loss: 0.9094 - val_acc: 0.7370 - val_loss: 0.7078\n",
      "Epoch 132/150\n",
      "\u001b[1m30/30\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step - acc: 0.6607 - loss: 0.9125 - val_acc: 0.7390 - val_loss: 0.7071\n",
      "Epoch 133/150\n",
      "\u001b[1m30/30\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 13ms/step - acc: 0.6802 - loss: 0.8844 - val_acc: 0.7350 - val_loss: 0.7041\n",
      "Epoch 134/150\n",
      "\u001b[1m30/30\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 17ms/step - acc: 0.6543 - loss: 0.9186 - val_acc: 0.7370 - val_loss: 0.7033\n",
      "Epoch 135/150\n",
      "\u001b[1m30/30\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step - acc: 0.6675 - loss: 0.8903 - val_acc: 0.7380 - val_loss: 0.7035\n",
      "Epoch 136/150\n",
      "\u001b[1m30/30\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 12ms/step - acc: 0.6690 - loss: 0.8967 - val_acc: 0.7350 - val_loss: 0.7008\n",
      "Epoch 137/150\n",
      "\u001b[1m30/30\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 12ms/step - acc: 0.6666 - loss: 0.9158 - val_acc: 0.7380 - val_loss: 0.6996\n",
      "Epoch 138/150\n",
      "\u001b[1m30/30\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 11ms/step - acc: 0.6754 - loss: 0.8800 - val_acc: 0.7370 - val_loss: 0.6969\n",
      "Epoch 139/150\n",
      "\u001b[1m30/30\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step - acc: 0.6763 - loss: 0.8789 - val_acc: 0.7400 - val_loss: 0.6956\n",
      "Epoch 140/150\n",
      "\u001b[1m30/30\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 11ms/step - acc: 0.6726 - loss: 0.8921 - val_acc: 0.7400 - val_loss: 0.6941\n",
      "Epoch 141/150\n",
      "\u001b[1m30/30\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 12ms/step - acc: 0.6714 - loss: 0.8983 - val_acc: 0.7440 - val_loss: 0.6945\n",
      "Epoch 142/150\n",
      "\u001b[1m30/30\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 12ms/step - acc: 0.6771 - loss: 0.8664 - val_acc: 0.7390 - val_loss: 0.6900\n",
      "Epoch 143/150\n",
      "\u001b[1m30/30\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 12ms/step - acc: 0.6883 - loss: 0.8550 - val_acc: 0.7370 - val_loss: 0.6884\n",
      "Epoch 144/150\n",
      "\u001b[1m30/30\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step - acc: 0.6788 - loss: 0.8681 - val_acc: 0.7390 - val_loss: 0.6856\n",
      "Epoch 145/150\n",
      "\u001b[1m30/30\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 13ms/step - acc: 0.6766 - loss: 0.8816 - val_acc: 0.7390 - val_loss: 0.6875\n",
      "Epoch 146/150\n",
      "\u001b[1m30/30\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 11ms/step - acc: 0.6863 - loss: 0.8531 - val_acc: 0.7380 - val_loss: 0.6829\n",
      "Epoch 147/150\n",
      "\u001b[1m30/30\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 13ms/step - acc: 0.6837 - loss: 0.8712 - val_acc: 0.7390 - val_loss: 0.6812\n",
      "Epoch 148/150\n",
      "\u001b[1m30/30\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step - acc: 0.6797 - loss: 0.8631 - val_acc: 0.7390 - val_loss: 0.6791\n",
      "Epoch 149/150\n",
      "\u001b[1m30/30\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 11ms/step - acc: 0.6708 - loss: 0.8619 - val_acc: 0.7400 - val_loss: 0.6762\n",
      "Epoch 150/150\n",
      "\u001b[1m30/30\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 11ms/step - acc: 0.6855 - loss: 0.8434 - val_acc: 0.7420 - val_loss: 0.6733\n"
     ]
    }
   ],
   "source": [
    "# ⏰ This cell may take about a minute to run\n",
    "random.seed(123)\n",
    "dropout_model = models.Sequential()\n",
    "\n",
    "# Implement dropout to the input layer\n",
    "# NOTE: This is where you define the number of units in the input layer\n",
    "dropout_model.add(layers.Dropout(0.3, input_shape=(X_train_tokens.shape[1],)))\n",
    "\n",
    "# Add the first hidden layer\n",
    "dropout_model.add(layers.Dense(50, activation='relu'))\n",
    "\n",
    "\n",
    "# Implement dropout to the first hidden layer \n",
    "dropout_model.add(layers.Dropout(0.3))\n",
    "\n",
    "# Add the second hidden layer\n",
    "dropout_model.add(layers.Dense(25, activation='relu'))\n",
    "\n",
    "# Implement dropout to the second hidden layer \n",
    "dropout_model.add(layers.Dropout(0.3))\n",
    "\n",
    "# Add the output layer\n",
    "dropout_model.add(layers.Dense(7, activation='softmax'))\n",
    "\n",
    "\n",
    "# Compile the model\n",
    "dropout_model.compile(optimizer='SGD', \n",
    "                      loss='categorical_crossentropy', \n",
    "                      metrics=['acc'])\n",
    "\n",
    "# Train the model\n",
    "dropout_model_val = dropout_model.fit(X_train_tokens, \n",
    "                                      y_train_lb, \n",
    "                                      epochs=150, \n",
    "                                      batch_size=256, \n",
    "                                      validation_data=(X_val_tokens, y_val_lb))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m235/235\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - acc: 0.8120 - loss: 0.5565\n",
      "Training Loss: 0.562 \n",
      "Training Accuracy: 0.801\n",
      "----------\n",
      "\u001b[1m47/47\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - acc: 0.7687 - loss: 0.6143\n",
      "Test Loss: 0.621 \n",
      "Test Accuracy: 0.777\n"
     ]
    }
   ],
   "source": [
    "results_train = dropout_model.evaluate(X_train_tokens, y_train_lb)\n",
    "print(f'Training Loss: {results_train[0]:.3} \\nTraining Accuracy: {results_train[1]:.3}')\n",
    "\n",
    "print('----------')\n",
    "\n",
    "results_test = dropout_model.evaluate(X_test_tokens, y_test_lb)\n",
    "print(f'Test Loss: {results_test[0]:.3} \\nTest Accuracy: {results_test[1]:.3}') "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can see here that the validation performance has improved again, and the training and test accuracy are very close!  \n",
    "\n",
    "## Bigger Data? \n",
    "\n",
    "Finally, let's examine if we can improve the model's performance just by adding more data. We've quadrapled the sample dataset from 10,000 to 40,000 observations, and all you need to do is run the code! "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_bigger_sample = df.sample(40000, random_state=123)\n",
    "\n",
    "X = df['Consumer complaint narrative']\n",
    "y = df['Product']\n",
    "\n",
    "# Train-test split\n",
    "X_train_bigger, X_test_bigger, y_train_bigger, y_test_bigger = train_test_split(X, \n",
    "                                                                                y, \n",
    "                                                                                test_size=6000, \n",
    "                                                                                random_state=42)\n",
    "\n",
    "# Validation set\n",
    "X_train_final_bigger, X_val_bigger, y_train_final_bigger, y_val_bigger = train_test_split(X_train_bigger, \n",
    "                                                                                          y_train_bigger, \n",
    "                                                                                          test_size=4000, \n",
    "                                                                                          random_state=42)\n",
    "\n",
    "\n",
    "# One-hot encoding of the complaints\n",
    "tokenizer = Tokenizer(num_words=2000)\n",
    "tokenizer.fit_on_texts(X_train_final_bigger)\n",
    "\n",
    "X_train_tokens_bigger = tokenizer.texts_to_matrix(X_train_final_bigger, mode='binary')\n",
    "X_val_tokens_bigger = tokenizer.texts_to_matrix(X_val_bigger, mode='binary')\n",
    "X_test_tokens_bigger = tokenizer.texts_to_matrix(X_test_bigger, mode='binary')\n",
    "\n",
    "# One-hot encoding of products\n",
    "lb = LabelBinarizer()\n",
    "lb.fit(y_train_final_bigger)\n",
    "\n",
    "y_train_lb_bigger = to_categorical(lb.transform(y_train_final_bigger))[:, :, 1]\n",
    "y_val_lb_bigger = to_categorical(lb.transform(y_val_bigger))[:, :, 1]\n",
    "y_test_lb_bigger = to_categorical(lb.transform(y_test_bigger))[:, :, 1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\ADMIN\\anaconda3\\Lib\\site-packages\\keras\\src\\layers\\core\\dense.py:87: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
      "  super().__init__(activity_regularizer=activity_regularizer, **kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/150\n",
      "\u001b[1m196/196\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 6ms/step - acc: 0.1717 - loss: 1.9439 - val_acc: 0.2830 - val_loss: 1.8758\n",
      "Epoch 2/150\n",
      "\u001b[1m196/196\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 4ms/step - acc: 0.3162 - loss: 1.8244 - val_acc: 0.4295 - val_loss: 1.6361\n",
      "Epoch 3/150\n",
      "\u001b[1m196/196\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 4ms/step - acc: 0.4734 - loss: 1.5363 - val_acc: 0.5730 - val_loss: 1.2982\n",
      "Epoch 4/150\n",
      "\u001b[1m196/196\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 4ms/step - acc: 0.6144 - loss: 1.2073 - val_acc: 0.6580 - val_loss: 1.0533\n",
      "Epoch 5/150\n",
      "\u001b[1m196/196\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 4ms/step - acc: 0.6779 - loss: 0.9895 - val_acc: 0.6940 - val_loss: 0.9074\n",
      "Epoch 6/150\n",
      "\u001b[1m196/196\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 5ms/step - acc: 0.7026 - loss: 0.8632 - val_acc: 0.7140 - val_loss: 0.8216\n",
      "Epoch 7/150\n",
      "\u001b[1m196/196\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 5ms/step - acc: 0.7223 - loss: 0.7796 - val_acc: 0.7318 - val_loss: 0.7654\n",
      "Epoch 8/150\n",
      "\u001b[1m196/196\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 4ms/step - acc: 0.7373 - loss: 0.7332 - val_acc: 0.7375 - val_loss: 0.7281\n",
      "Epoch 9/150\n",
      "\u001b[1m196/196\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 4ms/step - acc: 0.7505 - loss: 0.6870 - val_acc: 0.7462 - val_loss: 0.6991\n",
      "Epoch 10/150\n",
      "\u001b[1m196/196\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 4ms/step - acc: 0.7598 - loss: 0.6596 - val_acc: 0.7505 - val_loss: 0.6789\n",
      "Epoch 11/150\n",
      "\u001b[1m196/196\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 4ms/step - acc: 0.7677 - loss: 0.6334 - val_acc: 0.7590 - val_loss: 0.6591\n",
      "Epoch 12/150\n",
      "\u001b[1m196/196\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 4ms/step - acc: 0.7765 - loss: 0.6155 - val_acc: 0.7657 - val_loss: 0.6482\n",
      "Epoch 13/150\n",
      "\u001b[1m196/196\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 4ms/step - acc: 0.7824 - loss: 0.5951 - val_acc: 0.7715 - val_loss: 0.6333\n",
      "Epoch 14/150\n",
      "\u001b[1m196/196\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 5ms/step - acc: 0.7836 - loss: 0.5897 - val_acc: 0.7735 - val_loss: 0.6228\n",
      "Epoch 15/150\n",
      "\u001b[1m196/196\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 4ms/step - acc: 0.7928 - loss: 0.5699 - val_acc: 0.7775 - val_loss: 0.6176\n",
      "Epoch 16/150\n",
      "\u001b[1m196/196\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 4ms/step - acc: 0.7921 - loss: 0.5632 - val_acc: 0.7785 - val_loss: 0.6068\n",
      "Epoch 17/150\n",
      "\u001b[1m196/196\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 5ms/step - acc: 0.7984 - loss: 0.5489 - val_acc: 0.7850 - val_loss: 0.5996\n",
      "Epoch 18/150\n",
      "\u001b[1m196/196\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 4ms/step - acc: 0.8055 - loss: 0.5389 - val_acc: 0.7880 - val_loss: 0.5939\n",
      "Epoch 19/150\n",
      "\u001b[1m196/196\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 5ms/step - acc: 0.8062 - loss: 0.5345 - val_acc: 0.7872 - val_loss: 0.5906\n",
      "Epoch 20/150\n",
      "\u001b[1m196/196\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 4ms/step - acc: 0.8135 - loss: 0.5208 - val_acc: 0.7855 - val_loss: 0.5862\n",
      "Epoch 21/150\n",
      "\u001b[1m196/196\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 5ms/step - acc: 0.8112 - loss: 0.5273 - val_acc: 0.7928 - val_loss: 0.5794\n",
      "Epoch 22/150\n",
      "\u001b[1m196/196\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 4ms/step - acc: 0.8138 - loss: 0.5143 - val_acc: 0.7922 - val_loss: 0.5759\n",
      "Epoch 23/150\n",
      "\u001b[1m196/196\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 4ms/step - acc: 0.8187 - loss: 0.5022 - val_acc: 0.7925 - val_loss: 0.5736\n",
      "Epoch 24/150\n",
      "\u001b[1m196/196\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 4ms/step - acc: 0.8233 - loss: 0.4951 - val_acc: 0.7947 - val_loss: 0.5687\n",
      "Epoch 25/150\n",
      "\u001b[1m196/196\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 5ms/step - acc: 0.8207 - loss: 0.4943 - val_acc: 0.7937 - val_loss: 0.5658\n",
      "Epoch 26/150\n",
      "\u001b[1m196/196\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 4ms/step - acc: 0.8273 - loss: 0.4882 - val_acc: 0.7972 - val_loss: 0.5613\n",
      "Epoch 27/150\n",
      "\u001b[1m196/196\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 5ms/step - acc: 0.8252 - loss: 0.4851 - val_acc: 0.7975 - val_loss: 0.5595\n",
      "Epoch 28/150\n",
      "\u001b[1m196/196\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 6ms/step - acc: 0.8259 - loss: 0.4840 - val_acc: 0.7987 - val_loss: 0.5586\n",
      "Epoch 29/150\n",
      "\u001b[1m196/196\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 5ms/step - acc: 0.8292 - loss: 0.4760 - val_acc: 0.8012 - val_loss: 0.5574\n",
      "Epoch 30/150\n",
      "\u001b[1m196/196\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 5ms/step - acc: 0.8313 - loss: 0.4726 - val_acc: 0.8020 - val_loss: 0.5562\n",
      "Epoch 31/150\n",
      "\u001b[1m196/196\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 5ms/step - acc: 0.8334 - loss: 0.4669 - val_acc: 0.8055 - val_loss: 0.5561\n",
      "Epoch 32/150\n",
      "\u001b[1m196/196\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 5ms/step - acc: 0.8336 - loss: 0.4656 - val_acc: 0.8045 - val_loss: 0.5525\n",
      "Epoch 33/150\n",
      "\u001b[1m196/196\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 4ms/step - acc: 0.8355 - loss: 0.4590 - val_acc: 0.8060 - val_loss: 0.5540\n",
      "Epoch 34/150\n",
      "\u001b[1m196/196\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 5ms/step - acc: 0.8383 - loss: 0.4525 - val_acc: 0.8055 - val_loss: 0.5474\n",
      "Epoch 35/150\n",
      "\u001b[1m196/196\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 5ms/step - acc: 0.8388 - loss: 0.4483 - val_acc: 0.8060 - val_loss: 0.5482\n",
      "Epoch 36/150\n",
      "\u001b[1m196/196\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 5ms/step - acc: 0.8411 - loss: 0.4461 - val_acc: 0.8062 - val_loss: 0.5472\n",
      "Epoch 37/150\n",
      "\u001b[1m196/196\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 5ms/step - acc: 0.8396 - loss: 0.4465 - val_acc: 0.8048 - val_loss: 0.5468\n",
      "Epoch 38/150\n",
      "\u001b[1m196/196\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 6ms/step - acc: 0.8384 - loss: 0.4490 - val_acc: 0.8058 - val_loss: 0.5458\n",
      "Epoch 39/150\n",
      "\u001b[1m196/196\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 6ms/step - acc: 0.8426 - loss: 0.4412 - val_acc: 0.8067 - val_loss: 0.5500\n",
      "Epoch 40/150\n",
      "\u001b[1m196/196\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 7ms/step - acc: 0.8471 - loss: 0.4316 - val_acc: 0.8060 - val_loss: 0.5439\n",
      "Epoch 41/150\n",
      "\u001b[1m196/196\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 5ms/step - acc: 0.8477 - loss: 0.4337 - val_acc: 0.8083 - val_loss: 0.5467\n",
      "Epoch 42/150\n",
      "\u001b[1m196/196\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 6ms/step - acc: 0.8469 - loss: 0.4328 - val_acc: 0.8125 - val_loss: 0.5434\n",
      "Epoch 43/150\n",
      "\u001b[1m196/196\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 5ms/step - acc: 0.8475 - loss: 0.4258 - val_acc: 0.8105 - val_loss: 0.5447\n",
      "Epoch 44/150\n",
      "\u001b[1m196/196\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 6ms/step - acc: 0.8512 - loss: 0.4247 - val_acc: 0.8115 - val_loss: 0.5409\n",
      "Epoch 45/150\n",
      "\u001b[1m196/196\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 5ms/step - acc: 0.8493 - loss: 0.4267 - val_acc: 0.8102 - val_loss: 0.5432\n",
      "Epoch 46/150\n",
      "\u001b[1m196/196\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 5ms/step - acc: 0.8513 - loss: 0.4196 - val_acc: 0.8135 - val_loss: 0.5436\n",
      "Epoch 47/150\n",
      "\u001b[1m196/196\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 5ms/step - acc: 0.8519 - loss: 0.4200 - val_acc: 0.8098 - val_loss: 0.5414\n",
      "Epoch 48/150\n",
      "\u001b[1m196/196\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 5ms/step - acc: 0.8524 - loss: 0.4156 - val_acc: 0.8115 - val_loss: 0.5414\n",
      "Epoch 49/150\n",
      "\u001b[1m196/196\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 5ms/step - acc: 0.8514 - loss: 0.4163 - val_acc: 0.8152 - val_loss: 0.5433\n",
      "Epoch 50/150\n",
      "\u001b[1m196/196\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 5ms/step - acc: 0.8549 - loss: 0.4116 - val_acc: 0.8148 - val_loss: 0.5432\n",
      "Epoch 51/150\n",
      "\u001b[1m196/196\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 5ms/step - acc: 0.8521 - loss: 0.4120 - val_acc: 0.8130 - val_loss: 0.5424\n",
      "Epoch 52/150\n",
      "\u001b[1m196/196\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 5ms/step - acc: 0.8549 - loss: 0.4106 - val_acc: 0.8150 - val_loss: 0.5397\n",
      "Epoch 53/150\n",
      "\u001b[1m196/196\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 5ms/step - acc: 0.8581 - loss: 0.4027 - val_acc: 0.8150 - val_loss: 0.5414\n",
      "Epoch 54/150\n",
      "\u001b[1m196/196\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 4ms/step - acc: 0.8605 - loss: 0.3964 - val_acc: 0.8135 - val_loss: 0.5398\n",
      "Epoch 55/150\n",
      "\u001b[1m196/196\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 4ms/step - acc: 0.8585 - loss: 0.4031 - val_acc: 0.8125 - val_loss: 0.5403\n",
      "Epoch 56/150\n",
      "\u001b[1m196/196\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 5ms/step - acc: 0.8601 - loss: 0.3972 - val_acc: 0.8142 - val_loss: 0.5401\n",
      "Epoch 57/150\n",
      "\u001b[1m196/196\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 4ms/step - acc: 0.8590 - loss: 0.3971 - val_acc: 0.8142 - val_loss: 0.5409\n",
      "Epoch 58/150\n",
      "\u001b[1m196/196\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 5ms/step - acc: 0.8608 - loss: 0.3946 - val_acc: 0.8165 - val_loss: 0.5383\n",
      "Epoch 59/150\n",
      "\u001b[1m196/196\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 4ms/step - acc: 0.8610 - loss: 0.3954 - val_acc: 0.8152 - val_loss: 0.5410\n",
      "Epoch 60/150\n",
      "\u001b[1m196/196\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 5ms/step - acc: 0.8658 - loss: 0.3870 - val_acc: 0.8123 - val_loss: 0.5434\n",
      "Epoch 61/150\n",
      "\u001b[1m196/196\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 4ms/step - acc: 0.8574 - loss: 0.4023 - val_acc: 0.8163 - val_loss: 0.5407\n",
      "Epoch 62/150\n",
      "\u001b[1m196/196\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 4ms/step - acc: 0.8621 - loss: 0.3923 - val_acc: 0.8163 - val_loss: 0.5401\n",
      "Epoch 63/150\n",
      "\u001b[1m196/196\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 5ms/step - acc: 0.8650 - loss: 0.3832 - val_acc: 0.8142 - val_loss: 0.5394\n",
      "Epoch 64/150\n",
      "\u001b[1m196/196\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 4ms/step - acc: 0.8656 - loss: 0.3840 - val_acc: 0.8138 - val_loss: 0.5417\n",
      "Epoch 65/150\n",
      "\u001b[1m196/196\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 5ms/step - acc: 0.8659 - loss: 0.3786 - val_acc: 0.8112 - val_loss: 0.5477\n",
      "Epoch 66/150\n",
      "\u001b[1m196/196\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 4ms/step - acc: 0.8644 - loss: 0.3816 - val_acc: 0.8133 - val_loss: 0.5413\n",
      "Epoch 67/150\n",
      "\u001b[1m196/196\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 4ms/step - acc: 0.8669 - loss: 0.3774 - val_acc: 0.8155 - val_loss: 0.5409\n",
      "Epoch 68/150\n",
      "\u001b[1m196/196\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 4ms/step - acc: 0.8675 - loss: 0.3773 - val_acc: 0.8133 - val_loss: 0.5415\n",
      "Epoch 69/150\n",
      "\u001b[1m196/196\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 5ms/step - acc: 0.8692 - loss: 0.3763 - val_acc: 0.8117 - val_loss: 0.5431\n",
      "Epoch 70/150\n",
      "\u001b[1m196/196\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 4ms/step - acc: 0.8689 - loss: 0.3761 - val_acc: 0.8158 - val_loss: 0.5482\n",
      "Epoch 71/150\n",
      "\u001b[1m196/196\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 4ms/step - acc: 0.8709 - loss: 0.3701 - val_acc: 0.8090 - val_loss: 0.5443\n",
      "Epoch 72/150\n",
      "\u001b[1m196/196\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 4ms/step - acc: 0.8695 - loss: 0.3692 - val_acc: 0.8130 - val_loss: 0.5452\n",
      "Epoch 73/150\n",
      "\u001b[1m196/196\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 5ms/step - acc: 0.8704 - loss: 0.3655 - val_acc: 0.8073 - val_loss: 0.5486\n",
      "Epoch 74/150\n",
      "\u001b[1m196/196\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 4ms/step - acc: 0.8696 - loss: 0.3698 - val_acc: 0.8152 - val_loss: 0.5430\n",
      "Epoch 75/150\n",
      "\u001b[1m196/196\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 4ms/step - acc: 0.8674 - loss: 0.3731 - val_acc: 0.8077 - val_loss: 0.5473\n",
      "Epoch 76/150\n",
      "\u001b[1m196/196\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 4ms/step - acc: 0.8715 - loss: 0.3668 - val_acc: 0.8110 - val_loss: 0.5466\n",
      "Epoch 77/150\n",
      "\u001b[1m196/196\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 5ms/step - acc: 0.8716 - loss: 0.3652 - val_acc: 0.8140 - val_loss: 0.5469\n",
      "Epoch 78/150\n",
      "\u001b[1m196/196\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 6ms/step - acc: 0.8730 - loss: 0.3615 - val_acc: 0.8117 - val_loss: 0.5462\n",
      "Epoch 79/150\n",
      "\u001b[1m196/196\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 5ms/step - acc: 0.8714 - loss: 0.3629 - val_acc: 0.8112 - val_loss: 0.5508\n",
      "Epoch 80/150\n",
      "\u001b[1m196/196\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 5ms/step - acc: 0.8714 - loss: 0.3624 - val_acc: 0.8073 - val_loss: 0.5506\n",
      "Epoch 81/150\n",
      "\u001b[1m196/196\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 6ms/step - acc: 0.8725 - loss: 0.3599 - val_acc: 0.8092 - val_loss: 0.5472\n",
      "Epoch 82/150\n",
      "\u001b[1m196/196\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 5ms/step - acc: 0.8751 - loss: 0.3563 - val_acc: 0.8142 - val_loss: 0.5470\n",
      "Epoch 83/150\n",
      "\u001b[1m196/196\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 5ms/step - acc: 0.8753 - loss: 0.3590 - val_acc: 0.8117 - val_loss: 0.5478\n",
      "Epoch 84/150\n",
      "\u001b[1m196/196\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 5ms/step - acc: 0.8743 - loss: 0.3589 - val_acc: 0.8075 - val_loss: 0.5542\n",
      "Epoch 85/150\n",
      "\u001b[1m196/196\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 5ms/step - acc: 0.8735 - loss: 0.3543 - val_acc: 0.8117 - val_loss: 0.5510\n",
      "Epoch 86/150\n",
      "\u001b[1m196/196\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 5ms/step - acc: 0.8780 - loss: 0.3543 - val_acc: 0.8130 - val_loss: 0.5515\n",
      "Epoch 87/150\n",
      "\u001b[1m196/196\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 5ms/step - acc: 0.8755 - loss: 0.3577 - val_acc: 0.8092 - val_loss: 0.5526\n",
      "Epoch 88/150\n",
      "\u001b[1m196/196\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 5ms/step - acc: 0.8768 - loss: 0.3467 - val_acc: 0.8127 - val_loss: 0.5494\n",
      "Epoch 89/150\n",
      "\u001b[1m196/196\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 5ms/step - acc: 0.8785 - loss: 0.3457 - val_acc: 0.8050 - val_loss: 0.5647\n",
      "Epoch 90/150\n",
      "\u001b[1m196/196\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 5ms/step - acc: 0.8769 - loss: 0.3481 - val_acc: 0.8125 - val_loss: 0.5515\n",
      "Epoch 91/150\n",
      "\u001b[1m196/196\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 5ms/step - acc: 0.8782 - loss: 0.3469 - val_acc: 0.8145 - val_loss: 0.5508\n",
      "Epoch 92/150\n",
      "\u001b[1m196/196\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 5ms/step - acc: 0.8764 - loss: 0.3526 - val_acc: 0.8108 - val_loss: 0.5546\n",
      "Epoch 93/150\n",
      "\u001b[1m196/196\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 5ms/step - acc: 0.8798 - loss: 0.3457 - val_acc: 0.8125 - val_loss: 0.5533\n",
      "Epoch 94/150\n",
      "\u001b[1m196/196\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 5ms/step - acc: 0.8781 - loss: 0.3466 - val_acc: 0.8067 - val_loss: 0.5677\n",
      "Epoch 95/150\n",
      "\u001b[1m196/196\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 5ms/step - acc: 0.8804 - loss: 0.3410 - val_acc: 0.8105 - val_loss: 0.5535\n",
      "Epoch 96/150\n",
      "\u001b[1m196/196\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 5ms/step - acc: 0.8784 - loss: 0.3519 - val_acc: 0.8060 - val_loss: 0.5619\n",
      "Epoch 97/150\n",
      "\u001b[1m196/196\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 5ms/step - acc: 0.8799 - loss: 0.3410 - val_acc: 0.8080 - val_loss: 0.5602\n",
      "Epoch 98/150\n",
      "\u001b[1m196/196\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 5ms/step - acc: 0.8814 - loss: 0.3410 - val_acc: 0.8112 - val_loss: 0.5578\n",
      "Epoch 99/150\n",
      "\u001b[1m196/196\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 6ms/step - acc: 0.8823 - loss: 0.3391 - val_acc: 0.8110 - val_loss: 0.5615\n",
      "Epoch 100/150\n",
      "\u001b[1m196/196\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 6ms/step - acc: 0.8803 - loss: 0.3415 - val_acc: 0.8112 - val_loss: 0.5557\n",
      "Epoch 101/150\n",
      "\u001b[1m196/196\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 7ms/step - acc: 0.8810 - loss: 0.3375 - val_acc: 0.8108 - val_loss: 0.5597\n",
      "Epoch 102/150\n",
      "\u001b[1m196/196\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 6ms/step - acc: 0.8819 - loss: 0.3384 - val_acc: 0.8115 - val_loss: 0.5598\n",
      "Epoch 103/150\n",
      "\u001b[1m196/196\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 5ms/step - acc: 0.8845 - loss: 0.3313 - val_acc: 0.8105 - val_loss: 0.5635\n",
      "Epoch 104/150\n",
      "\u001b[1m196/196\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 6ms/step - acc: 0.8837 - loss: 0.3362 - val_acc: 0.8125 - val_loss: 0.5599\n",
      "Epoch 105/150\n",
      "\u001b[1m196/196\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 5ms/step - acc: 0.8859 - loss: 0.3317 - val_acc: 0.8110 - val_loss: 0.5605\n",
      "Epoch 106/150\n",
      "\u001b[1m196/196\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 5ms/step - acc: 0.8822 - loss: 0.3389 - val_acc: 0.8100 - val_loss: 0.5626\n",
      "Epoch 107/150\n",
      "\u001b[1m196/196\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 6ms/step - acc: 0.8839 - loss: 0.3337 - val_acc: 0.8098 - val_loss: 0.5648\n",
      "Epoch 108/150\n",
      "\u001b[1m196/196\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 5ms/step - acc: 0.8842 - loss: 0.3312 - val_acc: 0.8115 - val_loss: 0.5640\n",
      "Epoch 109/150\n",
      "\u001b[1m196/196\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 5ms/step - acc: 0.8855 - loss: 0.3292 - val_acc: 0.8120 - val_loss: 0.5657\n",
      "Epoch 110/150\n",
      "\u001b[1m196/196\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 5ms/step - acc: 0.8859 - loss: 0.3289 - val_acc: 0.8127 - val_loss: 0.5642\n",
      "Epoch 111/150\n",
      "\u001b[1m196/196\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 5ms/step - acc: 0.8871 - loss: 0.3274 - val_acc: 0.8117 - val_loss: 0.5704\n",
      "Epoch 112/150\n",
      "\u001b[1m196/196\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 5ms/step - acc: 0.8858 - loss: 0.3317 - val_acc: 0.8067 - val_loss: 0.5733\n",
      "Epoch 113/150\n",
      "\u001b[1m196/196\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 5ms/step - acc: 0.8884 - loss: 0.3241 - val_acc: 0.8092 - val_loss: 0.5673\n",
      "Epoch 114/150\n",
      "\u001b[1m196/196\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 5ms/step - acc: 0.8852 - loss: 0.3300 - val_acc: 0.8140 - val_loss: 0.5695\n",
      "Epoch 115/150\n",
      "\u001b[1m196/196\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 5ms/step - acc: 0.8843 - loss: 0.3327 - val_acc: 0.8100 - val_loss: 0.5684\n",
      "Epoch 116/150\n",
      "\u001b[1m196/196\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 5ms/step - acc: 0.8853 - loss: 0.3282 - val_acc: 0.8112 - val_loss: 0.5725\n",
      "Epoch 117/150\n",
      "\u001b[1m196/196\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 5ms/step - acc: 0.8876 - loss: 0.3217 - val_acc: 0.8108 - val_loss: 0.5709\n",
      "Epoch 118/150\n",
      "\u001b[1m196/196\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 6ms/step - acc: 0.8892 - loss: 0.3198 - val_acc: 0.8075 - val_loss: 0.5846\n",
      "Epoch 119/150\n",
      "\u001b[1m196/196\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 5ms/step - acc: 0.8885 - loss: 0.3236 - val_acc: 0.8102 - val_loss: 0.5739\n",
      "Epoch 120/150\n",
      "\u001b[1m196/196\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 5ms/step - acc: 0.8893 - loss: 0.3196 - val_acc: 0.8112 - val_loss: 0.5762\n",
      "Epoch 121/150\n",
      "\u001b[1m196/196\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 5ms/step - acc: 0.8895 - loss: 0.3203 - val_acc: 0.8117 - val_loss: 0.5705\n",
      "Epoch 122/150\n",
      "\u001b[1m196/196\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 5ms/step - acc: 0.8901 - loss: 0.3122 - val_acc: 0.8090 - val_loss: 0.5724\n",
      "Epoch 123/150\n",
      "\u001b[1m196/196\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 5ms/step - acc: 0.8880 - loss: 0.3195 - val_acc: 0.8080 - val_loss: 0.5740\n",
      "Epoch 124/150\n",
      "\u001b[1m196/196\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 5ms/step - acc: 0.8922 - loss: 0.3143 - val_acc: 0.8123 - val_loss: 0.5740\n",
      "Epoch 125/150\n",
      "\u001b[1m196/196\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 5ms/step - acc: 0.8908 - loss: 0.3119 - val_acc: 0.8102 - val_loss: 0.5770\n",
      "Epoch 126/150\n",
      "\u001b[1m196/196\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 5ms/step - acc: 0.8911 - loss: 0.3135 - val_acc: 0.8120 - val_loss: 0.5760\n",
      "Epoch 127/150\n",
      "\u001b[1m196/196\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 5ms/step - acc: 0.8937 - loss: 0.3084 - val_acc: 0.8112 - val_loss: 0.5756\n",
      "Epoch 128/150\n",
      "\u001b[1m196/196\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 5ms/step - acc: 0.8914 - loss: 0.3128 - val_acc: 0.8090 - val_loss: 0.5781\n",
      "Epoch 129/150\n",
      "\u001b[1m196/196\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 6ms/step - acc: 0.8935 - loss: 0.3097 - val_acc: 0.8067 - val_loss: 0.5865\n",
      "Epoch 130/150\n",
      "\u001b[1m196/196\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 5ms/step - acc: 0.8912 - loss: 0.3108 - val_acc: 0.8100 - val_loss: 0.5810\n",
      "Epoch 131/150\n",
      "\u001b[1m196/196\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 5ms/step - acc: 0.8932 - loss: 0.3107 - val_acc: 0.8092 - val_loss: 0.5865\n",
      "Epoch 132/150\n",
      "\u001b[1m196/196\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 5ms/step - acc: 0.8949 - loss: 0.3059 - val_acc: 0.8098 - val_loss: 0.5843\n",
      "Epoch 133/150\n",
      "\u001b[1m196/196\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 5ms/step - acc: 0.8968 - loss: 0.3035 - val_acc: 0.8080 - val_loss: 0.5841\n",
      "Epoch 134/150\n",
      "\u001b[1m196/196\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 5ms/step - acc: 0.8918 - loss: 0.3125 - val_acc: 0.8067 - val_loss: 0.5823\n",
      "Epoch 135/150\n",
      "\u001b[1m196/196\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 6ms/step - acc: 0.8952 - loss: 0.3038 - val_acc: 0.8095 - val_loss: 0.5833\n",
      "Epoch 136/150\n",
      "\u001b[1m196/196\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 5ms/step - acc: 0.8954 - loss: 0.3044 - val_acc: 0.8092 - val_loss: 0.5851\n",
      "Epoch 137/150\n",
      "\u001b[1m196/196\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 5ms/step - acc: 0.8955 - loss: 0.3055 - val_acc: 0.8105 - val_loss: 0.5853\n",
      "Epoch 138/150\n",
      "\u001b[1m196/196\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 5ms/step - acc: 0.8965 - loss: 0.2949 - val_acc: 0.8075 - val_loss: 0.5890\n",
      "Epoch 139/150\n",
      "\u001b[1m196/196\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 5ms/step - acc: 0.8993 - loss: 0.2963 - val_acc: 0.8083 - val_loss: 0.5903\n",
      "Epoch 140/150\n",
      "\u001b[1m196/196\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 5ms/step - acc: 0.8958 - loss: 0.3037 - val_acc: 0.8083 - val_loss: 0.5937\n",
      "Epoch 141/150\n",
      "\u001b[1m196/196\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 5ms/step - acc: 0.8968 - loss: 0.2995 - val_acc: 0.8087 - val_loss: 0.5927\n",
      "Epoch 142/150\n",
      "\u001b[1m196/196\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 5ms/step - acc: 0.8979 - loss: 0.2989 - val_acc: 0.8102 - val_loss: 0.5896\n",
      "Epoch 143/150\n",
      "\u001b[1m196/196\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 6ms/step - acc: 0.9008 - loss: 0.2931 - val_acc: 0.8073 - val_loss: 0.5944\n",
      "Epoch 144/150\n",
      "\u001b[1m196/196\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 5ms/step - acc: 0.8980 - loss: 0.2950 - val_acc: 0.8062 - val_loss: 0.5917\n",
      "Epoch 145/150\n",
      "\u001b[1m196/196\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 5ms/step - acc: 0.8975 - loss: 0.2993 - val_acc: 0.8067 - val_loss: 0.5903\n",
      "Epoch 146/150\n",
      "\u001b[1m196/196\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 5ms/step - acc: 0.9015 - loss: 0.2922 - val_acc: 0.8080 - val_loss: 0.5916\n",
      "Epoch 147/150\n",
      "\u001b[1m196/196\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 5ms/step - acc: 0.8999 - loss: 0.2936 - val_acc: 0.8077 - val_loss: 0.5940\n",
      "Epoch 148/150\n",
      "\u001b[1m196/196\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 5ms/step - acc: 0.8972 - loss: 0.2969 - val_acc: 0.8092 - val_loss: 0.5960\n",
      "Epoch 149/150\n",
      "\u001b[1m196/196\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 6ms/step - acc: 0.9011 - loss: 0.2881 - val_acc: 0.8070 - val_loss: 0.5970\n",
      "Epoch 150/150\n",
      "\u001b[1m196/196\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 6ms/step - acc: 0.9021 - loss: 0.2894 - val_acc: 0.8087 - val_loss: 0.5968\n"
     ]
    }
   ],
   "source": [
    "# ⏰ This cell may take several minutes to run\n",
    "random.seed(123)\n",
    "bigger_data_model = models.Sequential()\n",
    "bigger_data_model.add(layers.Dense(50, activation='relu', input_shape=(2000,)))\n",
    "bigger_data_model.add(layers.Dense(25, activation='relu'))\n",
    "bigger_data_model.add(layers.Dense(7, activation='softmax'))\n",
    "\n",
    "bigger_data_model.compile(optimizer='SGD', \n",
    "                          loss='categorical_crossentropy', \n",
    "                          metrics=['acc'])\n",
    "\n",
    "bigger_data_model_val = bigger_data_model.fit(X_train_tokens_bigger,  \n",
    "                                              y_train_lb_bigger,  \n",
    "                                              epochs=150,  \n",
    "                                              batch_size=256,  \n",
    "                                              validation_data=(X_val_tokens_bigger, y_val_lb_bigger))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m1563/1563\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 2ms/step - acc: 0.9048 - loss: 0.2803\n",
      "Training Loss: 0.285 \n",
      "Training Accuracy: 0.903\n",
      "----------\n",
      "\u001b[1m125/125\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - acc: 0.8163 - loss: 0.5718\n",
      "Test Loss: 0.597 \n",
      "Test Accuracy: 0.809\n"
     ]
    }
   ],
   "source": [
    "results_train = bigger_data_model.evaluate(X_train_tokens_bigger, y_train_lb_bigger)\n",
    "print(f'Training Loss: {results_train[0]:.3} \\nTraining Accuracy: {results_train[1]:.3}')\n",
    "\n",
    "print('----------')\n",
    "\n",
    "results_test = bigger_data_model.evaluate(X_val_tokens_bigger, y_val_lb_bigger)\n",
    "print(f'Test Loss: {results_test[0]:.3} \\nTest Accuracy: {results_test[1]:.3}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "With the same amount of epochs and no regularization technique, you were able to get both better test accuracy and loss. You can still consider early stopping, L1, L2 and dropout here. It's clear that having more data has a strong impact on model performance! \n",
    "\n",
    "\n",
    "## Additional Resources\n",
    "\n",
    "* https://github.com/susanli2016/Machine-Learning-with-Python/blob/master/Consumer_complaints.ipynb\n",
    "* https://machinelearningmastery.com/dropout-regularization-deep-learning-models-keras/\n",
    "* https://catalog.data.gov/dataset/consumer-complaint-database \n",
    "\n",
    "\n",
    "## Summary  \n",
    "\n",
    "In this lesson, you built deep learning models using a validation set and used several techniques such as L2 and L1 regularization, dropout regularization, and early stopping to improve the accuracy of your models. "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
