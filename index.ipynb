{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Tuning Neural Networks with Regularization - Lab \n",
    "\n",
    "## Introduction\n",
    "\n",
    "In this lab, you'll use a train-test partition as well as a validation set to get better insights about how to tune neural networks using regularization techniques. You'll start by repeating the process from the last section: importing the data and performing preprocessing including one-hot encoding. From there, you'll define and compile the model like before. \n",
    "\n",
    "## Objectives\n",
    "\n",
    "You will be able to:\n",
    "\n",
    "- Apply early stopping criteria with a neural network \n",
    "- Apply L1, L2, and dropout regularization on a neural network  \n",
    "- Examine the effects of training with more data on a neural network  \n",
    "\n",
    "\n",
    "## Load the Data\n",
    "\n",
    "Run the following cell to import some of the libraries and classes you'll need in this lab. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import random\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "from sklearn.model_selection import train_test_split\n",
    "from tensorflow.keras.utils import to_categorical\n",
    "from sklearn.preprocessing import LabelBinarizer\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings(action='ignore', category=FutureWarning)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The data is stored in the file `'Bank_complaints.csv'`. Load and preview the dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Product</th>\n",
       "      <th>Consumer complaint narrative</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Student loan</td>\n",
       "      <td>In XX/XX/XXXX I filled out the Fedlaon applica...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Student loan</td>\n",
       "      <td>I am being contacted by a debt collector for p...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Student loan</td>\n",
       "      <td>I cosigned XXXX student loans at SallieMae for...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Student loan</td>\n",
       "      <td>Navient has sytematically and illegally failed...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Student loan</td>\n",
       "      <td>My wife became eligible for XXXX Loan Forgiven...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "        Product                       Consumer complaint narrative\n",
       "0  Student loan  In XX/XX/XXXX I filled out the Fedlaon applica...\n",
       "1  Student loan  I am being contacted by a debt collector for p...\n",
       "2  Student loan  I cosigned XXXX student loans at SallieMae for...\n",
       "3  Student loan  Navient has sytematically and illegally failed...\n",
       "4  Student loan  My wife became eligible for XXXX Loan Forgiven..."
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Load and preview the dataset\n",
    "df = pd.read_csv('Bank_complaints.csv')\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preprocessing Overview\n",
    "\n",
    "Before you begin to practice some of your new tools such as regularization and optimization, let's practice munging some data as you did in the previous section with bank complaints. Recall some techniques:\n",
    "\n",
    "* Sampling in order to reduce training time (investigate model accuracy vs data size later on)\n",
    "* Train - test split\n",
    "* One-hot encoding your complaint text\n",
    "* Transforming your category labels \n",
    "\n",
    "## Preprocessing: Generate a Random Sample\n",
    "\n",
    "Since you have quite a bit of data and training neural networks takes a substantial amount of time and resources, downsample in order to test your initial pipeline. Going forward, these can be interesting areas of investigation: how does your model's performance change as you increase (or decrease) the size of your dataset?  \n",
    "\n",
    "- Generate a random sample of 10,000 observations using seed 123 for consistency of results. \n",
    "- Split this sample into `X` and `y` "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Downsample the data\n",
    "np.random.seed(123)\n",
    "df_sample = df.sample(10000, random_state=123)\n",
    "\n",
    "# Split the data into X and y\n",
    "y = df_sample['Product']\n",
    "X = df_sample.drop(columns='Product', axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train-test split\n",
    "\n",
    "- Split the data into training and test sets \n",
    "- Assign 1500 obervations to the test set and use 42 as the seed "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split data into training and test sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=1500, random_state=42)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Validation set \n",
    "\n",
    "As mentioned in the previous lesson, it is good practice to set aside a validation set, which is then used during hyperparameter tuning. Afterwards, when you have decided upon a final model, the test set can then be used to determine an unbiased perforance of the model. \n",
    "\n",
    "Run the cell below to further divide the training data into training and validation sets. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split the data into training and validation sets\n",
    "X_train_final, X_val, y_train_final, y_val = train_test_split(X_train, y_train, test_size=1000, random_state=42)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preprocessing: One-hot Encoding the Complaints\n",
    "\n",
    "As before, you need to do some preprocessing before building a neural network model. \n",
    "\n",
    "- Keep the 2,000 most common words and use one-hot encoding to reformat the complaints into a matrix of vectors \n",
    "- Transform the training, validate, and test sets "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use one-hot encoding to reformat the complaints into a matrix of vectors \n",
    "# Only keep the 2000 most common words \n",
    "\n",
    "tokenizer = Tokenizer(2000)\n",
    "tokenizer.fit_on_texts(X_train_final['Consumer complaint narrative'])\n",
    "\n",
    "X_train_tokens = tokenizer.texts_to_matrix(X_train_final['Consumer complaint narrative'], mode='binary')\n",
    "X_val_tokens = tokenizer.texts_to_matrix(X_val['Consumer complaint narrative'], mode='binary')\n",
    "X_test_tokens = tokenizer.texts_to_matrix(X_test['Consumer complaint narrative'], mode='binary')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preprocessing: Encoding the Products\n",
    "\n",
    "Similarly, now transform the descriptive product labels to integers labels. After transforming them to integer labels, retransform them into a matrix of binary flags, one for each of the various product labels.  \n",
    "  \n",
    "> **Note**: This is similar to your previous work with dummy variables. Each of the various product categories will be its own column, and each observation will be a row. In turn, each of these observation rows will have a 1 in the column associated with it's label, and all other entries for the row will be zero. \n",
    "\n",
    "Transform the training, validate, and test sets. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Transform the product labels to numerical values\n",
    "lb = LabelBinarizer()\n",
    "\n",
    "\n",
    "y_train_lb = lb.fit_transform(y_train_final)\n",
    "y_val_lb = lb.transform(y_val)\n",
    "y_test_lb = lb.transform(y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## A Baseline Model \n",
    "\n",
    "Rebuild a fully connected (Dense) layer network:  \n",
    "- Use 2 hidden layers with 50 units in the first and 25 in the second layer, both with `'relu'` activation functions (since you are dealing with a multiclass problem, classifying the complaints into 7 classes) \n",
    "- Use a `'softmax'` activation function for the output layer  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Build a baseline neural network model using Keras\n",
    "random.seed(123)\n",
    "from keras import models\n",
    "from keras import layers\n",
    "baseline_model = models.Sequential()\n",
    "\n",
    "baseline_model.add(layers.Dense(50, activation='relu', input_shape=(X_train_tokens.shape[1],)))\n",
    "baseline_model.add(layers.Dense(25, activation='relu'))\n",
    "baseline_model.add(layers.Dense(7, activation='softmax'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Compile the Model\n",
    "\n",
    "Compile this model with: \n",
    "\n",
    "- a stochastic gradient descent optimizer \n",
    "- `'categorical_crossentropy'` as the loss function \n",
    "- a focus on `'accuracy'` "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compile the model\n",
    "baseline_model.compile(optimizer='sgd', loss='categorical_crossentropy', metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train the Model\n",
    "\n",
    "- Train the model for 150 epochs in mini-batches of 256 samples \n",
    "- Include the `validation_data` argument to ensure you keep track of the validation loss  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/150\n",
      "\u001b[1m30/30\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 16ms/step - accuracy: 0.1308 - loss: 1.9607 - val_accuracy: 0.1510 - val_loss: 1.9547\n",
      "Epoch 2/150\n",
      "\u001b[1m30/30\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 11ms/step - accuracy: 0.1663 - loss: 1.9400 - val_accuracy: 0.1880 - val_loss: 1.9400\n",
      "Epoch 3/150\n",
      "\u001b[1m30/30\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 9ms/step - accuracy: 0.2043 - loss: 1.9264 - val_accuracy: 0.2170 - val_loss: 1.9261\n",
      "Epoch 4/150\n",
      "\u001b[1m30/30\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - accuracy: 0.2373 - loss: 1.9101 - val_accuracy: 0.2430 - val_loss: 1.9103\n",
      "Epoch 5/150\n",
      "\u001b[1m30/30\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 9ms/step - accuracy: 0.2604 - loss: 1.8934 - val_accuracy: 0.2720 - val_loss: 1.8911\n",
      "Epoch 6/150\n",
      "\u001b[1m30/30\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 9ms/step - accuracy: 0.2889 - loss: 1.8720 - val_accuracy: 0.2980 - val_loss: 1.8693\n",
      "Epoch 7/150\n",
      "\u001b[1m30/30\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 9ms/step - accuracy: 0.3023 - loss: 1.8490 - val_accuracy: 0.3180 - val_loss: 1.8434\n",
      "Epoch 8/150\n",
      "\u001b[1m30/30\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - accuracy: 0.3289 - loss: 1.8167 - val_accuracy: 0.3300 - val_loss: 1.8123\n",
      "Epoch 9/150\n",
      "\u001b[1m30/30\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - accuracy: 0.3461 - loss: 1.7854 - val_accuracy: 0.3550 - val_loss: 1.7761\n",
      "Epoch 10/150\n",
      "\u001b[1m30/30\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - accuracy: 0.3703 - loss: 1.7470 - val_accuracy: 0.3760 - val_loss: 1.7363\n",
      "Epoch 11/150\n",
      "\u001b[1m30/30\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - accuracy: 0.4016 - loss: 1.7021 - val_accuracy: 0.3990 - val_loss: 1.6902\n",
      "Epoch 12/150\n",
      "\u001b[1m30/30\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 9ms/step - accuracy: 0.4197 - loss: 1.6631 - val_accuracy: 0.4360 - val_loss: 1.6432\n",
      "Epoch 13/150\n",
      "\u001b[1m30/30\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 10ms/step - accuracy: 0.4580 - loss: 1.6034 - val_accuracy: 0.4560 - val_loss: 1.5883\n",
      "Epoch 14/150\n",
      "\u001b[1m30/30\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - accuracy: 0.4876 - loss: 1.5441 - val_accuracy: 0.4900 - val_loss: 1.5348\n",
      "Epoch 15/150\n",
      "\u001b[1m30/30\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - accuracy: 0.5159 - loss: 1.4997 - val_accuracy: 0.5100 - val_loss: 1.4792\n",
      "Epoch 16/150\n",
      "\u001b[1m30/30\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - accuracy: 0.5459 - loss: 1.4317 - val_accuracy: 0.5380 - val_loss: 1.4218\n",
      "Epoch 17/150\n",
      "\u001b[1m30/30\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 9ms/step - accuracy: 0.5764 - loss: 1.3716 - val_accuracy: 0.5630 - val_loss: 1.3703\n",
      "Epoch 18/150\n",
      "\u001b[1m30/30\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - accuracy: 0.5909 - loss: 1.3167 - val_accuracy: 0.5880 - val_loss: 1.3153\n",
      "Epoch 19/150\n",
      "\u001b[1m30/30\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - accuracy: 0.6101 - loss: 1.2617 - val_accuracy: 0.5930 - val_loss: 1.2711\n",
      "Epoch 20/150\n",
      "\u001b[1m30/30\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - accuracy: 0.6250 - loss: 1.2170 - val_accuracy: 0.6160 - val_loss: 1.2204\n",
      "Epoch 21/150\n",
      "\u001b[1m30/30\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - accuracy: 0.6402 - loss: 1.1715 - val_accuracy: 0.6330 - val_loss: 1.1773\n",
      "Epoch 22/150\n",
      "\u001b[1m30/30\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - accuracy: 0.6628 - loss: 1.1207 - val_accuracy: 0.6460 - val_loss: 1.1377\n",
      "Epoch 23/150\n",
      "\u001b[1m30/30\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - accuracy: 0.6631 - loss: 1.0882 - val_accuracy: 0.6510 - val_loss: 1.1011\n",
      "Epoch 24/150\n",
      "\u001b[1m30/30\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 9ms/step - accuracy: 0.6767 - loss: 1.0483 - val_accuracy: 0.6540 - val_loss: 1.0712\n",
      "Epoch 25/150\n",
      "\u001b[1m30/30\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - accuracy: 0.6898 - loss: 1.0078 - val_accuracy: 0.6710 - val_loss: 1.0391\n",
      "Epoch 26/150\n",
      "\u001b[1m30/30\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - accuracy: 0.6840 - loss: 0.9947 - val_accuracy: 0.6750 - val_loss: 1.0113\n",
      "Epoch 27/150\n",
      "\u001b[1m30/30\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - accuracy: 0.6911 - loss: 0.9566 - val_accuracy: 0.6780 - val_loss: 0.9905\n",
      "Epoch 28/150\n",
      "\u001b[1m30/30\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 9ms/step - accuracy: 0.6998 - loss: 0.9312 - val_accuracy: 0.6830 - val_loss: 0.9614\n",
      "Epoch 29/150\n",
      "\u001b[1m30/30\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - accuracy: 0.7047 - loss: 0.9122 - val_accuracy: 0.6950 - val_loss: 0.9449\n",
      "Epoch 30/150\n",
      "\u001b[1m30/30\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - accuracy: 0.7139 - loss: 0.8819 - val_accuracy: 0.6970 - val_loss: 0.9247\n",
      "Epoch 31/150\n",
      "\u001b[1m30/30\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - accuracy: 0.7221 - loss: 0.8605 - val_accuracy: 0.6920 - val_loss: 0.9050\n",
      "Epoch 32/150\n",
      "\u001b[1m30/30\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - accuracy: 0.7153 - loss: 0.8486 - val_accuracy: 0.6990 - val_loss: 0.8910\n",
      "Epoch 33/150\n",
      "\u001b[1m30/30\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - accuracy: 0.7274 - loss: 0.8178 - val_accuracy: 0.7010 - val_loss: 0.8768\n",
      "Epoch 34/150\n",
      "\u001b[1m30/30\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - accuracy: 0.7316 - loss: 0.8168 - val_accuracy: 0.7060 - val_loss: 0.8626\n",
      "Epoch 35/150\n",
      "\u001b[1m30/30\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - accuracy: 0.7355 - loss: 0.7992 - val_accuracy: 0.7090 - val_loss: 0.8508\n",
      "Epoch 36/150\n",
      "\u001b[1m30/30\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 9ms/step - accuracy: 0.7339 - loss: 0.7933 - val_accuracy: 0.7120 - val_loss: 0.8381\n",
      "Epoch 37/150\n",
      "\u001b[1m30/30\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - accuracy: 0.7420 - loss: 0.7743 - val_accuracy: 0.7140 - val_loss: 0.8247\n",
      "Epoch 38/150\n",
      "\u001b[1m30/30\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - accuracy: 0.7520 - loss: 0.7469 - val_accuracy: 0.7150 - val_loss: 0.8161\n",
      "Epoch 39/150\n",
      "\u001b[1m30/30\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - accuracy: 0.7495 - loss: 0.7512 - val_accuracy: 0.7190 - val_loss: 0.8095\n",
      "Epoch 40/150\n",
      "\u001b[1m30/30\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - accuracy: 0.7496 - loss: 0.7438 - val_accuracy: 0.7140 - val_loss: 0.7994\n",
      "Epoch 41/150\n",
      "\u001b[1m30/30\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - accuracy: 0.7550 - loss: 0.7186 - val_accuracy: 0.7140 - val_loss: 0.7932\n",
      "Epoch 42/150\n",
      "\u001b[1m30/30\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 9ms/step - accuracy: 0.7577 - loss: 0.7195 - val_accuracy: 0.7190 - val_loss: 0.7859\n",
      "Epoch 43/150\n",
      "\u001b[1m30/30\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 9ms/step - accuracy: 0.7624 - loss: 0.6980 - val_accuracy: 0.7200 - val_loss: 0.7793\n",
      "Epoch 44/150\n",
      "\u001b[1m30/30\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 11ms/step - accuracy: 0.7703 - loss: 0.6818 - val_accuracy: 0.7200 - val_loss: 0.7729\n",
      "Epoch 45/150\n",
      "\u001b[1m30/30\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - accuracy: 0.7743 - loss: 0.6830 - val_accuracy: 0.7290 - val_loss: 0.7640\n",
      "Epoch 46/150\n",
      "\u001b[1m30/30\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 10ms/step - accuracy: 0.7759 - loss: 0.6729 - val_accuracy: 0.7290 - val_loss: 0.7557\n",
      "Epoch 47/150\n",
      "\u001b[1m30/30\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 12ms/step - accuracy: 0.7683 - loss: 0.6705 - val_accuracy: 0.7280 - val_loss: 0.7519\n",
      "Epoch 48/150\n",
      "\u001b[1m30/30\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 11ms/step - accuracy: 0.7708 - loss: 0.6615 - val_accuracy: 0.7350 - val_loss: 0.7460\n",
      "Epoch 49/150\n",
      "\u001b[1m30/30\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - accuracy: 0.7766 - loss: 0.6569 - val_accuracy: 0.7320 - val_loss: 0.7410\n",
      "Epoch 50/150\n",
      "\u001b[1m30/30\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - accuracy: 0.7841 - loss: 0.6423 - val_accuracy: 0.7300 - val_loss: 0.7392\n",
      "Epoch 51/150\n",
      "\u001b[1m30/30\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 10ms/step - accuracy: 0.7808 - loss: 0.6357 - val_accuracy: 0.7390 - val_loss: 0.7320\n",
      "Epoch 52/150\n",
      "\u001b[1m30/30\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 9ms/step - accuracy: 0.7851 - loss: 0.6273 - val_accuracy: 0.7330 - val_loss: 0.7306\n",
      "Epoch 53/150\n",
      "\u001b[1m30/30\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 10ms/step - accuracy: 0.7737 - loss: 0.6396 - val_accuracy: 0.7300 - val_loss: 0.7265\n",
      "Epoch 54/150\n",
      "\u001b[1m30/30\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step - accuracy: 0.7838 - loss: 0.6242 - val_accuracy: 0.7350 - val_loss: 0.7181\n",
      "Epoch 55/150\n",
      "\u001b[1m30/30\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 11ms/step - accuracy: 0.7829 - loss: 0.6262 - val_accuracy: 0.7330 - val_loss: 0.7212\n",
      "Epoch 56/150\n",
      "\u001b[1m30/30\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 11ms/step - accuracy: 0.7863 - loss: 0.6213 - val_accuracy: 0.7370 - val_loss: 0.7146\n",
      "Epoch 57/150\n",
      "\u001b[1m30/30\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 10ms/step - accuracy: 0.7874 - loss: 0.6070 - val_accuracy: 0.7380 - val_loss: 0.7097\n",
      "Epoch 58/150\n",
      "\u001b[1m30/30\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 11ms/step - accuracy: 0.7858 - loss: 0.6142 - val_accuracy: 0.7340 - val_loss: 0.7094\n",
      "Epoch 59/150\n",
      "\u001b[1m30/30\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 11ms/step - accuracy: 0.7954 - loss: 0.5979 - val_accuracy: 0.7340 - val_loss: 0.7082\n",
      "Epoch 60/150\n",
      "\u001b[1m30/30\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 9ms/step - accuracy: 0.7971 - loss: 0.5860 - val_accuracy: 0.7340 - val_loss: 0.7035\n",
      "Epoch 61/150\n",
      "\u001b[1m30/30\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - accuracy: 0.8015 - loss: 0.5799 - val_accuracy: 0.7350 - val_loss: 0.7026\n",
      "Epoch 62/150\n",
      "\u001b[1m30/30\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 10ms/step - accuracy: 0.8027 - loss: 0.5737 - val_accuracy: 0.7350 - val_loss: 0.6986\n",
      "Epoch 63/150\n",
      "\u001b[1m30/30\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 9ms/step - accuracy: 0.8009 - loss: 0.5678 - val_accuracy: 0.7450 - val_loss: 0.6916\n",
      "Epoch 64/150\n",
      "\u001b[1m30/30\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 11ms/step - accuracy: 0.8033 - loss: 0.5765 - val_accuracy: 0.7380 - val_loss: 0.6977\n",
      "Epoch 65/150\n",
      "\u001b[1m30/30\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 9ms/step - accuracy: 0.8071 - loss: 0.5663 - val_accuracy: 0.7400 - val_loss: 0.6913\n",
      "Epoch 66/150\n",
      "\u001b[1m30/30\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 9ms/step - accuracy: 0.8038 - loss: 0.5615 - val_accuracy: 0.7390 - val_loss: 0.6889\n",
      "Epoch 67/150\n",
      "\u001b[1m30/30\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 9ms/step - accuracy: 0.8104 - loss: 0.5610 - val_accuracy: 0.7420 - val_loss: 0.6881\n",
      "Epoch 68/150\n",
      "\u001b[1m30/30\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 11ms/step - accuracy: 0.8136 - loss: 0.5433 - val_accuracy: 0.7440 - val_loss: 0.6864\n",
      "Epoch 69/150\n",
      "\u001b[1m30/30\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 12ms/step - accuracy: 0.8226 - loss: 0.5178 - val_accuracy: 0.7380 - val_loss: 0.6871\n",
      "Epoch 70/150\n",
      "\u001b[1m30/30\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 10ms/step - accuracy: 0.8158 - loss: 0.5436 - val_accuracy: 0.7410 - val_loss: 0.6806\n",
      "Epoch 71/150\n",
      "\u001b[1m30/30\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 10ms/step - accuracy: 0.8193 - loss: 0.5356 - val_accuracy: 0.7430 - val_loss: 0.6819\n",
      "Epoch 72/150\n",
      "\u001b[1m30/30\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - accuracy: 0.8228 - loss: 0.5246 - val_accuracy: 0.7410 - val_loss: 0.6813\n",
      "Epoch 73/150\n",
      "\u001b[1m30/30\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 9ms/step - accuracy: 0.8210 - loss: 0.5197 - val_accuracy: 0.7460 - val_loss: 0.6805\n",
      "Epoch 74/150\n",
      "\u001b[1m30/30\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 9ms/step - accuracy: 0.8225 - loss: 0.5190 - val_accuracy: 0.7410 - val_loss: 0.6760\n",
      "Epoch 75/150\n",
      "\u001b[1m30/30\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 9ms/step - accuracy: 0.8254 - loss: 0.5141 - val_accuracy: 0.7430 - val_loss: 0.6728\n",
      "Epoch 76/150\n",
      "\u001b[1m30/30\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - accuracy: 0.8346 - loss: 0.4984 - val_accuracy: 0.7470 - val_loss: 0.6725\n",
      "Epoch 77/150\n",
      "\u001b[1m30/30\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 9ms/step - accuracy: 0.8298 - loss: 0.5116 - val_accuracy: 0.7450 - val_loss: 0.6703\n",
      "Epoch 78/150\n",
      "\u001b[1m30/30\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 12ms/step - accuracy: 0.8288 - loss: 0.5100 - val_accuracy: 0.7450 - val_loss: 0.6707\n",
      "Epoch 79/150\n",
      "\u001b[1m30/30\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 10ms/step - accuracy: 0.8349 - loss: 0.4977 - val_accuracy: 0.7410 - val_loss: 0.6731\n",
      "Epoch 80/150\n",
      "\u001b[1m30/30\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 9ms/step - accuracy: 0.8310 - loss: 0.4975 - val_accuracy: 0.7400 - val_loss: 0.6730\n",
      "Epoch 81/150\n",
      "\u001b[1m30/30\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 10ms/step - accuracy: 0.8366 - loss: 0.4829 - val_accuracy: 0.7400 - val_loss: 0.6728\n",
      "Epoch 82/150\n",
      "\u001b[1m30/30\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 11ms/step - accuracy: 0.8307 - loss: 0.4946 - val_accuracy: 0.7410 - val_loss: 0.6700\n",
      "Epoch 83/150\n",
      "\u001b[1m30/30\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 10ms/step - accuracy: 0.8345 - loss: 0.4893 - val_accuracy: 0.7440 - val_loss: 0.6645\n",
      "Epoch 84/150\n",
      "\u001b[1m30/30\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 10ms/step - accuracy: 0.8342 - loss: 0.4842 - val_accuracy: 0.7450 - val_loss: 0.6639\n",
      "Epoch 85/150\n",
      "\u001b[1m30/30\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 10ms/step - accuracy: 0.8448 - loss: 0.4744 - val_accuracy: 0.7380 - val_loss: 0.6649\n",
      "Epoch 86/150\n",
      "\u001b[1m30/30\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 9ms/step - accuracy: 0.8408 - loss: 0.4725 - val_accuracy: 0.7430 - val_loss: 0.6653\n",
      "Epoch 87/150\n",
      "\u001b[1m30/30\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - accuracy: 0.8431 - loss: 0.4741 - val_accuracy: 0.7390 - val_loss: 0.6629\n",
      "Epoch 88/150\n",
      "\u001b[1m30/30\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - accuracy: 0.8390 - loss: 0.4741 - val_accuracy: 0.7410 - val_loss: 0.6636\n",
      "Epoch 89/150\n",
      "\u001b[1m30/30\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 9ms/step - accuracy: 0.8430 - loss: 0.4631 - val_accuracy: 0.7390 - val_loss: 0.6631\n",
      "Epoch 90/150\n",
      "\u001b[1m30/30\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 11ms/step - accuracy: 0.8369 - loss: 0.4756 - val_accuracy: 0.7470 - val_loss: 0.6606\n",
      "Epoch 91/150\n",
      "\u001b[1m30/30\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 10ms/step - accuracy: 0.8503 - loss: 0.4560 - val_accuracy: 0.7450 - val_loss: 0.6595\n",
      "Epoch 92/150\n",
      "\u001b[1m30/30\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 10ms/step - accuracy: 0.8423 - loss: 0.4510 - val_accuracy: 0.7390 - val_loss: 0.6633\n",
      "Epoch 93/150\n",
      "\u001b[1m30/30\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 9ms/step - accuracy: 0.8460 - loss: 0.4535 - val_accuracy: 0.7440 - val_loss: 0.6617\n",
      "Epoch 94/150\n",
      "\u001b[1m30/30\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 10ms/step - accuracy: 0.8563 - loss: 0.4390 - val_accuracy: 0.7440 - val_loss: 0.6580\n",
      "Epoch 95/150\n",
      "\u001b[1m30/30\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 9ms/step - accuracy: 0.8541 - loss: 0.4334 - val_accuracy: 0.7380 - val_loss: 0.6607\n",
      "Epoch 96/150\n",
      "\u001b[1m30/30\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 13ms/step - accuracy: 0.8496 - loss: 0.4468 - val_accuracy: 0.7430 - val_loss: 0.6587\n",
      "Epoch 97/150\n",
      "\u001b[1m30/30\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 10ms/step - accuracy: 0.8558 - loss: 0.4320 - val_accuracy: 0.7420 - val_loss: 0.6615\n",
      "Epoch 98/150\n",
      "\u001b[1m30/30\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 9ms/step - accuracy: 0.8559 - loss: 0.4360 - val_accuracy: 0.7390 - val_loss: 0.6570\n",
      "Epoch 99/150\n",
      "\u001b[1m30/30\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 11ms/step - accuracy: 0.8528 - loss: 0.4395 - val_accuracy: 0.7400 - val_loss: 0.6559\n",
      "Epoch 100/150\n",
      "\u001b[1m30/30\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 10ms/step - accuracy: 0.8583 - loss: 0.4205 - val_accuracy: 0.7420 - val_loss: 0.6576\n",
      "Epoch 101/150\n",
      "\u001b[1m30/30\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 9ms/step - accuracy: 0.8600 - loss: 0.4155 - val_accuracy: 0.7440 - val_loss: 0.6582\n",
      "Epoch 102/150\n",
      "\u001b[1m30/30\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 13ms/step - accuracy: 0.8608 - loss: 0.4283 - val_accuracy: 0.7390 - val_loss: 0.6579\n",
      "Epoch 103/150\n",
      "\u001b[1m30/30\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 10ms/step - accuracy: 0.8700 - loss: 0.4073 - val_accuracy: 0.7420 - val_loss: 0.6573\n",
      "Epoch 104/150\n",
      "\u001b[1m30/30\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 10ms/step - accuracy: 0.8657 - loss: 0.4145 - val_accuracy: 0.7420 - val_loss: 0.6583\n",
      "Epoch 105/150\n",
      "\u001b[1m30/30\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 10ms/step - accuracy: 0.8687 - loss: 0.4100 - val_accuracy: 0.7450 - val_loss: 0.6578\n",
      "Epoch 106/150\n",
      "\u001b[1m30/30\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 12ms/step - accuracy: 0.8649 - loss: 0.4121 - val_accuracy: 0.7440 - val_loss: 0.6604\n",
      "Epoch 107/150\n",
      "\u001b[1m30/30\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 9ms/step - accuracy: 0.8625 - loss: 0.4072 - val_accuracy: 0.7420 - val_loss: 0.6591\n",
      "Epoch 108/150\n",
      "\u001b[1m30/30\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - accuracy: 0.8709 - loss: 0.3963 - val_accuracy: 0.7400 - val_loss: 0.6606\n",
      "Epoch 109/150\n",
      "\u001b[1m30/30\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 9ms/step - accuracy: 0.8661 - loss: 0.4075 - val_accuracy: 0.7430 - val_loss: 0.6628\n",
      "Epoch 110/150\n",
      "\u001b[1m30/30\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 13ms/step - accuracy: 0.8705 - loss: 0.3931 - val_accuracy: 0.7420 - val_loss: 0.6555\n",
      "Epoch 111/150\n",
      "\u001b[1m30/30\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 10ms/step - accuracy: 0.8719 - loss: 0.3944 - val_accuracy: 0.7430 - val_loss: 0.6592\n",
      "Epoch 112/150\n",
      "\u001b[1m30/30\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 9ms/step - accuracy: 0.8668 - loss: 0.3976 - val_accuracy: 0.7480 - val_loss: 0.6622\n",
      "Epoch 113/150\n",
      "\u001b[1m30/30\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 12ms/step - accuracy: 0.8742 - loss: 0.3831 - val_accuracy: 0.7390 - val_loss: 0.6678\n",
      "Epoch 114/150\n",
      "\u001b[1m30/30\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 9ms/step - accuracy: 0.8773 - loss: 0.3846 - val_accuracy: 0.7400 - val_loss: 0.6579\n",
      "Epoch 115/150\n",
      "\u001b[1m30/30\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - accuracy: 0.8763 - loss: 0.3811 - val_accuracy: 0.7490 - val_loss: 0.6602\n",
      "Epoch 116/150\n",
      "\u001b[1m30/30\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 10ms/step - accuracy: 0.8798 - loss: 0.3795 - val_accuracy: 0.7420 - val_loss: 0.6615\n",
      "Epoch 117/150\n",
      "\u001b[1m30/30\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - accuracy: 0.8800 - loss: 0.3751 - val_accuracy: 0.7400 - val_loss: 0.6595\n",
      "Epoch 118/150\n",
      "\u001b[1m30/30\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 9ms/step - accuracy: 0.8822 - loss: 0.3690 - val_accuracy: 0.7390 - val_loss: 0.6607\n",
      "Epoch 119/150\n",
      "\u001b[1m30/30\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 10ms/step - accuracy: 0.8844 - loss: 0.3798 - val_accuracy: 0.7420 - val_loss: 0.6644\n",
      "Epoch 120/150\n",
      "\u001b[1m30/30\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 9ms/step - accuracy: 0.8834 - loss: 0.3698 - val_accuracy: 0.7460 - val_loss: 0.6666\n",
      "Epoch 121/150\n",
      "\u001b[1m30/30\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 9ms/step - accuracy: 0.8798 - loss: 0.3736 - val_accuracy: 0.7490 - val_loss: 0.6638\n",
      "Epoch 122/150\n",
      "\u001b[1m30/30\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 9ms/step - accuracy: 0.8810 - loss: 0.3642 - val_accuracy: 0.7370 - val_loss: 0.6628\n",
      "Epoch 123/150\n",
      "\u001b[1m30/30\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 10ms/step - accuracy: 0.8918 - loss: 0.3550 - val_accuracy: 0.7500 - val_loss: 0.6627\n",
      "Epoch 124/150\n",
      "\u001b[1m30/30\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 13ms/step - accuracy: 0.8862 - loss: 0.3485 - val_accuracy: 0.7430 - val_loss: 0.6621\n",
      "Epoch 125/150\n",
      "\u001b[1m30/30\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 10ms/step - accuracy: 0.8869 - loss: 0.3579 - val_accuracy: 0.7370 - val_loss: 0.6634\n",
      "Epoch 126/150\n",
      "\u001b[1m30/30\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 11ms/step - accuracy: 0.8939 - loss: 0.3462 - val_accuracy: 0.7460 - val_loss: 0.6689\n",
      "Epoch 127/150\n",
      "\u001b[1m30/30\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 12ms/step - accuracy: 0.8893 - loss: 0.3532 - val_accuracy: 0.7460 - val_loss: 0.6687\n",
      "Epoch 128/150\n",
      "\u001b[1m30/30\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 9ms/step - accuracy: 0.8846 - loss: 0.3481 - val_accuracy: 0.7400 - val_loss: 0.6635\n",
      "Epoch 129/150\n",
      "\u001b[1m30/30\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 9ms/step - accuracy: 0.8880 - loss: 0.3554 - val_accuracy: 0.7400 - val_loss: 0.6642\n",
      "Epoch 130/150\n",
      "\u001b[1m30/30\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 13ms/step - accuracy: 0.8927 - loss: 0.3410 - val_accuracy: 0.7440 - val_loss: 0.6679\n",
      "Epoch 131/150\n",
      "\u001b[1m30/30\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 9ms/step - accuracy: 0.8927 - loss: 0.3468 - val_accuracy: 0.7370 - val_loss: 0.6646\n",
      "Epoch 132/150\n",
      "\u001b[1m30/30\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 10ms/step - accuracy: 0.8977 - loss: 0.3341 - val_accuracy: 0.7390 - val_loss: 0.6677\n",
      "Epoch 133/150\n",
      "\u001b[1m30/30\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 13ms/step - accuracy: 0.8931 - loss: 0.3384 - val_accuracy: 0.7430 - val_loss: 0.6684\n",
      "Epoch 134/150\n",
      "\u001b[1m30/30\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - accuracy: 0.8935 - loss: 0.3386 - val_accuracy: 0.7410 - val_loss: 0.6693\n",
      "Epoch 135/150\n",
      "\u001b[1m30/30\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 9ms/step - accuracy: 0.9012 - loss: 0.3291 - val_accuracy: 0.7400 - val_loss: 0.6738\n",
      "Epoch 136/150\n",
      "\u001b[1m30/30\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 9ms/step - accuracy: 0.9021 - loss: 0.3319 - val_accuracy: 0.7380 - val_loss: 0.6698\n",
      "Epoch 137/150\n",
      "\u001b[1m30/30\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 10ms/step - accuracy: 0.8989 - loss: 0.3296 - val_accuracy: 0.7400 - val_loss: 0.6774\n",
      "Epoch 138/150\n",
      "\u001b[1m30/30\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 11ms/step - accuracy: 0.9002 - loss: 0.3221 - val_accuracy: 0.7430 - val_loss: 0.6706\n",
      "Epoch 139/150\n",
      "\u001b[1m30/30\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 10ms/step - accuracy: 0.9005 - loss: 0.3269 - val_accuracy: 0.7460 - val_loss: 0.6722\n",
      "Epoch 140/150\n",
      "\u001b[1m30/30\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 9ms/step - accuracy: 0.9038 - loss: 0.3185 - val_accuracy: 0.7470 - val_loss: 0.6710\n",
      "Epoch 141/150\n",
      "\u001b[1m30/30\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - accuracy: 0.9062 - loss: 0.3212 - val_accuracy: 0.7470 - val_loss: 0.6732\n",
      "Epoch 142/150\n",
      "\u001b[1m30/30\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 9ms/step - accuracy: 0.9044 - loss: 0.3159 - val_accuracy: 0.7470 - val_loss: 0.6726\n",
      "Epoch 143/150\n",
      "\u001b[1m30/30\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - accuracy: 0.9072 - loss: 0.3200 - val_accuracy: 0.7470 - val_loss: 0.6766\n",
      "Epoch 144/150\n",
      "\u001b[1m30/30\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 9ms/step - accuracy: 0.9089 - loss: 0.3012 - val_accuracy: 0.7450 - val_loss: 0.6774\n",
      "Epoch 145/150\n",
      "\u001b[1m30/30\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 10ms/step - accuracy: 0.9046 - loss: 0.3154 - val_accuracy: 0.7390 - val_loss: 0.6776\n",
      "Epoch 146/150\n",
      "\u001b[1m30/30\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 10ms/step - accuracy: 0.9051 - loss: 0.3065 - val_accuracy: 0.7450 - val_loss: 0.6825\n",
      "Epoch 147/150\n",
      "\u001b[1m30/30\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 10ms/step - accuracy: 0.9095 - loss: 0.3034 - val_accuracy: 0.7410 - val_loss: 0.6831\n",
      "Epoch 148/150\n",
      "\u001b[1m30/30\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 9ms/step - accuracy: 0.9069 - loss: 0.3134 - val_accuracy: 0.7420 - val_loss: 0.6812\n",
      "Epoch 149/150\n",
      "\u001b[1m30/30\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 10ms/step - accuracy: 0.9077 - loss: 0.3047 - val_accuracy: 0.7450 - val_loss: 0.6845\n",
      "Epoch 150/150\n",
      "\u001b[1m30/30\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - accuracy: 0.9094 - loss: 0.3067 - val_accuracy: 0.7480 - val_loss: 0.6801\n"
     ]
    }
   ],
   "source": [
    "# Train the model\n",
    "baseline_model_val = baseline_model.fit(X_train_tokens, y_train_lb, epochs=150, batch_size=256, validation_data=(X_val_tokens, y_val_lb))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model Performance\n",
    "\n",
    "The attribute `.history` (stored as a dictionary) contains four entries now: one per metric that was being monitored during training and validation. Print the keys of this dictionary for confirmation: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dict_keys(['accuracy', 'loss', 'val_accuracy', 'val_loss'])"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Access the history attribute and store the dictionary\n",
    "history = baseline_model_val\n",
    "baseline_model_val_dict = history.history\n",
    "\n",
    "# Print the keys\n",
    "baseline_model_val_dict.keys()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Evaluate this model on the training data: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m235/235\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.9137 - loss: 0.2942\n",
      "----------\n",
      "Training Loss: 0.297 \n",
      "Training Accuracy: 0.912\n"
     ]
    }
   ],
   "source": [
    "results_train = baseline_model.evaluate(X_train_tokens, y_train_lb)\n",
    "print('----------')\n",
    "print(f'Training Loss: {results_train[0]:.3} \\nTraining Accuracy: {results_train[1]:.3}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Evaluate this model on the test data: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m47/47\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.7869 - loss: 0.5919\n",
      "----------\n",
      "Test Loss: 0.611 \n",
      "Test Accuracy: 0.788\n"
     ]
    }
   ],
   "source": [
    "results_test = baseline_model.evaluate(X_test_tokens, y_test_lb)\n",
    "print('----------')\n",
    "print(f'Test Loss: {results_test[0]:.3} \\nTest Accuracy: {results_test[1]:.3}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Plot the Results \n",
    "\n",
    "Plot the loss versus the number of epochs. Be sure to include the training and the validation loss in the same plot. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Loss vs number of epochs with train and validation sets\n",
    "plt.plot(history.history['loss'], label='Training Loss')\n",
    "plt.plot(history.history['val_loss'], label='Validation Loss')\n",
    "plt.title('Loss vs Number of Epochs')\n",
    "plt.xlabel('Epochs')\n",
    "plt.ylabel('Loss')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create a second plot comparing training and validation accuracy to the number of epochs. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Accuracy vs number of epochs with train and validation sets\n",
    "plt.plot(history.history['accuracy'])\n",
    "plt.plot(history.history['val_accuracy'])\n",
    "plt.title('Accuracy Vs Number of Epochs')\n",
    "plt.ylabel('Accuracy')\n",
    "plt.xlabel('Epoch')\n",
    "plt.legend(['Train', 'Validation'], loc='lower right')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Did you notice an interesting pattern here? Although the training accuracy keeps increasing when going through more epochs, and the training loss keeps decreasing, the validation accuracy and loss don't necessarily do the same. After a certain point, validation accuracy keeps swinging, which means that you're probably **overfitting** the model to the training data when you train for many epochs past a certain dropoff point. Let's tackle this now. You will now specify an early stopping point when training your model. \n",
    "\n",
    "\n",
    "## Early Stopping\n",
    "\n",
    "Overfitting neural networks is something you **_want_** to avoid at all costs. However, it's not possible to know in advance how many *epochs* you need to train your model on, and running the model multiple times with varying number of *epochs* maybe helpful, but is a time-consuming process. \n",
    "\n",
    "We've defined a model with the same architecture as above. This time specify an early stopping point when training the model. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "random.seed(123)\n",
    "model_2 = models.Sequential()\n",
    "model_2.add(layers.Dense(50, activation='relu', input_shape=(2000,)))\n",
    "model_2.add(layers.Dense(25, activation='relu'))\n",
    "model_2.add(layers.Dense(7, activation='softmax'))\n",
    "\n",
    "model_2.compile(optimizer='SGD', \n",
    "                loss='categorical_crossentropy', \n",
    "                metrics=['acc'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Import `EarlyStopping` and `ModelCheckpoint` from `keras.callbacks` \n",
    "- Define a list, `early_stopping`: \n",
    "  - Monitor `'val_loss'` and continue training for 10 epochs before stopping \n",
    "  - Save the best model while monitoring `'val_loss'` \n",
    " \n",
    "> If you need help, consult [documentation](https://keras.io/callbacks/).   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import EarlyStopping and ModelCheckpoint\n",
    "from tensorflow.keras.callbacks import EarlyStopping, ModelCheckpoint\n",
    "\n",
    "# Define the callbacks\n",
    "early_stopping = EarlyStopping(monitor='val_loss', patience=10)\n",
    "model_checkpoint = ModelCheckpoint('best_model.keras', monitor='val_loss', save_best_only=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Train `model_2`. Make sure you set the `callbacks` argument to `early_stopping`. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/150\n",
      "\u001b[1m30/30\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 19ms/step - acc: 0.1292 - loss: 1.9600 - val_acc: 0.1490 - val_loss: 1.9544\n",
      "Epoch 2/150\n",
      "\u001b[1m30/30\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - acc: 0.1770 - loss: 1.9384 - val_acc: 0.1850 - val_loss: 1.9399\n",
      "Epoch 3/150\n",
      "\u001b[1m30/30\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 10ms/step - acc: 0.2081 - loss: 1.9255 - val_acc: 0.2100 - val_loss: 1.9265\n",
      "Epoch 4/150\n",
      "\u001b[1m30/30\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 9ms/step - acc: 0.2329 - loss: 1.9125 - val_acc: 0.2450 - val_loss: 1.9106\n",
      "Epoch 5/150\n",
      "\u001b[1m30/30\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 9ms/step - acc: 0.2592 - loss: 1.8937 - val_acc: 0.2660 - val_loss: 1.8920\n",
      "Epoch 6/150\n",
      "\u001b[1m30/30\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - acc: 0.2861 - loss: 1.8681 - val_acc: 0.2840 - val_loss: 1.8703\n",
      "Epoch 7/150\n",
      "\u001b[1m30/30\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - acc: 0.3059 - loss: 1.8477 - val_acc: 0.3150 - val_loss: 1.8437\n",
      "Epoch 8/150\n",
      "\u001b[1m30/30\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - acc: 0.3236 - loss: 1.8229 - val_acc: 0.3290 - val_loss: 1.8126\n",
      "Epoch 9/150\n",
      "\u001b[1m30/30\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - acc: 0.3419 - loss: 1.7852 - val_acc: 0.3480 - val_loss: 1.7773\n",
      "Epoch 10/150\n",
      "\u001b[1m30/30\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - acc: 0.3659 - loss: 1.7450 - val_acc: 0.3720 - val_loss: 1.7369\n",
      "Epoch 11/150\n",
      "\u001b[1m30/30\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - acc: 0.3998 - loss: 1.6978 - val_acc: 0.4040 - val_loss: 1.6921\n",
      "Epoch 12/150\n",
      "\u001b[1m30/30\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 10ms/step - acc: 0.4242 - loss: 1.6542 - val_acc: 0.4350 - val_loss: 1.6431\n",
      "Epoch 13/150\n",
      "\u001b[1m30/30\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - acc: 0.4584 - loss: 1.6004 - val_acc: 0.4580 - val_loss: 1.5923\n",
      "Epoch 14/150\n",
      "\u001b[1m30/30\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - acc: 0.4820 - loss: 1.5508 - val_acc: 0.4840 - val_loss: 1.5361\n",
      "Epoch 15/150\n",
      "\u001b[1m30/30\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - acc: 0.5173 - loss: 1.4928 - val_acc: 0.5000 - val_loss: 1.4815\n",
      "Epoch 16/150\n",
      "\u001b[1m30/30\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - acc: 0.5301 - loss: 1.4401 - val_acc: 0.5280 - val_loss: 1.4254\n",
      "Epoch 17/150\n",
      "\u001b[1m30/30\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 10ms/step - acc: 0.5598 - loss: 1.3771 - val_acc: 0.5530 - val_loss: 1.3717\n",
      "Epoch 18/150\n",
      "\u001b[1m30/30\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - acc: 0.5965 - loss: 1.3180 - val_acc: 0.5790 - val_loss: 1.3209\n",
      "Epoch 19/150\n",
      "\u001b[1m30/30\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - acc: 0.6161 - loss: 1.2638 - val_acc: 0.5980 - val_loss: 1.2693\n",
      "Epoch 20/150\n",
      "\u001b[1m30/30\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - acc: 0.6297 - loss: 1.2078 - val_acc: 0.6120 - val_loss: 1.2223\n",
      "Epoch 21/150\n",
      "\u001b[1m30/30\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - acc: 0.6386 - loss: 1.1695 - val_acc: 0.6280 - val_loss: 1.1829\n",
      "Epoch 22/150\n",
      "\u001b[1m30/30\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - acc: 0.6494 - loss: 1.1369 - val_acc: 0.6500 - val_loss: 1.1396\n",
      "Epoch 23/150\n",
      "\u001b[1m30/30\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - acc: 0.6743 - loss: 1.0840 - val_acc: 0.6550 - val_loss: 1.1021\n",
      "Epoch 24/150\n",
      "\u001b[1m30/30\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - acc: 0.6750 - loss: 1.0511 - val_acc: 0.6640 - val_loss: 1.0716\n",
      "Epoch 25/150\n",
      "\u001b[1m30/30\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - acc: 0.6875 - loss: 1.0077 - val_acc: 0.6730 - val_loss: 1.0418\n",
      "Epoch 26/150\n",
      "\u001b[1m30/30\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 9ms/step - acc: 0.6956 - loss: 0.9764 - val_acc: 0.6790 - val_loss: 1.0128\n",
      "Epoch 27/150\n",
      "\u001b[1m30/30\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - acc: 0.6957 - loss: 0.9641 - val_acc: 0.6730 - val_loss: 0.9924\n",
      "Epoch 28/150\n",
      "\u001b[1m30/30\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - acc: 0.6901 - loss: 0.9366 - val_acc: 0.6870 - val_loss: 0.9627\n",
      "Epoch 29/150\n",
      "\u001b[1m30/30\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 10ms/step - acc: 0.7063 - loss: 0.9091 - val_acc: 0.6860 - val_loss: 0.9433\n",
      "Epoch 30/150\n",
      "\u001b[1m30/30\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 9ms/step - acc: 0.7147 - loss: 0.8832 - val_acc: 0.6900 - val_loss: 0.9228\n",
      "Epoch 31/150\n",
      "\u001b[1m30/30\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 10ms/step - acc: 0.7196 - loss: 0.8753 - val_acc: 0.7020 - val_loss: 0.9052\n",
      "Epoch 32/150\n",
      "\u001b[1m30/30\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - acc: 0.7225 - loss: 0.8400 - val_acc: 0.7040 - val_loss: 0.8912\n",
      "Epoch 33/150\n",
      "\u001b[1m30/30\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - acc: 0.7252 - loss: 0.8224 - val_acc: 0.7030 - val_loss: 0.8816\n",
      "Epoch 34/150\n",
      "\u001b[1m30/30\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - acc: 0.7339 - loss: 0.8076 - val_acc: 0.7070 - val_loss: 0.8604\n",
      "Epoch 35/150\n",
      "\u001b[1m30/30\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - acc: 0.7391 - loss: 0.7887 - val_acc: 0.7110 - val_loss: 0.8491\n",
      "Epoch 36/150\n",
      "\u001b[1m30/30\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 10ms/step - acc: 0.7378 - loss: 0.7937 - val_acc: 0.7110 - val_loss: 0.8422\n",
      "Epoch 37/150\n",
      "\u001b[1m30/30\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 10ms/step - acc: 0.7454 - loss: 0.7707 - val_acc: 0.7110 - val_loss: 0.8281\n",
      "Epoch 38/150\n",
      "\u001b[1m30/30\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 9ms/step - acc: 0.7427 - loss: 0.7550 - val_acc: 0.7120 - val_loss: 0.8159\n",
      "Epoch 39/150\n",
      "\u001b[1m30/30\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 9ms/step - acc: 0.7418 - loss: 0.7568 - val_acc: 0.7150 - val_loss: 0.8076\n",
      "Epoch 40/150\n",
      "\u001b[1m30/30\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 11ms/step - acc: 0.7559 - loss: 0.7293 - val_acc: 0.7210 - val_loss: 0.7994\n",
      "Epoch 41/150\n",
      "\u001b[1m30/30\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - acc: 0.7568 - loss: 0.7216 - val_acc: 0.7170 - val_loss: 0.7956\n",
      "Epoch 42/150\n",
      "\u001b[1m30/30\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - acc: 0.7611 - loss: 0.7101 - val_acc: 0.7210 - val_loss: 0.7864\n",
      "Epoch 43/150\n",
      "\u001b[1m30/30\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - acc: 0.7532 - loss: 0.7133 - val_acc: 0.7260 - val_loss: 0.7769\n",
      "Epoch 44/150\n",
      "\u001b[1m30/30\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 9ms/step - acc: 0.7655 - loss: 0.6945 - val_acc: 0.7260 - val_loss: 0.7695\n",
      "Epoch 45/150\n",
      "\u001b[1m30/30\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - acc: 0.7661 - loss: 0.6804 - val_acc: 0.7270 - val_loss: 0.7663\n",
      "Epoch 46/150\n",
      "\u001b[1m30/30\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 9ms/step - acc: 0.7650 - loss: 0.6863 - val_acc: 0.7310 - val_loss: 0.7562\n",
      "Epoch 47/150\n",
      "\u001b[1m30/30\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - acc: 0.7676 - loss: 0.6643 - val_acc: 0.7280 - val_loss: 0.7506\n",
      "Epoch 48/150\n",
      "\u001b[1m30/30\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - acc: 0.7808 - loss: 0.6575 - val_acc: 0.7350 - val_loss: 0.7456\n",
      "Epoch 49/150\n",
      "\u001b[1m30/30\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - acc: 0.7650 - loss: 0.6725 - val_acc: 0.7310 - val_loss: 0.7415\n",
      "Epoch 50/150\n",
      "\u001b[1m30/30\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - acc: 0.7725 - loss: 0.6596 - val_acc: 0.7320 - val_loss: 0.7364\n",
      "Epoch 51/150\n",
      "\u001b[1m30/30\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - acc: 0.7724 - loss: 0.6495 - val_acc: 0.7310 - val_loss: 0.7333\n",
      "Epoch 52/150\n",
      "\u001b[1m30/30\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 9ms/step - acc: 0.7859 - loss: 0.6254 - val_acc: 0.7340 - val_loss: 0.7287\n",
      "Epoch 53/150\n",
      "\u001b[1m30/30\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 10ms/step - acc: 0.7808 - loss: 0.6330 - val_acc: 0.7320 - val_loss: 0.7243\n",
      "Epoch 54/150\n",
      "\u001b[1m30/30\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 10ms/step - acc: 0.7854 - loss: 0.6145 - val_acc: 0.7250 - val_loss: 0.7247\n",
      "Epoch 55/150\n",
      "\u001b[1m30/30\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - acc: 0.7795 - loss: 0.6259 - val_acc: 0.7290 - val_loss: 0.7200\n",
      "Epoch 56/150\n",
      "\u001b[1m30/30\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 10ms/step - acc: 0.7877 - loss: 0.6117 - val_acc: 0.7360 - val_loss: 0.7149\n",
      "Epoch 57/150\n",
      "\u001b[1m30/30\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - acc: 0.7907 - loss: 0.5971 - val_acc: 0.7300 - val_loss: 0.7199\n",
      "Epoch 58/150\n",
      "\u001b[1m30/30\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - acc: 0.7910 - loss: 0.6039 - val_acc: 0.7300 - val_loss: 0.7094\n",
      "Epoch 59/150\n",
      "\u001b[1m30/30\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - acc: 0.7923 - loss: 0.5981 - val_acc: 0.7360 - val_loss: 0.7040\n",
      "Epoch 60/150\n",
      "\u001b[1m30/30\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - acc: 0.7899 - loss: 0.5931 - val_acc: 0.7360 - val_loss: 0.7034\n",
      "Epoch 61/150\n",
      "\u001b[1m30/30\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - acc: 0.7945 - loss: 0.5950 - val_acc: 0.7370 - val_loss: 0.7024\n",
      "Epoch 62/150\n",
      "\u001b[1m30/30\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 9ms/step - acc: 0.7969 - loss: 0.5840 - val_acc: 0.7460 - val_loss: 0.6971\n",
      "Epoch 63/150\n",
      "\u001b[1m30/30\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - acc: 0.8008 - loss: 0.5749 - val_acc: 0.7380 - val_loss: 0.6968\n",
      "Epoch 64/150\n",
      "\u001b[1m30/30\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - acc: 0.8073 - loss: 0.5578 - val_acc: 0.7390 - val_loss: 0.6949\n",
      "Epoch 65/150\n",
      "\u001b[1m30/30\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - acc: 0.8026 - loss: 0.5570 - val_acc: 0.7430 - val_loss: 0.6892\n",
      "Epoch 66/150\n",
      "\u001b[1m30/30\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - acc: 0.8122 - loss: 0.5551 - val_acc: 0.7400 - val_loss: 0.6908\n",
      "Epoch 67/150\n",
      "\u001b[1m30/30\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - acc: 0.8149 - loss: 0.5414 - val_acc: 0.7400 - val_loss: 0.6897\n",
      "Epoch 68/150\n",
      "\u001b[1m30/30\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - acc: 0.8068 - loss: 0.5466 - val_acc: 0.7480 - val_loss: 0.6829\n",
      "Epoch 69/150\n",
      "\u001b[1m30/30\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - acc: 0.8166 - loss: 0.5477 - val_acc: 0.7450 - val_loss: 0.6813\n",
      "Epoch 70/150\n",
      "\u001b[1m30/30\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - acc: 0.8170 - loss: 0.5268 - val_acc: 0.7460 - val_loss: 0.6794\n",
      "Epoch 71/150\n",
      "\u001b[1m30/30\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 9ms/step - acc: 0.8173 - loss: 0.5319 - val_acc: 0.7440 - val_loss: 0.6780\n",
      "Epoch 72/150\n",
      "\u001b[1m30/30\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - acc: 0.8234 - loss: 0.5297 - val_acc: 0.7420 - val_loss: 0.6773\n",
      "Epoch 73/150\n",
      "\u001b[1m30/30\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - acc: 0.8255 - loss: 0.5193 - val_acc: 0.7380 - val_loss: 0.6808\n",
      "Epoch 74/150\n",
      "\u001b[1m30/30\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 9ms/step - acc: 0.8237 - loss: 0.5130 - val_acc: 0.7450 - val_loss: 0.6740\n",
      "Epoch 75/150\n",
      "\u001b[1m30/30\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - acc: 0.8271 - loss: 0.5159 - val_acc: 0.7430 - val_loss: 0.6754\n",
      "Epoch 76/150\n",
      "\u001b[1m30/30\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - acc: 0.8230 - loss: 0.5178 - val_acc: 0.7440 - val_loss: 0.6745\n",
      "Epoch 77/150\n",
      "\u001b[1m30/30\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 9ms/step - acc: 0.8325 - loss: 0.5085 - val_acc: 0.7450 - val_loss: 0.6725\n",
      "Epoch 78/150\n",
      "\u001b[1m30/30\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - acc: 0.8292 - loss: 0.5033 - val_acc: 0.7450 - val_loss: 0.6707\n",
      "Epoch 79/150\n",
      "\u001b[1m30/30\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - acc: 0.8340 - loss: 0.4949 - val_acc: 0.7440 - val_loss: 0.6723\n",
      "Epoch 80/150\n",
      "\u001b[1m30/30\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - acc: 0.8338 - loss: 0.4923 - val_acc: 0.7390 - val_loss: 0.6773\n",
      "Epoch 81/150\n",
      "\u001b[1m30/30\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - acc: 0.8326 - loss: 0.4911 - val_acc: 0.7410 - val_loss: 0.6695\n",
      "Epoch 82/150\n",
      "\u001b[1m30/30\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - acc: 0.8433 - loss: 0.4759 - val_acc: 0.7400 - val_loss: 0.6691\n",
      "Epoch 83/150\n",
      "\u001b[1m30/30\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 10ms/step - acc: 0.8353 - loss: 0.4896 - val_acc: 0.7400 - val_loss: 0.6665\n",
      "Epoch 84/150\n",
      "\u001b[1m30/30\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - acc: 0.8326 - loss: 0.4927 - val_acc: 0.7370 - val_loss: 0.6697\n",
      "Epoch 85/150\n",
      "\u001b[1m30/30\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 9ms/step - acc: 0.8446 - loss: 0.4717 - val_acc: 0.7440 - val_loss: 0.6636\n",
      "Epoch 86/150\n",
      "\u001b[1m30/30\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 9ms/step - acc: 0.8393 - loss: 0.4657 - val_acc: 0.7430 - val_loss: 0.6635\n",
      "Epoch 87/150\n",
      "\u001b[1m30/30\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 10ms/step - acc: 0.8375 - loss: 0.4711 - val_acc: 0.7400 - val_loss: 0.6699\n",
      "Epoch 88/150\n",
      "\u001b[1m30/30\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 10ms/step - acc: 0.8406 - loss: 0.4696 - val_acc: 0.7440 - val_loss: 0.6626\n",
      "Epoch 89/150\n",
      "\u001b[1m30/30\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 9ms/step - acc: 0.8450 - loss: 0.4692 - val_acc: 0.7410 - val_loss: 0.6642\n",
      "Epoch 90/150\n",
      "\u001b[1m30/30\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - acc: 0.8416 - loss: 0.4596 - val_acc: 0.7400 - val_loss: 0.6676\n",
      "Epoch 91/150\n",
      "\u001b[1m30/30\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 10ms/step - acc: 0.8424 - loss: 0.4592 - val_acc: 0.7420 - val_loss: 0.6610\n",
      "Epoch 92/150\n",
      "\u001b[1m30/30\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 11ms/step - acc: 0.8438 - loss: 0.4615 - val_acc: 0.7410 - val_loss: 0.6608\n",
      "Epoch 93/150\n",
      "\u001b[1m30/30\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 9ms/step - acc: 0.8518 - loss: 0.4504 - val_acc: 0.7400 - val_loss: 0.6604\n",
      "Epoch 94/150\n",
      "\u001b[1m30/30\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - acc: 0.8519 - loss: 0.4501 - val_acc: 0.7370 - val_loss: 0.6627\n",
      "Epoch 95/150\n",
      "\u001b[1m30/30\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 9ms/step - acc: 0.8544 - loss: 0.4366 - val_acc: 0.7400 - val_loss: 0.6581\n",
      "Epoch 96/150\n",
      "\u001b[1m30/30\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 9ms/step - acc: 0.8540 - loss: 0.4474 - val_acc: 0.7360 - val_loss: 0.6604\n",
      "Epoch 97/150\n",
      "\u001b[1m30/30\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 9ms/step - acc: 0.8559 - loss: 0.4367 - val_acc: 0.7400 - val_loss: 0.6635\n",
      "Epoch 98/150\n",
      "\u001b[1m30/30\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 9ms/step - acc: 0.8583 - loss: 0.4345 - val_acc: 0.7400 - val_loss: 0.6589\n",
      "Epoch 99/150\n",
      "\u001b[1m30/30\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 9ms/step - acc: 0.8595 - loss: 0.4308 - val_acc: 0.7430 - val_loss: 0.6583\n",
      "Epoch 100/150\n",
      "\u001b[1m30/30\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 9ms/step - acc: 0.8656 - loss: 0.4173 - val_acc: 0.7360 - val_loss: 0.6627\n",
      "Epoch 101/150\n",
      "\u001b[1m30/30\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - acc: 0.8653 - loss: 0.4153 - val_acc: 0.7410 - val_loss: 0.6628\n",
      "Epoch 102/150\n",
      "\u001b[1m30/30\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 11ms/step - acc: 0.8634 - loss: 0.4250 - val_acc: 0.7400 - val_loss: 0.6574\n",
      "Epoch 103/150\n",
      "\u001b[1m30/30\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 10ms/step - acc: 0.8567 - loss: 0.4205 - val_acc: 0.7420 - val_loss: 0.6583\n",
      "Epoch 104/150\n",
      "\u001b[1m30/30\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 11ms/step - acc: 0.8607 - loss: 0.4236 - val_acc: 0.7400 - val_loss: 0.6623\n",
      "Epoch 105/150\n",
      "\u001b[1m30/30\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 11ms/step - acc: 0.8664 - loss: 0.4097 - val_acc: 0.7420 - val_loss: 0.6571\n",
      "Epoch 106/150\n",
      "\u001b[1m30/30\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 10ms/step - acc: 0.8679 - loss: 0.4063 - val_acc: 0.7440 - val_loss: 0.6584\n",
      "Epoch 107/150\n",
      "\u001b[1m30/30\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 9ms/step - acc: 0.8746 - loss: 0.3981 - val_acc: 0.7440 - val_loss: 0.6597\n",
      "Epoch 108/150\n",
      "\u001b[1m30/30\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 10ms/step - acc: 0.8678 - loss: 0.4008 - val_acc: 0.7400 - val_loss: 0.6615\n",
      "Epoch 109/150\n",
      "\u001b[1m30/30\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 9ms/step - acc: 0.8668 - loss: 0.3995 - val_acc: 0.7450 - val_loss: 0.6629\n",
      "Epoch 110/150\n",
      "\u001b[1m30/30\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - acc: 0.8625 - loss: 0.4033 - val_acc: 0.7420 - val_loss: 0.6607\n",
      "Epoch 111/150\n",
      "\u001b[1m30/30\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - acc: 0.8735 - loss: 0.3943 - val_acc: 0.7410 - val_loss: 0.6572\n",
      "Epoch 112/150\n",
      "\u001b[1m30/30\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 9ms/step - acc: 0.8755 - loss: 0.3894 - val_acc: 0.7420 - val_loss: 0.6595\n",
      "Epoch 113/150\n",
      "\u001b[1m30/30\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 10ms/step - acc: 0.8710 - loss: 0.3849 - val_acc: 0.7420 - val_loss: 0.6585\n",
      "Epoch 114/150\n",
      "\u001b[1m30/30\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - acc: 0.8696 - loss: 0.3969 - val_acc: 0.7430 - val_loss: 0.6600\n",
      "Epoch 115/150\n",
      "\u001b[1m30/30\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 9ms/step - acc: 0.8713 - loss: 0.3875 - val_acc: 0.7440 - val_loss: 0.6608\n"
     ]
    }
   ],
   "source": [
    "history_2 = model_2.fit(X_train_tokens, y_train_lb, epochs=150, batch_size=256, validation_data=(X_val_tokens, y_val_lb), callbacks=[early_stopping, model_checkpoint])\n",
    "\n",
    "model_2_val = history_2.history"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Load the best (saved) model. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the best (saved) model\n",
    "from tensorflow.keras.models import load_model\n",
    "\n",
    "saved_model = load_model('best_model.keras') "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, use this model to to calculate the training and test accuracy: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m235/235\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - acc: 0.8705 - loss: 0.4049\n",
      "Training Loss: 0.408 \n",
      "Training Accuracy: 0.867\n",
      "----------\n",
      "\u001b[1m47/47\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - acc: 0.7992 - loss: 0.5802\n",
      "Test Loss: 0.595 \n",
      "Test Accuracy: 0.79\n"
     ]
    }
   ],
   "source": [
    "results_train = saved_model.evaluate(X_train_tokens, y_train_lb)\n",
    "print(f'Training Loss: {results_train[0]:.3} \\nTraining Accuracy: {results_train[1]:.3}')\n",
    "\n",
    "print('----------')\n",
    "\n",
    "results_test = saved_model.evaluate(X_test_tokens, y_test_lb)\n",
    "print(f'Test Loss: {results_test[0]:.3} \\nTest Accuracy: {results_test[1]:.3}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Nicely done! Did you notice that the model didn't train for all 150 epochs? You reduced your training time. \n",
    "\n",
    "Now, take a look at how regularization techniques can further improve your model performance. \n",
    "\n",
    "## L2 Regularization \n",
    "\n",
    "First, take a look at L2 regularization. Keras makes L2 regularization easy. Simply add the `kernel_regularizer=keras.regularizers.l2(lambda_coeff)` parameter to any model layer. The `lambda_coeff` parameter determines the strength of the regularization you wish to perform. \n",
    "\n",
    "- Use 2 hidden layers with 50 units in the first and 25 in the second layer, both with `'relu'` activation functions \n",
    "- Add L2 regularization to both the hidden layers with 0.005 as the `lambda_coeff` "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/150\n",
      "\u001b[1m30/30\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 19ms/step - acc: 0.1303 - loss: 2.6172 - val_acc: 0.1510 - val_loss: 2.6082\n",
      "Epoch 2/150\n",
      "\u001b[1m30/30\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 10ms/step - acc: 0.1707 - loss: 2.5939 - val_acc: 0.1850 - val_loss: 2.5899\n",
      "Epoch 3/150\n",
      "\u001b[1m30/30\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - acc: 0.2069 - loss: 2.5746 - val_acc: 0.2130 - val_loss: 2.5724\n",
      "Epoch 4/150\n",
      "\u001b[1m30/30\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - acc: 0.2423 - loss: 2.5578 - val_acc: 0.2470 - val_loss: 2.5536\n",
      "Epoch 5/150\n",
      "\u001b[1m30/30\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - acc: 0.2613 - loss: 2.5368 - val_acc: 0.2690 - val_loss: 2.5313\n",
      "Epoch 6/150\n",
      "\u001b[1m30/30\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - acc: 0.2845 - loss: 2.5104 - val_acc: 0.2970 - val_loss: 2.5061\n",
      "Epoch 7/150\n",
      "\u001b[1m30/30\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - acc: 0.3043 - loss: 2.4824 - val_acc: 0.3200 - val_loss: 2.4780\n",
      "Epoch 8/150\n",
      "\u001b[1m30/30\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - acc: 0.3240 - loss: 2.4578 - val_acc: 0.3330 - val_loss: 2.4448\n",
      "Epoch 9/150\n",
      "\u001b[1m30/30\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - acc: 0.3433 - loss: 2.4203 - val_acc: 0.3400 - val_loss: 2.4086\n",
      "Epoch 10/150\n",
      "\u001b[1m30/30\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - acc: 0.3655 - loss: 2.3804 - val_acc: 0.3750 - val_loss: 2.3656\n",
      "Epoch 11/150\n",
      "\u001b[1m30/30\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - acc: 0.3949 - loss: 2.3360 - val_acc: 0.3980 - val_loss: 2.3206\n",
      "Epoch 12/150\n",
      "\u001b[1m30/30\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - acc: 0.4160 - loss: 2.2877 - val_acc: 0.4280 - val_loss: 2.2733\n",
      "Epoch 13/150\n",
      "\u001b[1m30/30\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - acc: 0.4502 - loss: 2.2310 - val_acc: 0.4570 - val_loss: 2.2202\n",
      "Epoch 14/150\n",
      "\u001b[1m30/30\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - acc: 0.4841 - loss: 2.1763 - val_acc: 0.4710 - val_loss: 2.1702\n",
      "Epoch 15/150\n",
      "\u001b[1m30/30\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - acc: 0.5199 - loss: 2.1153 - val_acc: 0.5090 - val_loss: 2.1120\n",
      "Epoch 16/150\n",
      "\u001b[1m30/30\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - acc: 0.5360 - loss: 2.0664 - val_acc: 0.5250 - val_loss: 2.0584\n",
      "Epoch 17/150\n",
      "\u001b[1m30/30\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - acc: 0.5534 - loss: 2.0113 - val_acc: 0.5510 - val_loss: 2.0029\n",
      "Epoch 18/150\n",
      "\u001b[1m30/30\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - acc: 0.5768 - loss: 1.9614 - val_acc: 0.5660 - val_loss: 1.9522\n",
      "Epoch 19/150\n",
      "\u001b[1m30/30\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - acc: 0.6013 - loss: 1.9034 - val_acc: 0.5870 - val_loss: 1.9013\n",
      "Epoch 20/150\n",
      "\u001b[1m30/30\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - acc: 0.6135 - loss: 1.8520 - val_acc: 0.5980 - val_loss: 1.8596\n",
      "Epoch 21/150\n",
      "\u001b[1m30/30\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - acc: 0.6380 - loss: 1.7968 - val_acc: 0.6280 - val_loss: 1.8128\n",
      "Epoch 22/150\n",
      "\u001b[1m30/30\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - acc: 0.6497 - loss: 1.7628 - val_acc: 0.6390 - val_loss: 1.7707\n",
      "Epoch 23/150\n",
      "\u001b[1m30/30\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - acc: 0.6584 - loss: 1.7206 - val_acc: 0.6410 - val_loss: 1.7320\n",
      "Epoch 24/150\n",
      "\u001b[1m30/30\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - acc: 0.6677 - loss: 1.6859 - val_acc: 0.6590 - val_loss: 1.6987\n",
      "Epoch 25/150\n",
      "\u001b[1m30/30\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - acc: 0.6864 - loss: 1.6410 - val_acc: 0.6600 - val_loss: 1.6666\n",
      "Epoch 26/150\n",
      "\u001b[1m30/30\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - acc: 0.6881 - loss: 1.6152 - val_acc: 0.6700 - val_loss: 1.6408\n",
      "Epoch 27/150\n",
      "\u001b[1m30/30\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 9ms/step - acc: 0.6979 - loss: 1.5823 - val_acc: 0.6800 - val_loss: 1.6125\n",
      "Epoch 28/150\n",
      "\u001b[1m30/30\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - acc: 0.7063 - loss: 1.5512 - val_acc: 0.6870 - val_loss: 1.5863\n",
      "Epoch 29/150\n",
      "\u001b[1m30/30\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - acc: 0.7012 - loss: 1.5370 - val_acc: 0.6920 - val_loss: 1.5663\n",
      "Epoch 30/150\n",
      "\u001b[1m30/30\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - acc: 0.7103 - loss: 1.5032 - val_acc: 0.6940 - val_loss: 1.5421\n",
      "Epoch 31/150\n",
      "\u001b[1m30/30\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - acc: 0.7089 - loss: 1.4861 - val_acc: 0.6950 - val_loss: 1.5249\n",
      "Epoch 32/150\n",
      "\u001b[1m30/30\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 9ms/step - acc: 0.7114 - loss: 1.4813 - val_acc: 0.7000 - val_loss: 1.5024\n",
      "Epoch 33/150\n",
      "\u001b[1m30/30\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 9ms/step - acc: 0.7293 - loss: 1.4388 - val_acc: 0.7020 - val_loss: 1.4853\n",
      "Epoch 34/150\n",
      "\u001b[1m30/30\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 9ms/step - acc: 0.7316 - loss: 1.4255 - val_acc: 0.7090 - val_loss: 1.4717\n",
      "Epoch 35/150\n",
      "\u001b[1m30/30\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 9ms/step - acc: 0.7390 - loss: 1.4037 - val_acc: 0.7130 - val_loss: 1.4555\n",
      "Epoch 36/150\n",
      "\u001b[1m30/30\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 9ms/step - acc: 0.7312 - loss: 1.3973 - val_acc: 0.7060 - val_loss: 1.4441\n",
      "Epoch 37/150\n",
      "\u001b[1m30/30\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - acc: 0.7289 - loss: 1.3844 - val_acc: 0.7100 - val_loss: 1.4313\n",
      "Epoch 38/150\n",
      "\u001b[1m30/30\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - acc: 0.7349 - loss: 1.3682 - val_acc: 0.7090 - val_loss: 1.4162\n",
      "Epoch 39/150\n",
      "\u001b[1m30/30\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - acc: 0.7398 - loss: 1.3529 - val_acc: 0.7160 - val_loss: 1.4036\n",
      "Epoch 40/150\n",
      "\u001b[1m30/30\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - acc: 0.7540 - loss: 1.3270 - val_acc: 0.7140 - val_loss: 1.3952\n",
      "Epoch 41/150\n",
      "\u001b[1m30/30\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 9ms/step - acc: 0.7504 - loss: 1.3425 - val_acc: 0.7190 - val_loss: 1.3865\n",
      "Epoch 42/150\n",
      "\u001b[1m30/30\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 11ms/step - acc: 0.7586 - loss: 1.3092 - val_acc: 0.7200 - val_loss: 1.3730\n",
      "Epoch 43/150\n",
      "\u001b[1m30/30\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 9ms/step - acc: 0.7602 - loss: 1.2959 - val_acc: 0.7190 - val_loss: 1.3650\n",
      "Epoch 44/150\n",
      "\u001b[1m30/30\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - acc: 0.7582 - loss: 1.2904 - val_acc: 0.7170 - val_loss: 1.3566\n",
      "Epoch 45/150\n",
      "\u001b[1m30/30\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - acc: 0.7669 - loss: 1.2743 - val_acc: 0.7190 - val_loss: 1.3482\n",
      "Epoch 46/150\n",
      "\u001b[1m30/30\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - acc: 0.7589 - loss: 1.2866 - val_acc: 0.7290 - val_loss: 1.3389\n",
      "Epoch 47/150\n",
      "\u001b[1m30/30\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - acc: 0.7707 - loss: 1.2565 - val_acc: 0.7340 - val_loss: 1.3313\n",
      "Epoch 48/150\n",
      "\u001b[1m30/30\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - acc: 0.7719 - loss: 1.2401 - val_acc: 0.7340 - val_loss: 1.3237\n",
      "Epoch 49/150\n",
      "\u001b[1m30/30\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - acc: 0.7771 - loss: 1.2464 - val_acc: 0.7330 - val_loss: 1.3166\n",
      "Epoch 50/150\n",
      "\u001b[1m30/30\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - acc: 0.7764 - loss: 1.2322 - val_acc: 0.7310 - val_loss: 1.3097\n",
      "Epoch 51/150\n",
      "\u001b[1m30/30\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 10ms/step - acc: 0.7814 - loss: 1.2133 - val_acc: 0.7390 - val_loss: 1.3026\n",
      "Epoch 52/150\n",
      "\u001b[1m30/30\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 9ms/step - acc: 0.7752 - loss: 1.2172 - val_acc: 0.7260 - val_loss: 1.2994\n",
      "Epoch 53/150\n",
      "\u001b[1m30/30\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 9ms/step - acc: 0.7791 - loss: 1.2157 - val_acc: 0.7390 - val_loss: 1.2899\n",
      "Epoch 54/150\n",
      "\u001b[1m30/30\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 9ms/step - acc: 0.7808 - loss: 1.2030 - val_acc: 0.7210 - val_loss: 1.2867\n",
      "Epoch 55/150\n",
      "\u001b[1m30/30\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 9ms/step - acc: 0.7786 - loss: 1.2079 - val_acc: 0.7330 - val_loss: 1.2775\n",
      "Epoch 56/150\n",
      "\u001b[1m30/30\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 9ms/step - acc: 0.7827 - loss: 1.1890 - val_acc: 0.7300 - val_loss: 1.2728\n",
      "Epoch 57/150\n",
      "\u001b[1m30/30\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 9ms/step - acc: 0.7813 - loss: 1.1882 - val_acc: 0.7400 - val_loss: 1.2687\n",
      "Epoch 58/150\n",
      "\u001b[1m30/30\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 9ms/step - acc: 0.7907 - loss: 1.1660 - val_acc: 0.7240 - val_loss: 1.2646\n",
      "Epoch 59/150\n",
      "\u001b[1m30/30\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 9ms/step - acc: 0.7916 - loss: 1.1604 - val_acc: 0.7300 - val_loss: 1.2613\n",
      "Epoch 60/150\n",
      "\u001b[1m30/30\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - acc: 0.7927 - loss: 1.1599 - val_acc: 0.7340 - val_loss: 1.2519\n",
      "Epoch 61/150\n",
      "\u001b[1m30/30\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - acc: 0.7957 - loss: 1.1478 - val_acc: 0.7360 - val_loss: 1.2459\n",
      "Epoch 62/150\n",
      "\u001b[1m30/30\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - acc: 0.7914 - loss: 1.1530 - val_acc: 0.7380 - val_loss: 1.2502\n",
      "Epoch 63/150\n",
      "\u001b[1m30/30\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 9ms/step - acc: 0.7985 - loss: 1.1367 - val_acc: 0.7390 - val_loss: 1.2374\n",
      "Epoch 64/150\n",
      "\u001b[1m30/30\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - acc: 0.8007 - loss: 1.1236 - val_acc: 0.7340 - val_loss: 1.2351\n",
      "Epoch 65/150\n",
      "\u001b[1m30/30\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 10ms/step - acc: 0.8021 - loss: 1.1281 - val_acc: 0.7400 - val_loss: 1.2310\n",
      "Epoch 66/150\n",
      "\u001b[1m30/30\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 10ms/step - acc: 0.8045 - loss: 1.1255 - val_acc: 0.7400 - val_loss: 1.2236\n",
      "Epoch 67/150\n",
      "\u001b[1m30/30\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 10ms/step - acc: 0.8045 - loss: 1.1075 - val_acc: 0.7460 - val_loss: 1.2199\n",
      "Epoch 68/150\n",
      "\u001b[1m30/30\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 9ms/step - acc: 0.8084 - loss: 1.1062 - val_acc: 0.7430 - val_loss: 1.2175\n",
      "Epoch 69/150\n",
      "\u001b[1m30/30\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 11ms/step - acc: 0.8056 - loss: 1.0945 - val_acc: 0.7420 - val_loss: 1.2120\n",
      "Epoch 70/150\n",
      "\u001b[1m30/30\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 11ms/step - acc: 0.8044 - loss: 1.0970 - val_acc: 0.7360 - val_loss: 1.2120\n",
      "Epoch 71/150\n",
      "\u001b[1m30/30\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 9ms/step - acc: 0.8076 - loss: 1.0851 - val_acc: 0.7420 - val_loss: 1.2069\n",
      "Epoch 72/150\n",
      "\u001b[1m30/30\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - acc: 0.8106 - loss: 1.0814 - val_acc: 0.7430 - val_loss: 1.2002\n",
      "Epoch 73/150\n",
      "\u001b[1m30/30\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 9ms/step - acc: 0.8172 - loss: 1.0703 - val_acc: 0.7440 - val_loss: 1.1980\n",
      "Epoch 74/150\n",
      "\u001b[1m30/30\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - acc: 0.8196 - loss: 1.0666 - val_acc: 0.7460 - val_loss: 1.1955\n",
      "Epoch 75/150\n",
      "\u001b[1m30/30\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 9ms/step - acc: 0.8186 - loss: 1.0567 - val_acc: 0.7420 - val_loss: 1.1929\n",
      "Epoch 76/150\n",
      "\u001b[1m30/30\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - acc: 0.8145 - loss: 1.0599 - val_acc: 0.7530 - val_loss: 1.1859\n",
      "Epoch 77/150\n",
      "\u001b[1m30/30\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 9ms/step - acc: 0.8188 - loss: 1.0575 - val_acc: 0.7410 - val_loss: 1.1846\n",
      "Epoch 78/150\n",
      "\u001b[1m30/30\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 13ms/step - acc: 0.8222 - loss: 1.0413 - val_acc: 0.7460 - val_loss: 1.1824\n",
      "Epoch 79/150\n",
      "\u001b[1m30/30\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 11ms/step - acc: 0.8307 - loss: 1.0365 - val_acc: 0.7490 - val_loss: 1.1793\n",
      "Epoch 80/150\n",
      "\u001b[1m30/30\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 10ms/step - acc: 0.8255 - loss: 1.0344 - val_acc: 0.7490 - val_loss: 1.1735\n",
      "Epoch 81/150\n",
      "\u001b[1m30/30\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 10ms/step - acc: 0.8174 - loss: 1.0440 - val_acc: 0.7470 - val_loss: 1.1732\n",
      "Epoch 82/150\n",
      "\u001b[1m30/30\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 9ms/step - acc: 0.8329 - loss: 1.0082 - val_acc: 0.7440 - val_loss: 1.1684\n",
      "Epoch 83/150\n",
      "\u001b[1m30/30\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 12ms/step - acc: 0.8270 - loss: 1.0253 - val_acc: 0.7500 - val_loss: 1.1644\n",
      "Epoch 84/150\n",
      "\u001b[1m30/30\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 14ms/step - acc: 0.8281 - loss: 1.0155 - val_acc: 0.7470 - val_loss: 1.1604\n",
      "Epoch 85/150\n",
      "\u001b[1m30/30\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 11ms/step - acc: 0.8317 - loss: 1.0027 - val_acc: 0.7460 - val_loss: 1.1594\n",
      "Epoch 86/150\n",
      "\u001b[1m30/30\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 10ms/step - acc: 0.8368 - loss: 0.9984 - val_acc: 0.7460 - val_loss: 1.1575\n",
      "Epoch 87/150\n",
      "\u001b[1m30/30\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 10ms/step - acc: 0.8316 - loss: 1.0046 - val_acc: 0.7470 - val_loss: 1.1555\n",
      "Epoch 88/150\n",
      "\u001b[1m30/30\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 11ms/step - acc: 0.8360 - loss: 0.9933 - val_acc: 0.7420 - val_loss: 1.1543\n",
      "Epoch 89/150\n",
      "\u001b[1m30/30\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 11ms/step - acc: 0.8368 - loss: 0.9900 - val_acc: 0.7480 - val_loss: 1.1465\n",
      "Epoch 90/150\n",
      "\u001b[1m30/30\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 10ms/step - acc: 0.8388 - loss: 0.9882 - val_acc: 0.7450 - val_loss: 1.1488\n",
      "Epoch 91/150\n",
      "\u001b[1m30/30\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 10ms/step - acc: 0.8377 - loss: 0.9823 - val_acc: 0.7480 - val_loss: 1.1423\n",
      "Epoch 92/150\n",
      "\u001b[1m30/30\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 10ms/step - acc: 0.8414 - loss: 0.9686 - val_acc: 0.7480 - val_loss: 1.1408\n",
      "Epoch 93/150\n",
      "\u001b[1m30/30\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 11ms/step - acc: 0.8368 - loss: 0.9828 - val_acc: 0.7480 - val_loss: 1.1384\n",
      "Epoch 94/150\n",
      "\u001b[1m30/30\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 10ms/step - acc: 0.8430 - loss: 0.9605 - val_acc: 0.7440 - val_loss: 1.1344\n",
      "Epoch 95/150\n",
      "\u001b[1m30/30\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 10ms/step - acc: 0.8518 - loss: 0.9597 - val_acc: 0.7450 - val_loss: 1.1347\n",
      "Epoch 96/150\n",
      "\u001b[1m30/30\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 9ms/step - acc: 0.8444 - loss: 0.9565 - val_acc: 0.7490 - val_loss: 1.1299\n",
      "Epoch 97/150\n",
      "\u001b[1m30/30\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 10ms/step - acc: 0.8407 - loss: 0.9650 - val_acc: 0.7480 - val_loss: 1.1279\n",
      "Epoch 98/150\n",
      "\u001b[1m30/30\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 10ms/step - acc: 0.8475 - loss: 0.9488 - val_acc: 0.7440 - val_loss: 1.1255\n",
      "Epoch 99/150\n",
      "\u001b[1m30/30\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 10ms/step - acc: 0.8416 - loss: 0.9571 - val_acc: 0.7470 - val_loss: 1.1242\n",
      "Epoch 100/150\n",
      "\u001b[1m30/30\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 10ms/step - acc: 0.8490 - loss: 0.9466 - val_acc: 0.7450 - val_loss: 1.1213\n",
      "Epoch 101/150\n",
      "\u001b[1m30/30\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - acc: 0.8449 - loss: 0.9375 - val_acc: 0.7460 - val_loss: 1.1180\n",
      "Epoch 102/150\n",
      "\u001b[1m30/30\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 10ms/step - acc: 0.8537 - loss: 0.9258 - val_acc: 0.7460 - val_loss: 1.1171\n",
      "Epoch 103/150\n",
      "\u001b[1m30/30\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 11ms/step - acc: 0.8491 - loss: 0.9416 - val_acc: 0.7470 - val_loss: 1.1157\n",
      "Epoch 104/150\n",
      "\u001b[1m30/30\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 9ms/step - acc: 0.8509 - loss: 0.9316 - val_acc: 0.7460 - val_loss: 1.1121\n",
      "Epoch 105/150\n",
      "\u001b[1m30/30\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - acc: 0.8560 - loss: 0.9274 - val_acc: 0.7460 - val_loss: 1.1151\n",
      "Epoch 106/150\n",
      "\u001b[1m30/30\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - acc: 0.8581 - loss: 0.9151 - val_acc: 0.7480 - val_loss: 1.1081\n",
      "Epoch 107/150\n",
      "\u001b[1m30/30\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 9ms/step - acc: 0.8537 - loss: 0.9144 - val_acc: 0.7500 - val_loss: 1.1053\n",
      "Epoch 108/150\n",
      "\u001b[1m30/30\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 10ms/step - acc: 0.8495 - loss: 0.9257 - val_acc: 0.7450 - val_loss: 1.1062\n",
      "Epoch 109/150\n",
      "\u001b[1m30/30\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - acc: 0.8513 - loss: 0.9096 - val_acc: 0.7490 - val_loss: 1.1049\n",
      "Epoch 110/150\n",
      "\u001b[1m30/30\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - acc: 0.8606 - loss: 0.8967 - val_acc: 0.7470 - val_loss: 1.0987\n",
      "Epoch 111/150\n",
      "\u001b[1m30/30\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 9ms/step - acc: 0.8639 - loss: 0.8893 - val_acc: 0.7480 - val_loss: 1.1017\n",
      "Epoch 112/150\n",
      "\u001b[1m30/30\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - acc: 0.8601 - loss: 0.8977 - val_acc: 0.7460 - val_loss: 1.0970\n",
      "Epoch 113/150\n",
      "\u001b[1m30/30\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 9ms/step - acc: 0.8658 - loss: 0.8812 - val_acc: 0.7470 - val_loss: 1.0934\n",
      "Epoch 114/150\n",
      "\u001b[1m30/30\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - acc: 0.8639 - loss: 0.8886 - val_acc: 0.7450 - val_loss: 1.0924\n",
      "Epoch 115/150\n",
      "\u001b[1m30/30\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 9ms/step - acc: 0.8648 - loss: 0.8791 - val_acc: 0.7490 - val_loss: 1.0902\n",
      "Epoch 116/150\n",
      "\u001b[1m30/30\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 11ms/step - acc: 0.8731 - loss: 0.8682 - val_acc: 0.7480 - val_loss: 1.0888\n",
      "Epoch 117/150\n",
      "\u001b[1m30/30\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 10ms/step - acc: 0.8702 - loss: 0.8686 - val_acc: 0.7480 - val_loss: 1.0885\n",
      "Epoch 118/150\n",
      "\u001b[1m30/30\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 10ms/step - acc: 0.8682 - loss: 0.8699 - val_acc: 0.7490 - val_loss: 1.0885\n",
      "Epoch 119/150\n",
      "\u001b[1m30/30\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - acc: 0.8674 - loss: 0.8701 - val_acc: 0.7470 - val_loss: 1.0830\n",
      "Epoch 120/150\n",
      "\u001b[1m30/30\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 9ms/step - acc: 0.8741 - loss: 0.8549 - val_acc: 0.7480 - val_loss: 1.0817\n",
      "Epoch 121/150\n",
      "\u001b[1m30/30\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 10ms/step - acc: 0.8679 - loss: 0.8647 - val_acc: 0.7450 - val_loss: 1.0820\n",
      "Epoch 122/150\n",
      "\u001b[1m30/30\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 12ms/step - acc: 0.8723 - loss: 0.8541 - val_acc: 0.7440 - val_loss: 1.0791\n",
      "Epoch 123/150\n",
      "\u001b[1m30/30\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 10ms/step - acc: 0.8722 - loss: 0.8487 - val_acc: 0.7500 - val_loss: 1.0769\n",
      "Epoch 124/150\n",
      "\u001b[1m30/30\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 10ms/step - acc: 0.8662 - loss: 0.8653 - val_acc: 0.7450 - val_loss: 1.0781\n",
      "Epoch 125/150\n",
      "\u001b[1m30/30\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 11ms/step - acc: 0.8776 - loss: 0.8433 - val_acc: 0.7460 - val_loss: 1.0737\n",
      "Epoch 126/150\n",
      "\u001b[1m30/30\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 11ms/step - acc: 0.8749 - loss: 0.8499 - val_acc: 0.7450 - val_loss: 1.0728\n",
      "Epoch 127/150\n",
      "\u001b[1m30/30\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 9ms/step - acc: 0.8762 - loss: 0.8422 - val_acc: 0.7500 - val_loss: 1.0704\n",
      "Epoch 128/150\n",
      "\u001b[1m30/30\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 10ms/step - acc: 0.8791 - loss: 0.8316 - val_acc: 0.7420 - val_loss: 1.0691\n",
      "Epoch 129/150\n",
      "\u001b[1m30/30\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 9ms/step - acc: 0.8777 - loss: 0.8408 - val_acc: 0.7490 - val_loss: 1.0664\n",
      "Epoch 130/150\n",
      "\u001b[1m30/30\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 10ms/step - acc: 0.8798 - loss: 0.8303 - val_acc: 0.7470 - val_loss: 1.0700\n",
      "Epoch 131/150\n",
      "\u001b[1m30/30\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - acc: 0.8813 - loss: 0.8218 - val_acc: 0.7520 - val_loss: 1.0662\n",
      "Epoch 132/150\n",
      "\u001b[1m30/30\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 11ms/step - acc: 0.8788 - loss: 0.8240 - val_acc: 0.7480 - val_loss: 1.0648\n",
      "Epoch 133/150\n",
      "\u001b[1m30/30\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 12ms/step - acc: 0.8735 - loss: 0.8374 - val_acc: 0.7460 - val_loss: 1.0604\n",
      "Epoch 134/150\n",
      "\u001b[1m30/30\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - acc: 0.8817 - loss: 0.8146 - val_acc: 0.7440 - val_loss: 1.0607\n",
      "Epoch 135/150\n",
      "\u001b[1m30/30\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - acc: 0.8816 - loss: 0.8156 - val_acc: 0.7480 - val_loss: 1.0588\n",
      "Epoch 136/150\n",
      "\u001b[1m30/30\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - acc: 0.8818 - loss: 0.8135 - val_acc: 0.7540 - val_loss: 1.0602\n",
      "Epoch 137/150\n",
      "\u001b[1m30/30\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 10ms/step - acc: 0.8820 - loss: 0.8035 - val_acc: 0.7460 - val_loss: 1.0600\n",
      "Epoch 138/150\n",
      "\u001b[1m30/30\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 9ms/step - acc: 0.8855 - loss: 0.8023 - val_acc: 0.7440 - val_loss: 1.0576\n",
      "Epoch 139/150\n",
      "\u001b[1m30/30\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - acc: 0.8845 - loss: 0.8007 - val_acc: 0.7450 - val_loss: 1.0555\n",
      "Epoch 140/150\n",
      "\u001b[1m30/30\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 9ms/step - acc: 0.8868 - loss: 0.8007 - val_acc: 0.7460 - val_loss: 1.0546\n",
      "Epoch 141/150\n",
      "\u001b[1m30/30\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 9ms/step - acc: 0.8825 - loss: 0.8002 - val_acc: 0.7480 - val_loss: 1.0512\n",
      "Epoch 142/150\n",
      "\u001b[1m30/30\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 9ms/step - acc: 0.8862 - loss: 0.7905 - val_acc: 0.7460 - val_loss: 1.0559\n",
      "Epoch 143/150\n",
      "\u001b[1m30/30\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step - acc: 0.8907 - loss: 0.7857 - val_acc: 0.7470 - val_loss: 1.0523\n",
      "Epoch 144/150\n",
      "\u001b[1m30/30\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 15ms/step - acc: 0.8885 - loss: 0.7894 - val_acc: 0.7410 - val_loss: 1.0536\n",
      "Epoch 145/150\n",
      "\u001b[1m30/30\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 12ms/step - acc: 0.8952 - loss: 0.7727 - val_acc: 0.7460 - val_loss: 1.0572\n",
      "Epoch 146/150\n",
      "\u001b[1m30/30\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 10ms/step - acc: 0.8886 - loss: 0.7805 - val_acc: 0.7520 - val_loss: 1.0447\n",
      "Epoch 147/150\n",
      "\u001b[1m30/30\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 12ms/step - acc: 0.8871 - loss: 0.7867 - val_acc: 0.7450 - val_loss: 1.0504\n",
      "Epoch 148/150\n",
      "\u001b[1m30/30\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step - acc: 0.8957 - loss: 0.7718 - val_acc: 0.7440 - val_loss: 1.0467\n",
      "Epoch 149/150\n",
      "\u001b[1m30/30\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 12ms/step - acc: 0.8854 - loss: 0.7736 - val_acc: 0.7460 - val_loss: 1.0442\n",
      "Epoch 150/150\n",
      "\u001b[1m30/30\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 13ms/step - acc: 0.9016 - loss: 0.7571 - val_acc: 0.7450 - val_loss: 1.0428\n"
     ]
    }
   ],
   "source": [
    "# Import regularizers\n",
    "from tensorflow import keras\n",
    "random.seed(123)\n",
    "L2_model = models.Sequential()\n",
    "\n",
    "# Add the input and first hidden layer\n",
    "L2_model.add(layers.Dense(50, activation='relu', kernel_regularizer=keras.regularizers.l2(0.005), input_shape=(X_train_tokens.shape[1],)))\n",
    "\n",
    "# Add another hidden layer\n",
    "L2_model.add(layers.Dense(25, activation='relu', kernel_regularizer=keras.regularizers.l2(0.005)))\n",
    "\n",
    "# Add an output layer\n",
    "L2_model.add(layers.Dense(7, activation='softmax'))\n",
    "\n",
    "# Compile the model\n",
    "L2_model.compile(optimizer='SGD', \n",
    "                 loss='categorical_crossentropy', \n",
    "                 metrics=['acc'])\n",
    "\n",
    "# Train the model \n",
    "L2_model_val = L2_model.fit(X_train_tokens, \n",
    "                            y_train_lb, \n",
    "                            epochs=150, \n",
    "                            batch_size=256, \n",
    "                            validation_data=(X_val_tokens, y_val_lb))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, look at the training as well as the validation accuracy for both the L2 and the baseline models. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [],
   "source": [
    "# L2 model details\n",
    "L2_model_dict = L2_model_val.history\n",
    "L2_acc_values = L2_model_dict['acc'] \n",
    "L2_val_acc_values = L2_model_dict['val_acc']\n",
    "\n",
    "# Baseline model\n",
    "baseline_model_acc = baseline_model_val_dict['accuracy'] \n",
    "baseline_model_val_acc = baseline_model_val_dict['val_accuracy']\n",
    "\n",
    "# Plot the accuracy for these models\n",
    "fig, ax = plt.subplots(figsize=(12, 8))\n",
    "epochs = range(1, len(L2_acc_values) + 1)\n",
    "ax.plot(epochs, L2_acc_values, label='Training acc L2')\n",
    "ax.plot(epochs, L2_val_acc_values, label='Validation acc L2')\n",
    "ax.plot(epochs, baseline_model_acc, label='Training acc')\n",
    "ax.plot(epochs, baseline_model_val_acc, label='Validation acc')\n",
    "ax.set_title('Training & validation accuracy L2 vs regular')\n",
    "ax.set_xlabel('Epochs')\n",
    "ax.set_ylabel('Accuracy')\n",
    "ax.legend();"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The results of L2 regularization are quite disappointing here. Notice the discrepancy between validation and training accuracy seems to have decreased slightly, but the end result is definitely not getting better.  \n",
    "\n",
    "\n",
    "## L1 Regularization\n",
    "\n",
    "Now have a look at L1 regularization. Will this work better? \n",
    "\n",
    "- Use 2 hidden layers with 50 units in the first and 25 in the second layer, both with `'relu'` activation functions \n",
    "- Add L1 regularization to both the hidden layers with 0.005 as the `lambda_coeff` "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/150\n",
      "\u001b[1m30/30\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 33ms/step - acc: 0.1298 - loss: 16.1867 - val_acc: 0.1510 - val_loss: 15.6188\n",
      "Epoch 2/150\n",
      "\u001b[1m30/30\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 13ms/step - acc: 0.1684 - loss: 15.4217 - val_acc: 0.1910 - val_loss: 14.8760\n",
      "Epoch 3/150\n",
      "\u001b[1m30/30\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 12ms/step - acc: 0.2098 - loss: 14.6872 - val_acc: 0.2180 - val_loss: 14.1556\n",
      "Epoch 4/150\n",
      "\u001b[1m30/30\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 13ms/step - acc: 0.2373 - loss: 13.9710 - val_acc: 0.2570 - val_loss: 13.4551\n",
      "Epoch 5/150\n",
      "\u001b[1m30/30\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 12ms/step - acc: 0.2598 - loss: 13.2748 - val_acc: 0.2800 - val_loss: 12.7731\n",
      "Epoch 6/150\n",
      "\u001b[1m30/30\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 12ms/step - acc: 0.2898 - loss: 12.5971 - val_acc: 0.2960 - val_loss: 12.1106\n",
      "Epoch 7/150\n",
      "\u001b[1m30/30\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 12ms/step - acc: 0.3139 - loss: 11.9374 - val_acc: 0.3050 - val_loss: 11.4680\n",
      "Epoch 8/150\n",
      "\u001b[1m30/30\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 12ms/step - acc: 0.3290 - loss: 11.2990 - val_acc: 0.3350 - val_loss: 10.8432\n",
      "Epoch 9/150\n",
      "\u001b[1m30/30\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 11ms/step - acc: 0.3444 - loss: 10.6816 - val_acc: 0.3570 - val_loss: 10.2369\n",
      "Epoch 10/150\n",
      "\u001b[1m30/30\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 12ms/step - acc: 0.3680 - loss: 10.0749 - val_acc: 0.3900 - val_loss: 9.6493\n",
      "Epoch 11/150\n",
      "\u001b[1m30/30\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 10ms/step - acc: 0.3972 - loss: 9.4946 - val_acc: 0.4090 - val_loss: 9.0836\n",
      "Epoch 12/150\n",
      "\u001b[1m30/30\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 12ms/step - acc: 0.4180 - loss: 8.9287 - val_acc: 0.4340 - val_loss: 8.5364\n",
      "Epoch 13/150\n",
      "\u001b[1m30/30\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 13ms/step - acc: 0.4426 - loss: 8.3864 - val_acc: 0.4640 - val_loss: 8.0096\n",
      "Epoch 14/150\n",
      "\u001b[1m30/30\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 12ms/step - acc: 0.4719 - loss: 7.8653 - val_acc: 0.4840 - val_loss: 7.5050\n",
      "Epoch 15/150\n",
      "\u001b[1m30/30\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step - acc: 0.5086 - loss: 7.3602 - val_acc: 0.5160 - val_loss: 7.0245\n",
      "Epoch 16/150\n",
      "\u001b[1m30/30\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step - acc: 0.5350 - loss: 6.8757 - val_acc: 0.5280 - val_loss: 6.5644\n",
      "Epoch 17/150\n",
      "\u001b[1m30/30\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 12ms/step - acc: 0.5573 - loss: 6.4238 - val_acc: 0.5600 - val_loss: 6.1250\n",
      "Epoch 18/150\n",
      "\u001b[1m30/30\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 11ms/step - acc: 0.5762 - loss: 5.9949 - val_acc: 0.5660 - val_loss: 5.7110\n",
      "Epoch 19/150\n",
      "\u001b[1m30/30\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step - acc: 0.5948 - loss: 5.5895 - val_acc: 0.5680 - val_loss: 5.3242\n",
      "Epoch 20/150\n",
      "\u001b[1m30/30\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 13ms/step - acc: 0.5944 - loss: 5.2032 - val_acc: 0.5970 - val_loss: 4.9572\n",
      "Epoch 21/150\n",
      "\u001b[1m30/30\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 12ms/step - acc: 0.6135 - loss: 4.8357 - val_acc: 0.6020 - val_loss: 4.6120\n",
      "Epoch 22/150\n",
      "\u001b[1m30/30\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 12ms/step - acc: 0.6203 - loss: 4.5067 - val_acc: 0.6110 - val_loss: 4.2930\n",
      "Epoch 23/150\n",
      "\u001b[1m30/30\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 12ms/step - acc: 0.6160 - loss: 4.1941 - val_acc: 0.6250 - val_loss: 3.9953\n",
      "Epoch 24/150\n",
      "\u001b[1m30/30\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 12ms/step - acc: 0.6364 - loss: 3.9003 - val_acc: 0.6230 - val_loss: 3.7247\n",
      "Epoch 25/150\n",
      "\u001b[1m30/30\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 12ms/step - acc: 0.6401 - loss: 3.6298 - val_acc: 0.6350 - val_loss: 3.4711\n",
      "Epoch 26/150\n",
      "\u001b[1m30/30\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 11ms/step - acc: 0.6469 - loss: 3.3875 - val_acc: 0.6340 - val_loss: 3.2418\n",
      "Epoch 27/150\n",
      "\u001b[1m30/30\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 12ms/step - acc: 0.6622 - loss: 3.1596 - val_acc: 0.6430 - val_loss: 3.0382\n",
      "Epoch 28/150\n",
      "\u001b[1m30/30\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 11ms/step - acc: 0.6530 - loss: 2.9672 - val_acc: 0.6430 - val_loss: 2.8498\n",
      "Epoch 29/150\n",
      "\u001b[1m30/30\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 11ms/step - acc: 0.6592 - loss: 2.7849 - val_acc: 0.6490 - val_loss: 2.6844\n",
      "Epoch 30/150\n",
      "\u001b[1m30/30\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 12ms/step - acc: 0.6688 - loss: 2.6272 - val_acc: 0.6480 - val_loss: 2.5426\n",
      "Epoch 31/150\n",
      "\u001b[1m30/30\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 12ms/step - acc: 0.6647 - loss: 2.4907 - val_acc: 0.6570 - val_loss: 2.4195\n",
      "Epoch 32/150\n",
      "\u001b[1m30/30\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step - acc: 0.6734 - loss: 2.3751 - val_acc: 0.6560 - val_loss: 2.3143\n",
      "Epoch 33/150\n",
      "\u001b[1m30/30\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 15ms/step - acc: 0.6702 - loss: 2.2790 - val_acc: 0.6460 - val_loss: 2.2356\n",
      "Epoch 34/150\n",
      "\u001b[1m30/30\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 13ms/step - acc: 0.6710 - loss: 2.2015 - val_acc: 0.6600 - val_loss: 2.1621\n",
      "Epoch 35/150\n",
      "\u001b[1m30/30\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 18ms/step - acc: 0.6794 - loss: 2.1273 - val_acc: 0.6670 - val_loss: 2.1129\n",
      "Epoch 36/150\n",
      "\u001b[1m30/30\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 18ms/step - acc: 0.6685 - loss: 2.0878 - val_acc: 0.6640 - val_loss: 2.0739\n",
      "Epoch 37/150\n",
      "\u001b[1m30/30\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 15ms/step - acc: 0.6751 - loss: 2.0575 - val_acc: 0.6640 - val_loss: 2.0472\n",
      "Epoch 38/150\n",
      "\u001b[1m30/30\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 15ms/step - acc: 0.6797 - loss: 2.0295 - val_acc: 0.6640 - val_loss: 2.0273\n",
      "Epoch 39/150\n",
      "\u001b[1m30/30\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 15ms/step - acc: 0.6728 - loss: 2.0019 - val_acc: 0.6650 - val_loss: 2.0021\n",
      "Epoch 40/150\n",
      "\u001b[1m30/30\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 15ms/step - acc: 0.6749 - loss: 1.9878 - val_acc: 0.6690 - val_loss: 1.9856\n",
      "Epoch 41/150\n",
      "\u001b[1m30/30\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 32ms/step - acc: 0.6840 - loss: 1.9677 - val_acc: 0.6740 - val_loss: 1.9631\n",
      "Epoch 42/150\n",
      "\u001b[1m30/30\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 19ms/step - acc: 0.6775 - loss: 1.9485 - val_acc: 0.6660 - val_loss: 1.9515\n",
      "Epoch 43/150\n",
      "\u001b[1m30/30\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step - acc: 0.6889 - loss: 1.9230 - val_acc: 0.6690 - val_loss: 1.9300\n",
      "Epoch 44/150\n",
      "\u001b[1m30/30\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 12ms/step - acc: 0.6802 - loss: 1.9160 - val_acc: 0.6690 - val_loss: 1.9144\n",
      "Epoch 45/150\n",
      "\u001b[1m30/30\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 13ms/step - acc: 0.6867 - loss: 1.9042 - val_acc: 0.6710 - val_loss: 1.9028\n",
      "Epoch 46/150\n",
      "\u001b[1m30/30\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 13ms/step - acc: 0.6754 - loss: 1.8945 - val_acc: 0.6690 - val_loss: 1.8922\n",
      "Epoch 47/150\n",
      "\u001b[1m30/30\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 13ms/step - acc: 0.6810 - loss: 1.8744 - val_acc: 0.6720 - val_loss: 1.8731\n",
      "Epoch 48/150\n",
      "\u001b[1m30/30\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 16ms/step - acc: 0.6853 - loss: 1.8585 - val_acc: 0.6740 - val_loss: 1.8588\n",
      "Epoch 49/150\n",
      "\u001b[1m30/30\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 15ms/step - acc: 0.6941 - loss: 1.8402 - val_acc: 0.6740 - val_loss: 1.8498\n",
      "Epoch 50/150\n",
      "\u001b[1m30/30\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 22ms/step - acc: 0.6740 - loss: 1.8399 - val_acc: 0.6760 - val_loss: 1.8349\n",
      "Epoch 51/150\n",
      "\u001b[1m30/30\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step - acc: 0.6922 - loss: 1.8105 - val_acc: 0.6760 - val_loss: 1.8231\n",
      "Epoch 52/150\n",
      "\u001b[1m30/30\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 13ms/step - acc: 0.6879 - loss: 1.8124 - val_acc: 0.6740 - val_loss: 1.8167\n",
      "Epoch 53/150\n",
      "\u001b[1m30/30\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 12ms/step - acc: 0.6820 - loss: 1.8047 - val_acc: 0.6770 - val_loss: 1.7991\n",
      "Epoch 54/150\n",
      "\u001b[1m30/30\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 13ms/step - acc: 0.6936 - loss: 1.7838 - val_acc: 0.6790 - val_loss: 1.7901\n",
      "Epoch 55/150\n",
      "\u001b[1m30/30\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 13ms/step - acc: 0.6822 - loss: 1.7834 - val_acc: 0.6810 - val_loss: 1.7797\n",
      "Epoch 56/150\n",
      "\u001b[1m30/30\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 13ms/step - acc: 0.6945 - loss: 1.7629 - val_acc: 0.6790 - val_loss: 1.7691\n",
      "Epoch 57/150\n",
      "\u001b[1m30/30\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 12ms/step - acc: 0.6770 - loss: 1.7692 - val_acc: 0.6840 - val_loss: 1.7592\n",
      "Epoch 58/150\n",
      "\u001b[1m30/30\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 12ms/step - acc: 0.6811 - loss: 1.7524 - val_acc: 0.6800 - val_loss: 1.7514\n",
      "Epoch 59/150\n",
      "\u001b[1m30/30\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 13ms/step - acc: 0.6884 - loss: 1.7387 - val_acc: 0.6790 - val_loss: 1.7409\n",
      "Epoch 60/150\n",
      "\u001b[1m30/30\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 13ms/step - acc: 0.6900 - loss: 1.7261 - val_acc: 0.6760 - val_loss: 1.7315\n",
      "Epoch 61/150\n",
      "\u001b[1m30/30\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step - acc: 0.6938 - loss: 1.7139 - val_acc: 0.6790 - val_loss: 1.7218\n",
      "Epoch 62/150\n",
      "\u001b[1m30/30\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 13ms/step - acc: 0.6853 - loss: 1.7053 - val_acc: 0.6820 - val_loss: 1.7159\n",
      "Epoch 63/150\n",
      "\u001b[1m30/30\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 13ms/step - acc: 0.6812 - loss: 1.7131 - val_acc: 0.6860 - val_loss: 1.7015\n",
      "Epoch 64/150\n",
      "\u001b[1m30/30\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 13ms/step - acc: 0.6836 - loss: 1.6960 - val_acc: 0.6850 - val_loss: 1.6930\n",
      "Epoch 65/150\n",
      "\u001b[1m30/30\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 13ms/step - acc: 0.6868 - loss: 1.6902 - val_acc: 0.6840 - val_loss: 1.6854\n",
      "Epoch 66/150\n",
      "\u001b[1m30/30\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 13ms/step - acc: 0.6928 - loss: 1.6704 - val_acc: 0.6780 - val_loss: 1.6775\n",
      "Epoch 67/150\n",
      "\u001b[1m30/30\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 13ms/step - acc: 0.6883 - loss: 1.6696 - val_acc: 0.6860 - val_loss: 1.6705\n",
      "Epoch 68/150\n",
      "\u001b[1m30/30\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 12ms/step - acc: 0.6882 - loss: 1.6623 - val_acc: 0.6840 - val_loss: 1.6611\n",
      "Epoch 69/150\n",
      "\u001b[1m30/30\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 12ms/step - acc: 0.6920 - loss: 1.6500 - val_acc: 0.6850 - val_loss: 1.6666\n",
      "Epoch 70/150\n",
      "\u001b[1m30/30\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step - acc: 0.6936 - loss: 1.6421 - val_acc: 0.6840 - val_loss: 1.6565\n",
      "Epoch 71/150\n",
      "\u001b[1m30/30\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 12ms/step - acc: 0.6800 - loss: 1.6487 - val_acc: 0.6850 - val_loss: 1.6375\n",
      "Epoch 72/150\n",
      "\u001b[1m30/30\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 11ms/step - acc: 0.6972 - loss: 1.6217 - val_acc: 0.6830 - val_loss: 1.6277\n",
      "Epoch 73/150\n",
      "\u001b[1m30/30\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 12ms/step - acc: 0.6958 - loss: 1.6077 - val_acc: 0.6880 - val_loss: 1.6220\n",
      "Epoch 74/150\n",
      "\u001b[1m30/30\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 13ms/step - acc: 0.7026 - loss: 1.6024 - val_acc: 0.6830 - val_loss: 1.6170\n",
      "Epoch 75/150\n",
      "\u001b[1m30/30\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 13ms/step - acc: 0.6929 - loss: 1.6007 - val_acc: 0.6880 - val_loss: 1.6085\n",
      "Epoch 76/150\n",
      "\u001b[1m30/30\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 12ms/step - acc: 0.6990 - loss: 1.6014 - val_acc: 0.6870 - val_loss: 1.5997\n",
      "Epoch 77/150\n",
      "\u001b[1m30/30\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 13ms/step - acc: 0.6955 - loss: 1.5937 - val_acc: 0.6880 - val_loss: 1.5922\n",
      "Epoch 78/150\n",
      "\u001b[1m30/30\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 13ms/step - acc: 0.6926 - loss: 1.5845 - val_acc: 0.6820 - val_loss: 1.5868\n",
      "Epoch 79/150\n",
      "\u001b[1m30/30\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 11ms/step - acc: 0.6984 - loss: 1.5653 - val_acc: 0.6800 - val_loss: 1.5841\n",
      "Epoch 80/150\n",
      "\u001b[1m30/30\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step - acc: 0.6988 - loss: 1.5643 - val_acc: 0.6820 - val_loss: 1.5720\n",
      "Epoch 81/150\n",
      "\u001b[1m30/30\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 13ms/step - acc: 0.6995 - loss: 1.5559 - val_acc: 0.6840 - val_loss: 1.5651\n",
      "Epoch 82/150\n",
      "\u001b[1m30/30\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step - acc: 0.7025 - loss: 1.5581 - val_acc: 0.6840 - val_loss: 1.5592\n",
      "Epoch 83/150\n",
      "\u001b[1m30/30\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 14ms/step - acc: 0.7084 - loss: 1.5303 - val_acc: 0.6860 - val_loss: 1.5552\n",
      "Epoch 84/150\n",
      "\u001b[1m30/30\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 13ms/step - acc: 0.7070 - loss: 1.5242 - val_acc: 0.6860 - val_loss: 1.5500\n",
      "Epoch 85/150\n",
      "\u001b[1m30/30\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 13ms/step - acc: 0.6962 - loss: 1.5398 - val_acc: 0.6830 - val_loss: 1.5378\n",
      "Epoch 86/150\n",
      "\u001b[1m30/30\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 13ms/step - acc: 0.7021 - loss: 1.5263 - val_acc: 0.6860 - val_loss: 1.5334\n",
      "Epoch 87/150\n",
      "\u001b[1m30/30\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 13ms/step - acc: 0.6991 - loss: 1.5251 - val_acc: 0.6870 - val_loss: 1.5286\n",
      "Epoch 88/150\n",
      "\u001b[1m30/30\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step - acc: 0.7007 - loss: 1.5127 - val_acc: 0.6890 - val_loss: 1.5177\n",
      "Epoch 89/150\n",
      "\u001b[1m30/30\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 12ms/step - acc: 0.6971 - loss: 1.5066 - val_acc: 0.6830 - val_loss: 1.5145\n",
      "Epoch 90/150\n",
      "\u001b[1m30/30\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 12ms/step - acc: 0.7022 - loss: 1.4904 - val_acc: 0.6840 - val_loss: 1.5054\n",
      "Epoch 91/150\n",
      "\u001b[1m30/30\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step - acc: 0.6979 - loss: 1.4943 - val_acc: 0.6870 - val_loss: 1.5016\n",
      "Epoch 92/150\n",
      "\u001b[1m30/30\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 13ms/step - acc: 0.6985 - loss: 1.4981 - val_acc: 0.6850 - val_loss: 1.4942\n",
      "Epoch 93/150\n",
      "\u001b[1m30/30\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 12ms/step - acc: 0.7071 - loss: 1.4790 - val_acc: 0.6890 - val_loss: 1.4912\n",
      "Epoch 94/150\n",
      "\u001b[1m30/30\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 13ms/step - acc: 0.7035 - loss: 1.4851 - val_acc: 0.6870 - val_loss: 1.4820\n",
      "Epoch 95/150\n",
      "\u001b[1m30/30\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 13ms/step - acc: 0.7020 - loss: 1.4713 - val_acc: 0.6880 - val_loss: 1.4751\n",
      "Epoch 96/150\n",
      "\u001b[1m30/30\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 13ms/step - acc: 0.7028 - loss: 1.4635 - val_acc: 0.6860 - val_loss: 1.4730\n",
      "Epoch 97/150\n",
      "\u001b[1m30/30\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 13ms/step - acc: 0.7061 - loss: 1.4634 - val_acc: 0.6880 - val_loss: 1.4661\n",
      "Epoch 98/150\n",
      "\u001b[1m30/30\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step - acc: 0.7012 - loss: 1.4633 - val_acc: 0.6860 - val_loss: 1.4623\n",
      "Epoch 99/150\n",
      "\u001b[1m30/30\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 15ms/step - acc: 0.7049 - loss: 1.4427 - val_acc: 0.6880 - val_loss: 1.4556\n",
      "Epoch 100/150\n",
      "\u001b[1m30/30\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 13ms/step - acc: 0.7072 - loss: 1.4410 - val_acc: 0.6870 - val_loss: 1.4487\n",
      "Epoch 101/150\n",
      "\u001b[1m30/30\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step - acc: 0.7014 - loss: 1.4319 - val_acc: 0.6870 - val_loss: 1.4458\n",
      "Epoch 102/150\n",
      "\u001b[1m30/30\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 13ms/step - acc: 0.7089 - loss: 1.4340 - val_acc: 0.6900 - val_loss: 1.4389\n",
      "Epoch 103/150\n",
      "\u001b[1m30/30\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 13ms/step - acc: 0.7045 - loss: 1.4328 - val_acc: 0.6880 - val_loss: 1.4310\n",
      "Epoch 104/150\n",
      "\u001b[1m30/30\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 13ms/step - acc: 0.7189 - loss: 1.4023 - val_acc: 0.6860 - val_loss: 1.4372\n",
      "Epoch 105/150\n",
      "\u001b[1m30/30\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 13ms/step - acc: 0.7094 - loss: 1.4189 - val_acc: 0.6870 - val_loss: 1.4225\n",
      "Epoch 106/150\n",
      "\u001b[1m30/30\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step - acc: 0.7104 - loss: 1.3964 - val_acc: 0.6890 - val_loss: 1.4171\n",
      "Epoch 107/150\n",
      "\u001b[1m30/30\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 13ms/step - acc: 0.7090 - loss: 1.3982 - val_acc: 0.6870 - val_loss: 1.4118\n",
      "Epoch 108/150\n",
      "\u001b[1m30/30\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 13ms/step - acc: 0.7162 - loss: 1.3829 - val_acc: 0.6920 - val_loss: 1.4061\n",
      "Epoch 109/150\n",
      "\u001b[1m30/30\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 12ms/step - acc: 0.7044 - loss: 1.4015 - val_acc: 0.6930 - val_loss: 1.4005\n",
      "Epoch 110/150\n",
      "\u001b[1m30/30\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step - acc: 0.7085 - loss: 1.3924 - val_acc: 0.6910 - val_loss: 1.3992\n",
      "Epoch 111/150\n",
      "\u001b[1m30/30\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step - acc: 0.7141 - loss: 1.3686 - val_acc: 0.6920 - val_loss: 1.3919\n",
      "Epoch 112/150\n",
      "\u001b[1m30/30\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 24ms/step - acc: 0.7066 - loss: 1.3918 - val_acc: 0.6880 - val_loss: 1.3866\n",
      "Epoch 113/150\n",
      "\u001b[1m30/30\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step - acc: 0.7115 - loss: 1.3700 - val_acc: 0.6910 - val_loss: 1.3795\n",
      "Epoch 114/150\n",
      "\u001b[1m30/30\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step - acc: 0.7121 - loss: 1.3649 - val_acc: 0.6950 - val_loss: 1.3763\n",
      "Epoch 115/150\n",
      "\u001b[1m30/30\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 17ms/step - acc: 0.7159 - loss: 1.3473 - val_acc: 0.6920 - val_loss: 1.3754\n",
      "Epoch 116/150\n",
      "\u001b[1m30/30\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 13ms/step - acc: 0.7063 - loss: 1.3689 - val_acc: 0.6910 - val_loss: 1.3697\n",
      "Epoch 117/150\n",
      "\u001b[1m30/30\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 12ms/step - acc: 0.7166 - loss: 1.3422 - val_acc: 0.6910 - val_loss: 1.3652\n",
      "Epoch 118/150\n",
      "\u001b[1m30/30\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 13ms/step - acc: 0.7211 - loss: 1.3320 - val_acc: 0.6930 - val_loss: 1.3650\n",
      "Epoch 119/150\n",
      "\u001b[1m30/30\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 12ms/step - acc: 0.7144 - loss: 1.3415 - val_acc: 0.6980 - val_loss: 1.3535\n",
      "Epoch 120/150\n",
      "\u001b[1m30/30\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 18ms/step - acc: 0.6982 - loss: 1.3493 - val_acc: 0.6880 - val_loss: 1.3509\n",
      "Epoch 121/150\n",
      "\u001b[1m30/30\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step - acc: 0.7166 - loss: 1.3283 - val_acc: 0.6960 - val_loss: 1.3435\n",
      "Epoch 122/150\n",
      "\u001b[1m30/30\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 13ms/step - acc: 0.7165 - loss: 1.3108 - val_acc: 0.6970 - val_loss: 1.3393\n",
      "Epoch 123/150\n",
      "\u001b[1m30/30\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 16ms/step - acc: 0.7165 - loss: 1.3200 - val_acc: 0.6990 - val_loss: 1.3369\n",
      "Epoch 124/150\n",
      "\u001b[1m30/30\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 13ms/step - acc: 0.7102 - loss: 1.3257 - val_acc: 0.6930 - val_loss: 1.3303\n",
      "Epoch 125/150\n",
      "\u001b[1m30/30\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 13ms/step - acc: 0.7102 - loss: 1.3093 - val_acc: 0.6950 - val_loss: 1.3259\n",
      "Epoch 126/150\n",
      "\u001b[1m30/30\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 12ms/step - acc: 0.7058 - loss: 1.3099 - val_acc: 0.6930 - val_loss: 1.3224\n",
      "Epoch 127/150\n",
      "\u001b[1m30/30\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 12ms/step - acc: 0.7179 - loss: 1.2979 - val_acc: 0.6960 - val_loss: 1.3221\n",
      "Epoch 128/150\n",
      "\u001b[1m30/30\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 12ms/step - acc: 0.7091 - loss: 1.2995 - val_acc: 0.6920 - val_loss: 1.3160\n",
      "Epoch 129/150\n",
      "\u001b[1m30/30\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 15ms/step - acc: 0.7148 - loss: 1.3017 - val_acc: 0.6960 - val_loss: 1.3152\n",
      "Epoch 130/150\n",
      "\u001b[1m30/30\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 12ms/step - acc: 0.7117 - loss: 1.3011 - val_acc: 0.6870 - val_loss: 1.3114\n",
      "Epoch 131/150\n",
      "\u001b[1m30/30\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 12ms/step - acc: 0.7218 - loss: 1.2775 - val_acc: 0.6980 - val_loss: 1.3047\n",
      "Epoch 132/150\n",
      "\u001b[1m30/30\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 12ms/step - acc: 0.7217 - loss: 1.2747 - val_acc: 0.6950 - val_loss: 1.3039\n",
      "Epoch 133/150\n",
      "\u001b[1m30/30\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 12ms/step - acc: 0.7119 - loss: 1.2905 - val_acc: 0.6940 - val_loss: 1.2978\n",
      "Epoch 134/150\n",
      "\u001b[1m30/30\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 13ms/step - acc: 0.7177 - loss: 1.2743 - val_acc: 0.7010 - val_loss: 1.2935\n",
      "Epoch 135/150\n",
      "\u001b[1m30/30\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 17ms/step - acc: 0.7118 - loss: 1.2749 - val_acc: 0.6970 - val_loss: 1.2890\n",
      "Epoch 136/150\n",
      "\u001b[1m30/30\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 18ms/step - acc: 0.7145 - loss: 1.2682 - val_acc: 0.7040 - val_loss: 1.2875\n",
      "Epoch 137/150\n",
      "\u001b[1m30/30\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 13ms/step - acc: 0.7118 - loss: 1.2643 - val_acc: 0.6920 - val_loss: 1.2897\n",
      "Epoch 138/150\n",
      "\u001b[1m30/30\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 13ms/step - acc: 0.7206 - loss: 1.2555 - val_acc: 0.7010 - val_loss: 1.2792\n",
      "Epoch 139/150\n",
      "\u001b[1m30/30\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 13ms/step - acc: 0.7268 - loss: 1.2426 - val_acc: 0.6980 - val_loss: 1.2743\n",
      "Epoch 140/150\n",
      "\u001b[1m30/30\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 12ms/step - acc: 0.7129 - loss: 1.2581 - val_acc: 0.6990 - val_loss: 1.2745\n",
      "Epoch 141/150\n",
      "\u001b[1m30/30\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 13ms/step - acc: 0.7218 - loss: 1.2476 - val_acc: 0.6980 - val_loss: 1.2700\n",
      "Epoch 142/150\n",
      "\u001b[1m30/30\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 13ms/step - acc: 0.7221 - loss: 1.2479 - val_acc: 0.7000 - val_loss: 1.2639\n",
      "Epoch 143/150\n",
      "\u001b[1m30/30\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 11ms/step - acc: 0.7150 - loss: 1.2454 - val_acc: 0.6990 - val_loss: 1.2608\n",
      "Epoch 144/150\n",
      "\u001b[1m30/30\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step - acc: 0.7160 - loss: 1.2412 - val_acc: 0.6970 - val_loss: 1.2585\n",
      "Epoch 145/150\n",
      "\u001b[1m30/30\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 15ms/step - acc: 0.7223 - loss: 1.2245 - val_acc: 0.7010 - val_loss: 1.2586\n",
      "Epoch 146/150\n",
      "\u001b[1m30/30\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 14ms/step - acc: 0.7146 - loss: 1.2381 - val_acc: 0.7060 - val_loss: 1.2608\n",
      "Epoch 147/150\n",
      "\u001b[1m30/30\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 12ms/step - acc: 0.7134 - loss: 1.2405 - val_acc: 0.6980 - val_loss: 1.2590\n",
      "Epoch 148/150\n",
      "\u001b[1m30/30\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step - acc: 0.7156 - loss: 1.2327 - val_acc: 0.7010 - val_loss: 1.2475\n",
      "Epoch 149/150\n",
      "\u001b[1m30/30\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 12ms/step - acc: 0.7188 - loss: 1.2313 - val_acc: 0.7040 - val_loss: 1.2434\n",
      "Epoch 150/150\n",
      "\u001b[1m30/30\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 12ms/step - acc: 0.7279 - loss: 1.2132 - val_acc: 0.6990 - val_loss: 1.2528\n"
     ]
    }
   ],
   "source": [
    "random.seed(123)\n",
    "L1_model = models.Sequential()\n",
    "\n",
    "# Add the input and first hidden layer\n",
    "L1_model.add(layers.Dense(50, activation='relu', kernel_regularizer=keras.regularizers.l1(0.005), input_shape=(X_train_tokens.shape[1],)))\n",
    "\n",
    "# Add a hidden layer\n",
    "L1_model.add(layers.Dense(25, activation='relu', \n",
    "                          kernel_regularizer=keras.regularizers.l1(0.005)))\n",
    "\n",
    "# Add an output layer\n",
    "L1_model.add(layers.Dense(7, activation='softmax'))\n",
    "\n",
    "# Compile the model\n",
    "L1_model.compile(optimizer='SGD', \n",
    "                 loss='categorical_crossentropy', \n",
    "                 metrics=['acc'])\n",
    "\n",
    "# Train the model \n",
    "L1_model_val = L1_model.fit(X_train_tokens, \n",
    "                            y_train_lb, \n",
    "                            epochs=150, \n",
    "                            batch_size=256, \n",
    "                            validation_data=(X_val_tokens, y_val_lb))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Plot the training as well as the validation accuracy for the L1 model: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(figsize=(12, 8))\n",
    "\n",
    "L1_model_dict = L1_model_val.history\n",
    "\n",
    "acc_values = L1_model_dict['acc'] \n",
    "val_acc_values = L1_model_dict['val_acc']\n",
    "\n",
    "epochs = range(1, len(acc_values) + 1)\n",
    "ax.plot(epochs, acc_values, label='Training acc L1')\n",
    "ax.plot(epochs, val_acc_values, label='Validation acc L1')\n",
    "ax.set_title('Training & validation accuracy with L1 regularization')\n",
    "ax.set_xlabel('Epochs')\n",
    "ax.set_ylabel('Accuracy')\n",
    "ax.legend();"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Notice how the training and validation accuracy don't diverge as much as before. Unfortunately, the validation accuracy isn't still that good. Next, experiment with dropout regularization to see if it offers any advantages. \n",
    "\n",
    "\n",
    "## Dropout Regularization \n",
    "\n",
    "It's time to try another technique: applying dropout to layers. As discussed in the earlier lesson, this involves setting a certain proportion of units in each layer to zero. In the following cell: \n",
    "\n",
    "- Apply a dropout rate of 30% to the input layer \n",
    "- Add a first hidden layer with 50 units and `'relu'` activation \n",
    "- Apply a dropout rate of 30% to the first hidden layer \n",
    "- Add a second hidden layer with 25 units and `'relu'` activation \n",
    "- Apply a dropout rate of 30% to the second hidden layer \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/150\n",
      "\u001b[1m30/30\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 34ms/step - acc: 0.1577 - loss: 1.9768 - val_acc: 0.1770 - val_loss: 1.9323\n",
      "Epoch 2/150\n",
      "\u001b[1m30/30\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 18ms/step - acc: 0.1574 - loss: 1.9539 - val_acc: 0.1920 - val_loss: 1.9217\n",
      "Epoch 3/150\n",
      "\u001b[1m30/30\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 16ms/step - acc: 0.1667 - loss: 1.9409 - val_acc: 0.2000 - val_loss: 1.9136\n",
      "Epoch 4/150\n",
      "\u001b[1m30/30\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 18ms/step - acc: 0.1872 - loss: 1.9290 - val_acc: 0.2140 - val_loss: 1.9072\n",
      "Epoch 5/150\n",
      "\u001b[1m30/30\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 18ms/step - acc: 0.1927 - loss: 1.9252 - val_acc: 0.2090 - val_loss: 1.9007\n",
      "Epoch 6/150\n",
      "\u001b[1m30/30\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 21ms/step - acc: 0.1950 - loss: 1.9085 - val_acc: 0.2240 - val_loss: 1.8937\n",
      "Epoch 7/150\n",
      "\u001b[1m30/30\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 17ms/step - acc: 0.1989 - loss: 1.9080 - val_acc: 0.2310 - val_loss: 1.8867\n",
      "Epoch 8/150\n",
      "\u001b[1m30/30\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 17ms/step - acc: 0.1997 - loss: 1.9004 - val_acc: 0.2370 - val_loss: 1.8789\n",
      "Epoch 9/150\n",
      "\u001b[1m30/30\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step - acc: 0.2066 - loss: 1.8922 - val_acc: 0.2450 - val_loss: 1.8706\n",
      "Epoch 10/150\n",
      "\u001b[1m30/30\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 15ms/step - acc: 0.2070 - loss: 1.8914 - val_acc: 0.2520 - val_loss: 1.8613\n",
      "Epoch 11/150\n",
      "\u001b[1m30/30\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 16ms/step - acc: 0.2246 - loss: 1.8771 - val_acc: 0.2560 - val_loss: 1.8516\n",
      "Epoch 12/150\n",
      "\u001b[1m30/30\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 16ms/step - acc: 0.2361 - loss: 1.8656 - val_acc: 0.2650 - val_loss: 1.8414\n",
      "Epoch 13/150\n",
      "\u001b[1m30/30\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 17ms/step - acc: 0.2351 - loss: 1.8657 - val_acc: 0.2770 - val_loss: 1.8287\n",
      "Epoch 14/150\n",
      "\u001b[1m30/30\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 17ms/step - acc: 0.2429 - loss: 1.8504 - val_acc: 0.2810 - val_loss: 1.8149\n",
      "Epoch 15/150\n",
      "\u001b[1m30/30\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 17ms/step - acc: 0.2567 - loss: 1.8404 - val_acc: 0.2970 - val_loss: 1.8013\n",
      "Epoch 16/150\n",
      "\u001b[1m30/30\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 17ms/step - acc: 0.2590 - loss: 1.8291 - val_acc: 0.3080 - val_loss: 1.7871\n",
      "Epoch 17/150\n",
      "\u001b[1m30/30\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 17ms/step - acc: 0.2636 - loss: 1.8142 - val_acc: 0.3180 - val_loss: 1.7695\n",
      "Epoch 18/150\n",
      "\u001b[1m30/30\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step - acc: 0.2782 - loss: 1.8034 - val_acc: 0.3320 - val_loss: 1.7520\n",
      "Epoch 19/150\n",
      "\u001b[1m30/30\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 16ms/step - acc: 0.2862 - loss: 1.7947 - val_acc: 0.3210 - val_loss: 1.7330\n",
      "Epoch 20/150\n",
      "\u001b[1m30/30\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 16ms/step - acc: 0.2809 - loss: 1.7913 - val_acc: 0.3410 - val_loss: 1.7146\n",
      "Epoch 21/150\n",
      "\u001b[1m30/30\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 15ms/step - acc: 0.3016 - loss: 1.7487 - val_acc: 0.3560 - val_loss: 1.6936\n",
      "Epoch 22/150\n",
      "\u001b[1m30/30\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 17ms/step - acc: 0.3095 - loss: 1.7388 - val_acc: 0.3690 - val_loss: 1.6717\n",
      "Epoch 23/150\n",
      "\u001b[1m30/30\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 18ms/step - acc: 0.3113 - loss: 1.7350 - val_acc: 0.3860 - val_loss: 1.6518\n",
      "Epoch 24/150\n",
      "\u001b[1m30/30\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 17ms/step - acc: 0.3224 - loss: 1.7101 - val_acc: 0.4050 - val_loss: 1.6312\n",
      "Epoch 25/150\n",
      "\u001b[1m30/30\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 17ms/step - acc: 0.3370 - loss: 1.6867 - val_acc: 0.4190 - val_loss: 1.6102\n",
      "Epoch 26/150\n",
      "\u001b[1m30/30\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 15ms/step - acc: 0.3258 - loss: 1.6987 - val_acc: 0.4260 - val_loss: 1.5898\n",
      "Epoch 27/150\n",
      "\u001b[1m30/30\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 15ms/step - acc: 0.3414 - loss: 1.6806 - val_acc: 0.4440 - val_loss: 1.5698\n",
      "Epoch 28/150\n",
      "\u001b[1m30/30\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 16ms/step - acc: 0.3464 - loss: 1.6680 - val_acc: 0.4610 - val_loss: 1.5489\n",
      "Epoch 29/150\n",
      "\u001b[1m30/30\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 16ms/step - acc: 0.3554 - loss: 1.6504 - val_acc: 0.4750 - val_loss: 1.5276\n",
      "Epoch 30/150\n",
      "\u001b[1m30/30\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 17ms/step - acc: 0.3619 - loss: 1.6304 - val_acc: 0.4790 - val_loss: 1.5076\n",
      "Epoch 31/150\n",
      "\u001b[1m30/30\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 17ms/step - acc: 0.3736 - loss: 1.6141 - val_acc: 0.5090 - val_loss: 1.4887\n",
      "Epoch 32/150\n",
      "\u001b[1m30/30\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 17ms/step - acc: 0.3805 - loss: 1.5888 - val_acc: 0.5100 - val_loss: 1.4678\n",
      "Epoch 33/150\n",
      "\u001b[1m30/30\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 18ms/step - acc: 0.3819 - loss: 1.5878 - val_acc: 0.5360 - val_loss: 1.4495\n",
      "Epoch 34/150\n",
      "\u001b[1m30/30\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 16ms/step - acc: 0.3986 - loss: 1.5671 - val_acc: 0.5450 - val_loss: 1.4312\n",
      "Epoch 35/150\n",
      "\u001b[1m30/30\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 17ms/step - acc: 0.4179 - loss: 1.5297 - val_acc: 0.5500 - val_loss: 1.4101\n",
      "Epoch 36/150\n",
      "\u001b[1m30/30\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 15ms/step - acc: 0.4149 - loss: 1.5328 - val_acc: 0.5580 - val_loss: 1.3929\n",
      "Epoch 37/150\n",
      "\u001b[1m30/30\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step - acc: 0.4233 - loss: 1.5179 - val_acc: 0.5730 - val_loss: 1.3729\n",
      "Epoch 38/150\n",
      "\u001b[1m30/30\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 16ms/step - acc: 0.4279 - loss: 1.5094 - val_acc: 0.5880 - val_loss: 1.3535\n",
      "Epoch 39/150\n",
      "\u001b[1m30/30\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 15ms/step - acc: 0.4248 - loss: 1.4918 - val_acc: 0.5970 - val_loss: 1.3355\n",
      "Epoch 40/150\n",
      "\u001b[1m30/30\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 16ms/step - acc: 0.4305 - loss: 1.4835 - val_acc: 0.6000 - val_loss: 1.3192\n",
      "Epoch 41/150\n",
      "\u001b[1m30/30\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 17ms/step - acc: 0.4414 - loss: 1.4644 - val_acc: 0.6020 - val_loss: 1.3027\n",
      "Epoch 42/150\n",
      "\u001b[1m30/30\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 16ms/step - acc: 0.4382 - loss: 1.4727 - val_acc: 0.6210 - val_loss: 1.2838\n",
      "Epoch 43/150\n",
      "\u001b[1m30/30\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step - acc: 0.4479 - loss: 1.4346 - val_acc: 0.6280 - val_loss: 1.2664\n",
      "Epoch 44/150\n",
      "\u001b[1m30/30\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 16ms/step - acc: 0.4494 - loss: 1.4372 - val_acc: 0.6350 - val_loss: 1.2528\n",
      "Epoch 45/150\n",
      "\u001b[1m30/30\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 13ms/step - acc: 0.4654 - loss: 1.4153 - val_acc: 0.6350 - val_loss: 1.2374\n",
      "Epoch 46/150\n",
      "\u001b[1m30/30\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 15ms/step - acc: 0.4690 - loss: 1.3949 - val_acc: 0.6380 - val_loss: 1.2221\n",
      "Epoch 47/150\n",
      "\u001b[1m30/30\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 16ms/step - acc: 0.4727 - loss: 1.3770 - val_acc: 0.6380 - val_loss: 1.2076\n",
      "Epoch 48/150\n",
      "\u001b[1m30/30\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 15ms/step - acc: 0.4826 - loss: 1.3704 - val_acc: 0.6440 - val_loss: 1.1934\n",
      "Epoch 49/150\n",
      "\u001b[1m30/30\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 17ms/step - acc: 0.4789 - loss: 1.3755 - val_acc: 0.6520 - val_loss: 1.1810\n",
      "Epoch 50/150\n",
      "\u001b[1m30/30\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 16ms/step - acc: 0.4876 - loss: 1.3541 - val_acc: 0.6530 - val_loss: 1.1667\n",
      "Epoch 51/150\n",
      "\u001b[1m30/30\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 17ms/step - acc: 0.4884 - loss: 1.3587 - val_acc: 0.6530 - val_loss: 1.1548\n",
      "Epoch 52/150\n",
      "\u001b[1m30/30\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 16ms/step - acc: 0.5103 - loss: 1.3272 - val_acc: 0.6520 - val_loss: 1.1421\n",
      "Epoch 53/150\n",
      "\u001b[1m30/30\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 17ms/step - acc: 0.5023 - loss: 1.3197 - val_acc: 0.6570 - val_loss: 1.1289\n",
      "Epoch 54/150\n",
      "\u001b[1m30/30\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 17ms/step - acc: 0.4912 - loss: 1.3258 - val_acc: 0.6700 - val_loss: 1.1176\n",
      "Epoch 55/150\n",
      "\u001b[1m30/30\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 19ms/step - acc: 0.5091 - loss: 1.3059 - val_acc: 0.6670 - val_loss: 1.1051\n",
      "Epoch 56/150\n",
      "\u001b[1m30/30\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 18ms/step - acc: 0.5143 - loss: 1.2964 - val_acc: 0.6740 - val_loss: 1.0942\n",
      "Epoch 57/150\n",
      "\u001b[1m30/30\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 17ms/step - acc: 0.5170 - loss: 1.2931 - val_acc: 0.6740 - val_loss: 1.0830\n",
      "Epoch 58/150\n",
      "\u001b[1m30/30\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 17ms/step - acc: 0.5134 - loss: 1.2848 - val_acc: 0.6790 - val_loss: 1.0697\n",
      "Epoch 59/150\n",
      "\u001b[1m30/30\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 16ms/step - acc: 0.5256 - loss: 1.2617 - val_acc: 0.6790 - val_loss: 1.0581\n",
      "Epoch 60/150\n",
      "\u001b[1m30/30\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 18ms/step - acc: 0.5259 - loss: 1.2550 - val_acc: 0.6870 - val_loss: 1.0499\n",
      "Epoch 61/150\n",
      "\u001b[1m30/30\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 20ms/step - acc: 0.5368 - loss: 1.2547 - val_acc: 0.6880 - val_loss: 1.0399\n",
      "Epoch 62/150\n",
      "\u001b[1m30/30\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 20ms/step - acc: 0.5345 - loss: 1.2325 - val_acc: 0.6890 - val_loss: 1.0315\n",
      "Epoch 63/150\n",
      "\u001b[1m30/30\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 19ms/step - acc: 0.5465 - loss: 1.2281 - val_acc: 0.6890 - val_loss: 1.0176\n",
      "Epoch 64/150\n",
      "\u001b[1m30/30\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 18ms/step - acc: 0.5410 - loss: 1.2136 - val_acc: 0.6930 - val_loss: 1.0063\n",
      "Epoch 65/150\n",
      "\u001b[1m30/30\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 17ms/step - acc: 0.5518 - loss: 1.1982 - val_acc: 0.7000 - val_loss: 0.9964\n",
      "Epoch 66/150\n",
      "\u001b[1m30/30\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 17ms/step - acc: 0.5471 - loss: 1.1961 - val_acc: 0.7010 - val_loss: 0.9908\n",
      "Epoch 67/150\n",
      "\u001b[1m30/30\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 20ms/step - acc: 0.5564 - loss: 1.1990 - val_acc: 0.7070 - val_loss: 0.9828\n",
      "Epoch 68/150\n",
      "\u001b[1m30/30\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 18ms/step - acc: 0.5510 - loss: 1.1949 - val_acc: 0.7040 - val_loss: 0.9759\n",
      "Epoch 69/150\n",
      "\u001b[1m30/30\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 18ms/step - acc: 0.5540 - loss: 1.1901 - val_acc: 0.7130 - val_loss: 0.9654\n",
      "Epoch 70/150\n",
      "\u001b[1m30/30\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 22ms/step - acc: 0.5590 - loss: 1.1740 - val_acc: 0.7060 - val_loss: 0.9585\n",
      "Epoch 71/150\n",
      "\u001b[1m30/30\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 19ms/step - acc: 0.5648 - loss: 1.1607 - val_acc: 0.7100 - val_loss: 0.9479\n",
      "Epoch 72/150\n",
      "\u001b[1m30/30\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 17ms/step - acc: 0.5616 - loss: 1.1662 - val_acc: 0.7090 - val_loss: 0.9434\n",
      "Epoch 73/150\n",
      "\u001b[1m30/30\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 18ms/step - acc: 0.5535 - loss: 1.1704 - val_acc: 0.7130 - val_loss: 0.9360\n",
      "Epoch 74/150\n",
      "\u001b[1m30/30\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 16ms/step - acc: 0.5670 - loss: 1.1483 - val_acc: 0.7100 - val_loss: 0.9256\n",
      "Epoch 75/150\n",
      "\u001b[1m30/30\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 17ms/step - acc: 0.5746 - loss: 1.1407 - val_acc: 0.7160 - val_loss: 0.9218\n",
      "Epoch 76/150\n",
      "\u001b[1m30/30\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 16ms/step - acc: 0.5836 - loss: 1.1339 - val_acc: 0.7170 - val_loss: 0.9152\n",
      "Epoch 77/150\n",
      "\u001b[1m30/30\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 17ms/step - acc: 0.5776 - loss: 1.1310 - val_acc: 0.7180 - val_loss: 0.9066\n",
      "Epoch 78/150\n",
      "\u001b[1m30/30\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 21ms/step - acc: 0.5920 - loss: 1.1020 - val_acc: 0.7150 - val_loss: 0.8995\n",
      "Epoch 79/150\n",
      "\u001b[1m30/30\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 19ms/step - acc: 0.5861 - loss: 1.1033 - val_acc: 0.7130 - val_loss: 0.8919\n",
      "Epoch 80/150\n",
      "\u001b[1m30/30\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 18ms/step - acc: 0.5860 - loss: 1.1129 - val_acc: 0.7130 - val_loss: 0.8884\n",
      "Epoch 81/150\n",
      "\u001b[1m30/30\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 16ms/step - acc: 0.5789 - loss: 1.1213 - val_acc: 0.7160 - val_loss: 0.8824\n",
      "Epoch 82/150\n",
      "\u001b[1m30/30\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 17ms/step - acc: 0.5930 - loss: 1.1002 - val_acc: 0.7150 - val_loss: 0.8760\n",
      "Epoch 83/150\n",
      "\u001b[1m30/30\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 17ms/step - acc: 0.5956 - loss: 1.0959 - val_acc: 0.7180 - val_loss: 0.8707\n",
      "Epoch 84/150\n",
      "\u001b[1m30/30\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 16ms/step - acc: 0.6001 - loss: 1.0825 - val_acc: 0.7180 - val_loss: 0.8667\n",
      "Epoch 85/150\n",
      "\u001b[1m30/30\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 18ms/step - acc: 0.6009 - loss: 1.0866 - val_acc: 0.7190 - val_loss: 0.8596\n",
      "Epoch 86/150\n",
      "\u001b[1m30/30\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 18ms/step - acc: 0.6015 - loss: 1.0639 - val_acc: 0.7260 - val_loss: 0.8507\n",
      "Epoch 87/150\n",
      "\u001b[1m30/30\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 20ms/step - acc: 0.6065 - loss: 1.0702 - val_acc: 0.7250 - val_loss: 0.8462\n",
      "Epoch 88/150\n",
      "\u001b[1m30/30\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 20ms/step - acc: 0.6070 - loss: 1.0658 - val_acc: 0.7260 - val_loss: 0.8428\n",
      "Epoch 89/150\n",
      "\u001b[1m30/30\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 18ms/step - acc: 0.6096 - loss: 1.0546 - val_acc: 0.7280 - val_loss: 0.8377\n",
      "Epoch 90/150\n",
      "\u001b[1m30/30\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 22ms/step - acc: 0.6132 - loss: 1.0442 - val_acc: 0.7280 - val_loss: 0.8311\n",
      "Epoch 91/150\n",
      "\u001b[1m30/30\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 18ms/step - acc: 0.6050 - loss: 1.0498 - val_acc: 0.7290 - val_loss: 0.8302\n",
      "Epoch 92/150\n",
      "\u001b[1m30/30\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 17ms/step - acc: 0.6076 - loss: 1.0615 - val_acc: 0.7300 - val_loss: 0.8278\n",
      "Epoch 93/150\n",
      "\u001b[1m30/30\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 17ms/step - acc: 0.6193 - loss: 1.0187 - val_acc: 0.7300 - val_loss: 0.8214\n",
      "Epoch 94/150\n",
      "\u001b[1m30/30\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 17ms/step - acc: 0.6266 - loss: 1.0133 - val_acc: 0.7320 - val_loss: 0.8147\n",
      "Epoch 95/150\n",
      "\u001b[1m30/30\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 16ms/step - acc: 0.6238 - loss: 1.0340 - val_acc: 0.7270 - val_loss: 0.8112\n",
      "Epoch 96/150\n",
      "\u001b[1m30/30\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 16ms/step - acc: 0.6170 - loss: 1.0134 - val_acc: 0.7320 - val_loss: 0.8046\n",
      "Epoch 97/150\n",
      "\u001b[1m30/30\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 17ms/step - acc: 0.6296 - loss: 1.0170 - val_acc: 0.7320 - val_loss: 0.8010\n",
      "Epoch 98/150\n",
      "\u001b[1m30/30\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 19ms/step - acc: 0.6301 - loss: 1.0117 - val_acc: 0.7330 - val_loss: 0.7968\n",
      "Epoch 99/150\n",
      "\u001b[1m30/30\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 16ms/step - acc: 0.6314 - loss: 1.0093 - val_acc: 0.7340 - val_loss: 0.7955\n",
      "Epoch 100/150\n",
      "\u001b[1m30/30\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 16ms/step - acc: 0.6314 - loss: 1.0326 - val_acc: 0.7300 - val_loss: 0.7925\n",
      "Epoch 101/150\n",
      "\u001b[1m30/30\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 16ms/step - acc: 0.6294 - loss: 0.9974 - val_acc: 0.7340 - val_loss: 0.7860\n",
      "Epoch 102/150\n",
      "\u001b[1m30/30\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 19ms/step - acc: 0.6286 - loss: 1.0157 - val_acc: 0.7320 - val_loss: 0.7811\n",
      "Epoch 103/150\n",
      "\u001b[1m30/30\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 16ms/step - acc: 0.6397 - loss: 0.9837 - val_acc: 0.7330 - val_loss: 0.7788\n",
      "Epoch 104/150\n",
      "\u001b[1m30/30\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 18ms/step - acc: 0.6385 - loss: 0.9827 - val_acc: 0.7320 - val_loss: 0.7756\n",
      "Epoch 105/150\n",
      "\u001b[1m30/30\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 18ms/step - acc: 0.6328 - loss: 0.9769 - val_acc: 0.7370 - val_loss: 0.7716\n",
      "Epoch 106/150\n",
      "\u001b[1m30/30\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 21ms/step - acc: 0.6353 - loss: 0.9874 - val_acc: 0.7340 - val_loss: 0.7690\n",
      "Epoch 107/150\n",
      "\u001b[1m30/30\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 17ms/step - acc: 0.6524 - loss: 0.9651 - val_acc: 0.7350 - val_loss: 0.7647\n",
      "Epoch 108/150\n",
      "\u001b[1m30/30\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 17ms/step - acc: 0.6276 - loss: 0.9817 - val_acc: 0.7350 - val_loss: 0.7623\n",
      "Epoch 109/150\n",
      "\u001b[1m30/30\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 16ms/step - acc: 0.6440 - loss: 0.9696 - val_acc: 0.7340 - val_loss: 0.7582\n",
      "Epoch 110/150\n",
      "\u001b[1m30/30\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 16ms/step - acc: 0.6443 - loss: 0.9643 - val_acc: 0.7370 - val_loss: 0.7560\n",
      "Epoch 111/150\n",
      "\u001b[1m30/30\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 19ms/step - acc: 0.6412 - loss: 0.9565 - val_acc: 0.7360 - val_loss: 0.7514\n",
      "Epoch 112/150\n",
      "\u001b[1m30/30\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 16ms/step - acc: 0.6546 - loss: 0.9364 - val_acc: 0.7350 - val_loss: 0.7495\n",
      "Epoch 113/150\n",
      "\u001b[1m30/30\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 16ms/step - acc: 0.6359 - loss: 0.9685 - val_acc: 0.7340 - val_loss: 0.7496\n",
      "Epoch 114/150\n",
      "\u001b[1m30/30\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 16ms/step - acc: 0.6440 - loss: 0.9574 - val_acc: 0.7330 - val_loss: 0.7448\n",
      "Epoch 115/150\n",
      "\u001b[1m30/30\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 17ms/step - acc: 0.6479 - loss: 0.9412 - val_acc: 0.7300 - val_loss: 0.7442\n",
      "Epoch 116/150\n",
      "\u001b[1m30/30\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 15ms/step - acc: 0.6527 - loss: 0.9430 - val_acc: 0.7360 - val_loss: 0.7400\n",
      "Epoch 117/150\n",
      "\u001b[1m30/30\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 16ms/step - acc: 0.6518 - loss: 0.9282 - val_acc: 0.7350 - val_loss: 0.7348\n",
      "Epoch 118/150\n",
      "\u001b[1m30/30\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 17ms/step - acc: 0.6586 - loss: 0.9312 - val_acc: 0.7350 - val_loss: 0.7328\n",
      "Epoch 119/150\n",
      "\u001b[1m30/30\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 16ms/step - acc: 0.6691 - loss: 0.9184 - val_acc: 0.7370 - val_loss: 0.7306\n",
      "Epoch 120/150\n",
      "\u001b[1m30/30\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 17ms/step - acc: 0.6558 - loss: 0.9249 - val_acc: 0.7350 - val_loss: 0.7288\n",
      "Epoch 121/150\n",
      "\u001b[1m30/30\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 16ms/step - acc: 0.6605 - loss: 0.9305 - val_acc: 0.7340 - val_loss: 0.7291\n",
      "Epoch 122/150\n",
      "\u001b[1m30/30\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 16ms/step - acc: 0.6652 - loss: 0.9306 - val_acc: 0.7370 - val_loss: 0.7235\n",
      "Epoch 123/150\n",
      "\u001b[1m30/30\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 20ms/step - acc: 0.6630 - loss: 0.9100 - val_acc: 0.7340 - val_loss: 0.7213\n",
      "Epoch 124/150\n",
      "\u001b[1m30/30\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 18ms/step - acc: 0.6625 - loss: 0.9075 - val_acc: 0.7350 - val_loss: 0.7181\n",
      "Epoch 125/150\n",
      "\u001b[1m30/30\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 16ms/step - acc: 0.6673 - loss: 0.9186 - val_acc: 0.7340 - val_loss: 0.7163\n",
      "Epoch 126/150\n",
      "\u001b[1m30/30\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 16ms/step - acc: 0.6590 - loss: 0.9245 - val_acc: 0.7360 - val_loss: 0.7137\n",
      "Epoch 127/150\n",
      "\u001b[1m30/30\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 16ms/step - acc: 0.6647 - loss: 0.9104 - val_acc: 0.7340 - val_loss: 0.7107\n",
      "Epoch 128/150\n",
      "\u001b[1m30/30\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 18ms/step - acc: 0.6615 - loss: 0.9171 - val_acc: 0.7380 - val_loss: 0.7069\n",
      "Epoch 129/150\n",
      "\u001b[1m30/30\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 16ms/step - acc: 0.6701 - loss: 0.8884 - val_acc: 0.7410 - val_loss: 0.7067\n",
      "Epoch 130/150\n",
      "\u001b[1m30/30\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 17ms/step - acc: 0.6529 - loss: 0.9225 - val_acc: 0.7370 - val_loss: 0.7057\n",
      "Epoch 131/150\n",
      "\u001b[1m30/30\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 16ms/step - acc: 0.6665 - loss: 0.9113 - val_acc: 0.7360 - val_loss: 0.7056\n",
      "Epoch 132/150\n",
      "\u001b[1m30/30\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 17ms/step - acc: 0.6753 - loss: 0.9047 - val_acc: 0.7360 - val_loss: 0.7024\n",
      "Epoch 133/150\n",
      "\u001b[1m30/30\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 16ms/step - acc: 0.6688 - loss: 0.8922 - val_acc: 0.7400 - val_loss: 0.7000\n",
      "Epoch 134/150\n",
      "\u001b[1m30/30\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 16ms/step - acc: 0.6554 - loss: 0.8966 - val_acc: 0.7420 - val_loss: 0.6963\n",
      "Epoch 135/150\n",
      "\u001b[1m30/30\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 16ms/step - acc: 0.6651 - loss: 0.8922 - val_acc: 0.7400 - val_loss: 0.6958\n",
      "Epoch 136/150\n",
      "\u001b[1m30/30\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 16ms/step - acc: 0.6668 - loss: 0.8915 - val_acc: 0.7370 - val_loss: 0.6947\n",
      "Epoch 137/150\n",
      "\u001b[1m30/30\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 19ms/step - acc: 0.6673 - loss: 0.8919 - val_acc: 0.7390 - val_loss: 0.6929\n",
      "Epoch 138/150\n",
      "\u001b[1m30/30\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 16ms/step - acc: 0.6841 - loss: 0.8489 - val_acc: 0.7410 - val_loss: 0.6892\n",
      "Epoch 139/150\n",
      "\u001b[1m30/30\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 17ms/step - acc: 0.6752 - loss: 0.8789 - val_acc: 0.7380 - val_loss: 0.6876\n",
      "Epoch 140/150\n",
      "\u001b[1m30/30\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 16ms/step - acc: 0.6632 - loss: 0.9041 - val_acc: 0.7360 - val_loss: 0.6891\n",
      "Epoch 141/150\n",
      "\u001b[1m30/30\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 17ms/step - acc: 0.6696 - loss: 0.8864 - val_acc: 0.7380 - val_loss: 0.6881\n",
      "Epoch 142/150\n",
      "\u001b[1m30/30\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 17ms/step - acc: 0.6716 - loss: 0.8756 - val_acc: 0.7400 - val_loss: 0.6883\n",
      "Epoch 143/150\n",
      "\u001b[1m30/30\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 16ms/step - acc: 0.6835 - loss: 0.8457 - val_acc: 0.7390 - val_loss: 0.6867\n",
      "Epoch 144/150\n",
      "\u001b[1m30/30\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 15ms/step - acc: 0.6812 - loss: 0.8728 - val_acc: 0.7410 - val_loss: 0.6836\n",
      "Epoch 145/150\n",
      "\u001b[1m30/30\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 16ms/step - acc: 0.6719 - loss: 0.8644 - val_acc: 0.7370 - val_loss: 0.6857\n",
      "Epoch 146/150\n",
      "\u001b[1m30/30\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 19ms/step - acc: 0.6797 - loss: 0.8617 - val_acc: 0.7390 - val_loss: 0.6845\n",
      "Epoch 147/150\n",
      "\u001b[1m30/30\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 16ms/step - acc: 0.6878 - loss: 0.8727 - val_acc: 0.7390 - val_loss: 0.6818\n",
      "Epoch 148/150\n",
      "\u001b[1m30/30\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 16ms/step - acc: 0.6947 - loss: 0.8352 - val_acc: 0.7370 - val_loss: 0.6793\n",
      "Epoch 149/150\n",
      "\u001b[1m30/30\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 16ms/step - acc: 0.6818 - loss: 0.8501 - val_acc: 0.7380 - val_loss: 0.6771\n",
      "Epoch 150/150\n",
      "\u001b[1m30/30\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 20ms/step - acc: 0.6859 - loss: 0.8374 - val_acc: 0.7400 - val_loss: 0.6745\n"
     ]
    }
   ],
   "source": [
    "# ⏰ This cell may take about a minute to run\n",
    "random.seed(123)\n",
    "dropout_model = models.Sequential()\n",
    "\n",
    "# Implement dropout to the input layer\n",
    "# NOTE: This is where you define the number of units in the input layer\n",
    "dropout_model.add(layers.Dropout(0.3, input_shape=(X_train_tokens.shape[1],)))\n",
    "\n",
    "# Add the first hidden layer\n",
    "dropout_model.add(layers.Dense(50, activation='relu'))\n",
    "\n",
    "\n",
    "# Implement dropout to the first hidden layer \n",
    "dropout_model.add(layers.Dropout(0.3))\n",
    "\n",
    "# Add the second hidden layer\n",
    "dropout_model.add(layers.Dense(25, activation='relu'))\n",
    "\n",
    "# Implement dropout to the second hidden layer \n",
    "dropout_model.add(layers.Dropout(0.3))\n",
    "\n",
    "# Add the output layer\n",
    "dropout_model.add(layers.Dense(7, activation='softmax'))\n",
    "\n",
    "\n",
    "# Compile the model\n",
    "dropout_model.compile(optimizer='SGD', \n",
    "                      loss='categorical_crossentropy', \n",
    "                      metrics=['acc'])\n",
    "\n",
    "# Train the model\n",
    "dropout_model_val = dropout_model.fit(X_train_tokens, \n",
    "                                      y_train_lb, \n",
    "                                      epochs=150, \n",
    "                                      batch_size=256, \n",
    "                                      validation_data=(X_val_tokens, y_val_lb))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m235/235\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 4ms/step - acc: 0.8111 - loss: 0.5555\n",
      "Training Loss: 0.562 \n",
      "Training Accuracy: 0.8\n",
      "----------\n",
      "\u001b[1m47/47\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - acc: 0.7717 - loss: 0.6133\n",
      "Test Loss: 0.621 \n",
      "Test Accuracy: 0.778\n"
     ]
    }
   ],
   "source": [
    "results_train = dropout_model.evaluate(X_train_tokens, y_train_lb)\n",
    "print(f'Training Loss: {results_train[0]:.3} \\nTraining Accuracy: {results_train[1]:.3}')\n",
    "\n",
    "print('----------')\n",
    "\n",
    "results_test = dropout_model.evaluate(X_test_tokens, y_test_lb)\n",
    "print(f'Test Loss: {results_test[0]:.3} \\nTest Accuracy: {results_test[1]:.3}') "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can see here that the validation performance has improved again, and the training and test accuracy are very close!  \n",
    "\n",
    "## Bigger Data? \n",
    "\n",
    "Finally, let's examine if we can improve the model's performance just by adding more data. We've quadrapled the sample dataset from 10,000 to 40,000 observations, and all you need to do is run the code! "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_bigger_sample = df.sample(40000, random_state=123)\n",
    "\n",
    "X = df['Consumer complaint narrative']\n",
    "y = df['Product']\n",
    "\n",
    "# Train-test split\n",
    "X_train_bigger, X_test_bigger, y_train_bigger, y_test_bigger = train_test_split(X, \n",
    "                                                                                y, \n",
    "                                                                                test_size=6000, \n",
    "                                                                                random_state=42)\n",
    "\n",
    "# Validation set\n",
    "X_train_final_bigger, X_val_bigger, y_train_final_bigger, y_val_bigger = train_test_split(X_train_bigger, \n",
    "                                                                                          y_train_bigger, \n",
    "                                                                                          test_size=4000, \n",
    "                                                                                          random_state=42)\n",
    "\n",
    "\n",
    "# One-hot encoding of the complaints\n",
    "tokenizer = Tokenizer(num_words=2000)\n",
    "tokenizer.fit_on_texts(X_train_final_bigger)\n",
    "\n",
    "X_train_tokens_bigger = tokenizer.texts_to_matrix(X_train_final_bigger, mode='binary')\n",
    "X_val_tokens_bigger = tokenizer.texts_to_matrix(X_val_bigger, mode='binary')\n",
    "X_test_tokens_bigger = tokenizer.texts_to_matrix(X_test_bigger, mode='binary')\n",
    "\n",
    "# One-hot encoding of products\n",
    "lb = LabelBinarizer()\n",
    "lb.fit(y_train_final_bigger)\n",
    "\n",
    "y_train_lb_bigger = to_categorical(lb.transform(y_train_final_bigger))[:, :, 1]\n",
    "y_val_lb_bigger = to_categorical(lb.transform(y_val_bigger))[:, :, 1]\n",
    "y_test_lb_bigger = to_categorical(lb.transform(y_test_bigger))[:, :, 1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\ADMIN\\anaconda3\\Lib\\site-packages\\keras\\src\\layers\\core\\dense.py:87: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
      "  super().__init__(activity_regularizer=activity_regularizer, **kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/150\n",
      "\u001b[1m196/196\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 10ms/step - acc: 0.1688 - loss: 1.9457 - val_acc: 0.2812 - val_loss: 1.8794\n",
      "Epoch 2/150\n",
      "\u001b[1m196/196\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 7ms/step - acc: 0.3156 - loss: 1.8309 - val_acc: 0.4207 - val_loss: 1.6488\n",
      "Epoch 3/150\n",
      "\u001b[1m196/196\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 7ms/step - acc: 0.4681 - loss: 1.5509 - val_acc: 0.5642 - val_loss: 1.3095\n",
      "Epoch 4/150\n",
      "\u001b[1m196/196\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 7ms/step - acc: 0.6046 - loss: 1.2192 - val_acc: 0.6535 - val_loss: 1.0622\n",
      "Epoch 5/150\n",
      "\u001b[1m196/196\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 7ms/step - acc: 0.6761 - loss: 0.9957 - val_acc: 0.6930 - val_loss: 0.9132\n",
      "Epoch 6/150\n",
      "\u001b[1m196/196\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 7ms/step - acc: 0.7071 - loss: 0.8648 - val_acc: 0.7163 - val_loss: 0.8228\n",
      "Epoch 7/150\n",
      "\u001b[1m196/196\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 8ms/step - acc: 0.7213 - loss: 0.7844 - val_acc: 0.7303 - val_loss: 0.7671\n",
      "Epoch 8/150\n",
      "\u001b[1m196/196\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 8ms/step - acc: 0.7364 - loss: 0.7254 - val_acc: 0.7385 - val_loss: 0.7306\n",
      "Epoch 9/150\n",
      "\u001b[1m196/196\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 8ms/step - acc: 0.7520 - loss: 0.6846 - val_acc: 0.7452 - val_loss: 0.7035\n",
      "Epoch 10/150\n",
      "\u001b[1m196/196\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 8ms/step - acc: 0.7575 - loss: 0.6581 - val_acc: 0.7502 - val_loss: 0.6769\n",
      "Epoch 11/150\n",
      "\u001b[1m196/196\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 7ms/step - acc: 0.7670 - loss: 0.6320 - val_acc: 0.7602 - val_loss: 0.6605\n",
      "Epoch 12/150\n",
      "\u001b[1m196/196\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 8ms/step - acc: 0.7711 - loss: 0.6182 - val_acc: 0.7663 - val_loss: 0.6456\n",
      "Epoch 13/150\n",
      "\u001b[1m196/196\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 8ms/step - acc: 0.7824 - loss: 0.5970 - val_acc: 0.7703 - val_loss: 0.6350\n",
      "Epoch 14/150\n",
      "\u001b[1m196/196\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 7ms/step - acc: 0.7844 - loss: 0.5871 - val_acc: 0.7740 - val_loss: 0.6225\n",
      "Epoch 15/150\n",
      "\u001b[1m196/196\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 8ms/step - acc: 0.7910 - loss: 0.5706 - val_acc: 0.7790 - val_loss: 0.6154\n",
      "Epoch 16/150\n",
      "\u001b[1m196/196\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 7ms/step - acc: 0.7957 - loss: 0.5590 - val_acc: 0.7770 - val_loss: 0.6083\n",
      "Epoch 17/150\n",
      "\u001b[1m196/196\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 7ms/step - acc: 0.7996 - loss: 0.5543 - val_acc: 0.7820 - val_loss: 0.5978\n",
      "Epoch 18/150\n",
      "\u001b[1m196/196\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 7ms/step - acc: 0.8059 - loss: 0.5433 - val_acc: 0.7845 - val_loss: 0.5939\n",
      "Epoch 19/150\n",
      "\u001b[1m196/196\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 8ms/step - acc: 0.8061 - loss: 0.5313 - val_acc: 0.7905 - val_loss: 0.5893\n",
      "Epoch 20/150\n",
      "\u001b[1m196/196\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 8ms/step - acc: 0.8119 - loss: 0.5252 - val_acc: 0.7925 - val_loss: 0.5836\n",
      "Epoch 21/150\n",
      "\u001b[1m196/196\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 8ms/step - acc: 0.8157 - loss: 0.5132 - val_acc: 0.7937 - val_loss: 0.5794\n",
      "Epoch 22/150\n",
      "\u001b[1m196/196\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 8ms/step - acc: 0.8168 - loss: 0.5114 - val_acc: 0.7935 - val_loss: 0.5755\n",
      "Epoch 23/150\n",
      "\u001b[1m196/196\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 7ms/step - acc: 0.8192 - loss: 0.5071 - val_acc: 0.7965 - val_loss: 0.5704\n",
      "Epoch 24/150\n",
      "\u001b[1m196/196\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 7ms/step - acc: 0.8197 - loss: 0.5056 - val_acc: 0.7958 - val_loss: 0.5660\n",
      "Epoch 25/150\n",
      "\u001b[1m196/196\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 7ms/step - acc: 0.8251 - loss: 0.4846 - val_acc: 0.7972 - val_loss: 0.5662\n",
      "Epoch 26/150\n",
      "\u001b[1m196/196\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 11ms/step - acc: 0.8247 - loss: 0.4900 - val_acc: 0.7985 - val_loss: 0.5616\n",
      "Epoch 27/150\n",
      "\u001b[1m196/196\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 8ms/step - acc: 0.8278 - loss: 0.4884 - val_acc: 0.8005 - val_loss: 0.5586\n",
      "Epoch 28/150\n",
      "\u001b[1m196/196\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 8ms/step - acc: 0.8274 - loss: 0.4793 - val_acc: 0.8018 - val_loss: 0.5571\n",
      "Epoch 29/150\n",
      "\u001b[1m196/196\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 7ms/step - acc: 0.8309 - loss: 0.4723 - val_acc: 0.8010 - val_loss: 0.5543\n",
      "Epoch 30/150\n",
      "\u001b[1m196/196\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 7ms/step - acc: 0.8315 - loss: 0.4662 - val_acc: 0.8008 - val_loss: 0.5522\n",
      "Epoch 31/150\n",
      "\u001b[1m196/196\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 7ms/step - acc: 0.8305 - loss: 0.4661 - val_acc: 0.8033 - val_loss: 0.5521\n",
      "Epoch 32/150\n",
      "\u001b[1m196/196\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 8ms/step - acc: 0.8374 - loss: 0.4611 - val_acc: 0.8060 - val_loss: 0.5527\n",
      "Epoch 33/150\n",
      "\u001b[1m196/196\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 8ms/step - acc: 0.8352 - loss: 0.4587 - val_acc: 0.8052 - val_loss: 0.5516\n",
      "Epoch 34/150\n",
      "\u001b[1m196/196\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 7ms/step - acc: 0.8360 - loss: 0.4568 - val_acc: 0.8052 - val_loss: 0.5486\n",
      "Epoch 35/150\n",
      "\u001b[1m196/196\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 8ms/step - acc: 0.8396 - loss: 0.4467 - val_acc: 0.8075 - val_loss: 0.5485\n",
      "Epoch 36/150\n",
      "\u001b[1m196/196\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 9ms/step - acc: 0.8422 - loss: 0.4448 - val_acc: 0.8055 - val_loss: 0.5479\n",
      "Epoch 37/150\n",
      "\u001b[1m196/196\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 7ms/step - acc: 0.8417 - loss: 0.4446 - val_acc: 0.8085 - val_loss: 0.5465\n",
      "Epoch 38/150\n",
      "\u001b[1m196/196\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 8ms/step - acc: 0.8433 - loss: 0.4412 - val_acc: 0.8080 - val_loss: 0.5447\n",
      "Epoch 39/150\n",
      "\u001b[1m196/196\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 8ms/step - acc: 0.8441 - loss: 0.4360 - val_acc: 0.8080 - val_loss: 0.5460\n",
      "Epoch 40/150\n",
      "\u001b[1m196/196\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 8ms/step - acc: 0.8489 - loss: 0.4297 - val_acc: 0.8070 - val_loss: 0.5438\n",
      "Epoch 41/150\n",
      "\u001b[1m196/196\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 8ms/step - acc: 0.8471 - loss: 0.4286 - val_acc: 0.8138 - val_loss: 0.5458\n",
      "Epoch 42/150\n",
      "\u001b[1m196/196\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 7ms/step - acc: 0.8475 - loss: 0.4295 - val_acc: 0.8115 - val_loss: 0.5450\n",
      "Epoch 43/150\n",
      "\u001b[1m196/196\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 7ms/step - acc: 0.8491 - loss: 0.4282 - val_acc: 0.8105 - val_loss: 0.5406\n",
      "Epoch 44/150\n",
      "\u001b[1m196/196\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 8ms/step - acc: 0.8496 - loss: 0.4218 - val_acc: 0.8120 - val_loss: 0.5398\n",
      "Epoch 45/150\n",
      "\u001b[1m196/196\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 7ms/step - acc: 0.8487 - loss: 0.4258 - val_acc: 0.8152 - val_loss: 0.5400\n",
      "Epoch 46/150\n",
      "\u001b[1m196/196\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 8ms/step - acc: 0.8500 - loss: 0.4221 - val_acc: 0.8105 - val_loss: 0.5448\n",
      "Epoch 47/150\n",
      "\u001b[1m196/196\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 8ms/step - acc: 0.8506 - loss: 0.4214 - val_acc: 0.8123 - val_loss: 0.5390\n",
      "Epoch 48/150\n",
      "\u001b[1m196/196\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 7ms/step - acc: 0.8533 - loss: 0.4162 - val_acc: 0.8167 - val_loss: 0.5393\n",
      "Epoch 49/150\n",
      "\u001b[1m196/196\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 8ms/step - acc: 0.8561 - loss: 0.4084 - val_acc: 0.8090 - val_loss: 0.5480\n",
      "Epoch 50/150\n",
      "\u001b[1m196/196\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 8ms/step - acc: 0.8547 - loss: 0.4094 - val_acc: 0.8150 - val_loss: 0.5407\n",
      "Epoch 51/150\n",
      "\u001b[1m196/196\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 8ms/step - acc: 0.8573 - loss: 0.4040 - val_acc: 0.8148 - val_loss: 0.5399\n",
      "Epoch 52/150\n",
      "\u001b[1m196/196\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 8ms/step - acc: 0.8575 - loss: 0.4055 - val_acc: 0.8138 - val_loss: 0.5388\n",
      "Epoch 53/150\n",
      "\u001b[1m196/196\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 8ms/step - acc: 0.8609 - loss: 0.4035 - val_acc: 0.8138 - val_loss: 0.5384\n",
      "Epoch 54/150\n",
      "\u001b[1m196/196\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 8ms/step - acc: 0.8612 - loss: 0.3963 - val_acc: 0.8160 - val_loss: 0.5380\n",
      "Epoch 55/150\n",
      "\u001b[1m196/196\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 7ms/step - acc: 0.8590 - loss: 0.3966 - val_acc: 0.8130 - val_loss: 0.5397\n",
      "Epoch 56/150\n",
      "\u001b[1m196/196\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 7ms/step - acc: 0.8611 - loss: 0.3983 - val_acc: 0.8150 - val_loss: 0.5413\n",
      "Epoch 57/150\n",
      "\u001b[1m196/196\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 7ms/step - acc: 0.8630 - loss: 0.3942 - val_acc: 0.8130 - val_loss: 0.5389\n",
      "Epoch 58/150\n",
      "\u001b[1m196/196\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 7ms/step - acc: 0.8637 - loss: 0.3879 - val_acc: 0.8125 - val_loss: 0.5421\n",
      "Epoch 59/150\n",
      "\u001b[1m196/196\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 9ms/step - acc: 0.8618 - loss: 0.3902 - val_acc: 0.8092 - val_loss: 0.5440\n",
      "Epoch 60/150\n",
      "\u001b[1m196/196\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 8ms/step - acc: 0.8645 - loss: 0.3900 - val_acc: 0.8155 - val_loss: 0.5417\n",
      "Epoch 61/150\n",
      "\u001b[1m196/196\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 8ms/step - acc: 0.8619 - loss: 0.3873 - val_acc: 0.8133 - val_loss: 0.5399\n",
      "Epoch 62/150\n",
      "\u001b[1m196/196\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 8ms/step - acc: 0.8624 - loss: 0.3888 - val_acc: 0.8155 - val_loss: 0.5415\n",
      "Epoch 63/150\n",
      "\u001b[1m196/196\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 8ms/step - acc: 0.8653 - loss: 0.3832 - val_acc: 0.8100 - val_loss: 0.5419\n",
      "Epoch 64/150\n",
      "\u001b[1m196/196\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 8ms/step - acc: 0.8686 - loss: 0.3804 - val_acc: 0.8133 - val_loss: 0.5417\n",
      "Epoch 65/150\n",
      "\u001b[1m196/196\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 7ms/step - acc: 0.8638 - loss: 0.3816 - val_acc: 0.8160 - val_loss: 0.5393\n",
      "Epoch 66/150\n",
      "\u001b[1m196/196\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 8ms/step - acc: 0.8649 - loss: 0.3831 - val_acc: 0.8105 - val_loss: 0.5426\n",
      "Epoch 67/150\n",
      "\u001b[1m196/196\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 7ms/step - acc: 0.8658 - loss: 0.3768 - val_acc: 0.8073 - val_loss: 0.5543\n",
      "Epoch 68/150\n",
      "\u001b[1m196/196\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 8ms/step - acc: 0.8688 - loss: 0.3740 - val_acc: 0.8165 - val_loss: 0.5404\n",
      "Epoch 69/150\n",
      "\u001b[1m196/196\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 8ms/step - acc: 0.8651 - loss: 0.3789 - val_acc: 0.8117 - val_loss: 0.5421\n",
      "Epoch 70/150\n",
      "\u001b[1m196/196\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 8ms/step - acc: 0.8669 - loss: 0.3757 - val_acc: 0.8148 - val_loss: 0.5430\n",
      "Epoch 71/150\n",
      "\u001b[1m196/196\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 8ms/step - acc: 0.8706 - loss: 0.3736 - val_acc: 0.8140 - val_loss: 0.5420\n",
      "Epoch 72/150\n",
      "\u001b[1m196/196\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 8ms/step - acc: 0.8682 - loss: 0.3796 - val_acc: 0.8150 - val_loss: 0.5436\n",
      "Epoch 73/150\n",
      "\u001b[1m196/196\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 8ms/step - acc: 0.8693 - loss: 0.3685 - val_acc: 0.8135 - val_loss: 0.5413\n",
      "Epoch 74/150\n",
      "\u001b[1m196/196\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 7ms/step - acc: 0.8696 - loss: 0.3689 - val_acc: 0.8152 - val_loss: 0.5450\n",
      "Epoch 75/150\n",
      "\u001b[1m196/196\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 8ms/step - acc: 0.8714 - loss: 0.3698 - val_acc: 0.8163 - val_loss: 0.5442\n",
      "Epoch 76/150\n",
      "\u001b[1m196/196\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 8ms/step - acc: 0.8730 - loss: 0.3626 - val_acc: 0.8120 - val_loss: 0.5529\n",
      "Epoch 77/150\n",
      "\u001b[1m196/196\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 7ms/step - acc: 0.8702 - loss: 0.3660 - val_acc: 0.8140 - val_loss: 0.5463\n",
      "Epoch 78/150\n",
      "\u001b[1m196/196\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 8ms/step - acc: 0.8712 - loss: 0.3613 - val_acc: 0.8117 - val_loss: 0.5476\n",
      "Epoch 79/150\n",
      "\u001b[1m196/196\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 8ms/step - acc: 0.8708 - loss: 0.3643 - val_acc: 0.8102 - val_loss: 0.5463\n",
      "Epoch 80/150\n",
      "\u001b[1m196/196\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 7ms/step - acc: 0.8716 - loss: 0.3594 - val_acc: 0.8112 - val_loss: 0.5451\n",
      "Epoch 81/150\n",
      "\u001b[1m196/196\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 8ms/step - acc: 0.8744 - loss: 0.3586 - val_acc: 0.8120 - val_loss: 0.5496\n",
      "Epoch 82/150\n",
      "\u001b[1m196/196\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 8ms/step - acc: 0.8766 - loss: 0.3573 - val_acc: 0.8083 - val_loss: 0.5518\n",
      "Epoch 83/150\n",
      "\u001b[1m196/196\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 7ms/step - acc: 0.8744 - loss: 0.3559 - val_acc: 0.8115 - val_loss: 0.5463\n",
      "Epoch 84/150\n",
      "\u001b[1m196/196\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 8ms/step - acc: 0.8766 - loss: 0.3524 - val_acc: 0.8130 - val_loss: 0.5492\n",
      "Epoch 85/150\n",
      "\u001b[1m196/196\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 8ms/step - acc: 0.8747 - loss: 0.3552 - val_acc: 0.8170 - val_loss: 0.5480\n",
      "Epoch 86/150\n",
      "\u001b[1m196/196\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 7ms/step - acc: 0.8737 - loss: 0.3555 - val_acc: 0.8148 - val_loss: 0.5469\n",
      "Epoch 87/150\n",
      "\u001b[1m196/196\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 8ms/step - acc: 0.8767 - loss: 0.3465 - val_acc: 0.8127 - val_loss: 0.5485\n",
      "Epoch 88/150\n",
      "\u001b[1m196/196\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 8ms/step - acc: 0.8773 - loss: 0.3467 - val_acc: 0.8135 - val_loss: 0.5501\n",
      "Epoch 89/150\n",
      "\u001b[1m196/196\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 8ms/step - acc: 0.8789 - loss: 0.3466 - val_acc: 0.8163 - val_loss: 0.5500\n",
      "Epoch 90/150\n",
      "\u001b[1m196/196\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 7ms/step - acc: 0.8775 - loss: 0.3448 - val_acc: 0.8127 - val_loss: 0.5513\n",
      "Epoch 91/150\n",
      "\u001b[1m196/196\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 8ms/step - acc: 0.8748 - loss: 0.3518 - val_acc: 0.8120 - val_loss: 0.5525\n",
      "Epoch 92/150\n",
      "\u001b[1m196/196\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 8ms/step - acc: 0.8778 - loss: 0.3478 - val_acc: 0.8100 - val_loss: 0.5527\n",
      "Epoch 93/150\n",
      "\u001b[1m196/196\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 7ms/step - acc: 0.8797 - loss: 0.3395 - val_acc: 0.8123 - val_loss: 0.5548\n",
      "Epoch 94/150\n",
      "\u001b[1m196/196\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 8ms/step - acc: 0.8789 - loss: 0.3421 - val_acc: 0.8148 - val_loss: 0.5540\n",
      "Epoch 95/150\n",
      "\u001b[1m196/196\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 7ms/step - acc: 0.8807 - loss: 0.3414 - val_acc: 0.8142 - val_loss: 0.5559\n",
      "Epoch 96/150\n",
      "\u001b[1m196/196\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 8ms/step - acc: 0.8805 - loss: 0.3368 - val_acc: 0.8120 - val_loss: 0.5562\n",
      "Epoch 97/150\n",
      "\u001b[1m196/196\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 7ms/step - acc: 0.8828 - loss: 0.3402 - val_acc: 0.8095 - val_loss: 0.5656\n",
      "Epoch 98/150\n",
      "\u001b[1m196/196\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 8ms/step - acc: 0.8814 - loss: 0.3384 - val_acc: 0.8112 - val_loss: 0.5559\n",
      "Epoch 99/150\n",
      "\u001b[1m196/196\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 7ms/step - acc: 0.8809 - loss: 0.3398 - val_acc: 0.8112 - val_loss: 0.5584\n",
      "Epoch 100/150\n",
      "\u001b[1m196/196\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 8ms/step - acc: 0.8827 - loss: 0.3351 - val_acc: 0.8135 - val_loss: 0.5603\n",
      "Epoch 101/150\n",
      "\u001b[1m196/196\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 8ms/step - acc: 0.8816 - loss: 0.3378 - val_acc: 0.8130 - val_loss: 0.5578\n",
      "Epoch 102/150\n",
      "\u001b[1m196/196\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 8ms/step - acc: 0.8838 - loss: 0.3292 - val_acc: 0.8108 - val_loss: 0.5626\n",
      "Epoch 103/150\n",
      "\u001b[1m196/196\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 7ms/step - acc: 0.8819 - loss: 0.3359 - val_acc: 0.8135 - val_loss: 0.5592\n",
      "Epoch 104/150\n",
      "\u001b[1m196/196\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 8ms/step - acc: 0.8843 - loss: 0.3297 - val_acc: 0.8130 - val_loss: 0.5612\n",
      "Epoch 105/150\n",
      "\u001b[1m196/196\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 7ms/step - acc: 0.8857 - loss: 0.3242 - val_acc: 0.8140 - val_loss: 0.5624\n",
      "Epoch 106/150\n",
      "\u001b[1m196/196\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 8ms/step - acc: 0.8818 - loss: 0.3335 - val_acc: 0.8127 - val_loss: 0.5618\n",
      "Epoch 107/150\n",
      "\u001b[1m196/196\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 8ms/step - acc: 0.8852 - loss: 0.3333 - val_acc: 0.8130 - val_loss: 0.5650\n",
      "Epoch 108/150\n",
      "\u001b[1m196/196\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 8ms/step - acc: 0.8855 - loss: 0.3299 - val_acc: 0.8083 - val_loss: 0.5676\n",
      "Epoch 109/150\n",
      "\u001b[1m196/196\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 7ms/step - acc: 0.8884 - loss: 0.3197 - val_acc: 0.8148 - val_loss: 0.5651\n",
      "Epoch 110/150\n",
      "\u001b[1m196/196\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 8ms/step - acc: 0.8886 - loss: 0.3181 - val_acc: 0.8133 - val_loss: 0.5628\n",
      "Epoch 111/150\n",
      "\u001b[1m196/196\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 7ms/step - acc: 0.8851 - loss: 0.3291 - val_acc: 0.8123 - val_loss: 0.5637\n",
      "Epoch 112/150\n",
      "\u001b[1m196/196\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 7ms/step - acc: 0.8876 - loss: 0.3242 - val_acc: 0.8073 - val_loss: 0.5750\n",
      "Epoch 113/150\n",
      "\u001b[1m196/196\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 8ms/step - acc: 0.8858 - loss: 0.3258 - val_acc: 0.8125 - val_loss: 0.5736\n",
      "Epoch 114/150\n",
      "\u001b[1m196/196\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 8ms/step - acc: 0.8878 - loss: 0.3232 - val_acc: 0.8098 - val_loss: 0.5670\n",
      "Epoch 115/150\n",
      "\u001b[1m196/196\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 8ms/step - acc: 0.8914 - loss: 0.3170 - val_acc: 0.8123 - val_loss: 0.5708\n",
      "Epoch 116/150\n",
      "\u001b[1m196/196\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 8ms/step - acc: 0.8876 - loss: 0.3229 - val_acc: 0.8083 - val_loss: 0.5828\n",
      "Epoch 117/150\n",
      "\u001b[1m196/196\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 8ms/step - acc: 0.8908 - loss: 0.3204 - val_acc: 0.8130 - val_loss: 0.5757\n",
      "Epoch 118/150\n",
      "\u001b[1m196/196\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 7ms/step - acc: 0.8907 - loss: 0.3149 - val_acc: 0.8140 - val_loss: 0.5700\n",
      "Epoch 119/150\n",
      "\u001b[1m196/196\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 8ms/step - acc: 0.8883 - loss: 0.3166 - val_acc: 0.8135 - val_loss: 0.5733\n",
      "Epoch 120/150\n",
      "\u001b[1m196/196\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 8ms/step - acc: 0.8911 - loss: 0.3138 - val_acc: 0.8140 - val_loss: 0.5706\n",
      "Epoch 121/150\n",
      "\u001b[1m196/196\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 7ms/step - acc: 0.8917 - loss: 0.3121 - val_acc: 0.8087 - val_loss: 0.5748\n",
      "Epoch 122/150\n",
      "\u001b[1m196/196\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 8ms/step - acc: 0.8920 - loss: 0.3109 - val_acc: 0.8100 - val_loss: 0.5752\n",
      "Epoch 123/150\n",
      "\u001b[1m196/196\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 8ms/step - acc: 0.8919 - loss: 0.3120 - val_acc: 0.8120 - val_loss: 0.5732\n",
      "Epoch 124/150\n",
      "\u001b[1m196/196\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 7ms/step - acc: 0.8920 - loss: 0.3099 - val_acc: 0.8105 - val_loss: 0.5771\n",
      "Epoch 125/150\n",
      "\u001b[1m196/196\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 8ms/step - acc: 0.8918 - loss: 0.3085 - val_acc: 0.8130 - val_loss: 0.5745\n",
      "Epoch 126/150\n",
      "\u001b[1m196/196\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 8ms/step - acc: 0.8918 - loss: 0.3093 - val_acc: 0.8092 - val_loss: 0.5814\n",
      "Epoch 127/150\n",
      "\u001b[1m196/196\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 8ms/step - acc: 0.8945 - loss: 0.3069 - val_acc: 0.8085 - val_loss: 0.5823\n",
      "Epoch 128/150\n",
      "\u001b[1m196/196\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 8ms/step - acc: 0.8932 - loss: 0.3049 - val_acc: 0.8127 - val_loss: 0.5812\n",
      "Epoch 129/150\n",
      "\u001b[1m196/196\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 8ms/step - acc: 0.8962 - loss: 0.3030 - val_acc: 0.8112 - val_loss: 0.5808\n",
      "Epoch 130/150\n",
      "\u001b[1m196/196\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 7ms/step - acc: 0.8946 - loss: 0.3061 - val_acc: 0.8098 - val_loss: 0.5829\n",
      "Epoch 131/150\n",
      "\u001b[1m196/196\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 7ms/step - acc: 0.8955 - loss: 0.3028 - val_acc: 0.8110 - val_loss: 0.5823\n",
      "Epoch 132/150\n",
      "\u001b[1m196/196\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 8ms/step - acc: 0.8949 - loss: 0.3025 - val_acc: 0.8083 - val_loss: 0.5824\n",
      "Epoch 133/150\n",
      "\u001b[1m196/196\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 7ms/step - acc: 0.8963 - loss: 0.3030 - val_acc: 0.8085 - val_loss: 0.5917\n",
      "Epoch 134/150\n",
      "\u001b[1m196/196\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 8ms/step - acc: 0.8985 - loss: 0.2984 - val_acc: 0.8120 - val_loss: 0.5828\n",
      "Epoch 135/150\n",
      "\u001b[1m196/196\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 8ms/step - acc: 0.8979 - loss: 0.3038 - val_acc: 0.8085 - val_loss: 0.5886\n",
      "Epoch 136/150\n",
      "\u001b[1m196/196\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 8ms/step - acc: 0.8994 - loss: 0.2973 - val_acc: 0.8080 - val_loss: 0.5879\n",
      "Epoch 137/150\n",
      "\u001b[1m196/196\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 8ms/step - acc: 0.8978 - loss: 0.2997 - val_acc: 0.8075 - val_loss: 0.5872\n",
      "Epoch 138/150\n",
      "\u001b[1m196/196\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 7ms/step - acc: 0.8986 - loss: 0.2974 - val_acc: 0.8055 - val_loss: 0.5949\n",
      "Epoch 139/150\n",
      "\u001b[1m196/196\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 7ms/step - acc: 0.8983 - loss: 0.2990 - val_acc: 0.8108 - val_loss: 0.5877\n",
      "Epoch 140/150\n",
      "\u001b[1m196/196\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 7ms/step - acc: 0.9012 - loss: 0.2957 - val_acc: 0.8090 - val_loss: 0.5880\n",
      "Epoch 141/150\n",
      "\u001b[1m196/196\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 7ms/step - acc: 0.8995 - loss: 0.2937 - val_acc: 0.8092 - val_loss: 0.5936\n",
      "Epoch 142/150\n",
      "\u001b[1m196/196\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 8ms/step - acc: 0.8999 - loss: 0.2949 - val_acc: 0.8102 - val_loss: 0.5924\n",
      "Epoch 143/150\n",
      "\u001b[1m196/196\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 8ms/step - acc: 0.9013 - loss: 0.2898 - val_acc: 0.8062 - val_loss: 0.5985\n",
      "Epoch 144/150\n",
      "\u001b[1m196/196\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 7ms/step - acc: 0.9005 - loss: 0.2899 - val_acc: 0.8058 - val_loss: 0.6021\n",
      "Epoch 145/150\n",
      "\u001b[1m196/196\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 8ms/step - acc: 0.9029 - loss: 0.2826 - val_acc: 0.8087 - val_loss: 0.5944\n",
      "Epoch 146/150\n",
      "\u001b[1m196/196\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 8ms/step - acc: 0.9015 - loss: 0.2881 - val_acc: 0.8092 - val_loss: 0.5970\n",
      "Epoch 147/150\n",
      "\u001b[1m196/196\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 7ms/step - acc: 0.8994 - loss: 0.2906 - val_acc: 0.8067 - val_loss: 0.5988\n",
      "Epoch 148/150\n",
      "\u001b[1m196/196\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 8ms/step - acc: 0.9036 - loss: 0.2859 - val_acc: 0.8077 - val_loss: 0.5984\n",
      "Epoch 149/150\n",
      "\u001b[1m196/196\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 8ms/step - acc: 0.9033 - loss: 0.2840 - val_acc: 0.8075 - val_loss: 0.6037\n",
      "Epoch 150/150\n",
      "\u001b[1m196/196\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 7ms/step - acc: 0.9032 - loss: 0.2866 - val_acc: 0.8062 - val_loss: 0.6027\n"
     ]
    }
   ],
   "source": [
    "# ⏰ This cell may take several minutes to run\n",
    "random.seed(123)\n",
    "bigger_data_model = models.Sequential()\n",
    "bigger_data_model.add(layers.Dense(50, activation='relu', input_shape=(2000,)))\n",
    "bigger_data_model.add(layers.Dense(25, activation='relu'))\n",
    "bigger_data_model.add(layers.Dense(7, activation='softmax'))\n",
    "\n",
    "bigger_data_model.compile(optimizer='SGD', \n",
    "                          loss='categorical_crossentropy', \n",
    "                          metrics=['acc'])\n",
    "\n",
    "bigger_data_model_val = bigger_data_model.fit(X_train_tokens_bigger,  \n",
    "                                              y_train_lb_bigger,  \n",
    "                                              epochs=150,  \n",
    "                                              batch_size=256,  \n",
    "                                              validation_data=(X_val_tokens_bigger, y_val_lb_bigger))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m1563/1563\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 4ms/step - acc: 0.9052 - loss: 0.2787\n",
      "Training Loss: 0.283 \n",
      "Training Accuracy: 0.903\n",
      "----------\n",
      "\u001b[1m125/125\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 4ms/step - acc: 0.8145 - loss: 0.5772\n",
      "Test Loss: 0.603 \n",
      "Test Accuracy: 0.806\n"
     ]
    }
   ],
   "source": [
    "results_train = bigger_data_model.evaluate(X_train_tokens_bigger, y_train_lb_bigger)\n",
    "print(f'Training Loss: {results_train[0]:.3} \\nTraining Accuracy: {results_train[1]:.3}')\n",
    "\n",
    "print('----------')\n",
    "\n",
    "results_test = bigger_data_model.evaluate(X_val_tokens_bigger, y_val_lb_bigger)\n",
    "print(f'Test Loss: {results_test[0]:.3} \\nTest Accuracy: {results_test[1]:.3}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "With the same amount of epochs and no regularization technique, you were able to get both better test accuracy and loss. You can still consider early stopping, L1, L2 and dropout here. It's clear that having more data has a strong impact on model performance! \n",
    "\n",
    "\n",
    "## Additional Resources\n",
    "\n",
    "* https://github.com/susanli2016/Machine-Learning-with-Python/blob/master/Consumer_complaints.ipynb\n",
    "* https://machinelearningmastery.com/dropout-regularization-deep-learning-models-keras/\n",
    "* https://catalog.data.gov/dataset/consumer-complaint-database \n",
    "\n",
    "\n",
    "## Summary  \n",
    "\n",
    "In this lesson, you built deep learning models using a validation set and used several techniques such as L2 and L1 regularization, dropout regularization, and early stopping to improve the accuracy of your models. "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
